{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 Answers on various questions here. How many connections can one Centrifugo instance handle? \u00b6 This depends on many factors. Hardware, message rate, size of messages, Centrifugo features enabled, client distribution over channels, websocket compression on/off etc. So no certain answer on this question exists. Common sense, tests and monitoring can help here. Generally we suggest to not put more than 50-100k clients on one node - but you should measure for your personal use case. You can find a benchmark we did in this docs chapter \u2013 though the point above is still valid, measure and monitor your own setup. Memory usage per connection? \u00b6 Depending on features enabled an amount of RAM required per each connection can vary. At moment, you can expect that each connection will cost about 30-50 KB of RAM, thus a server with 1 GB of RAM can handle about 20-30k connections. Can Centrifugo scale horizontally? \u00b6 Yes, it can. It can do this using builtin Redis Engine. Redis is very fast \u2013 for example it can handle hundreds of thousands requests per second. This should be OK for most applications in an internet. if you approach Redis resource limits (CPU or memory) then it's possible to use Centrifugo consistent sharding support to balance queries between different Redis instances. Message delivery model and message order guarantees \u00b6 The model of message delivery of Centrifugo server is at most once. With recovery feature it's possible to achieve at least once guarantee during retention period to recover missed messages after a temporary disconnect. So without recovery feature message you send to Centrifugo can be theoretically lost while moving towards your clients. Offline clients do not receive messages. Centrifugo tries to do the best effort to prevent message losses on a way to online clients, but you should be aware of possible message loss. Your application should tolerate this. As said above Centrifugo has an option to automatically recover messages that have been lost because of short network disconnections. Also, it prevents message loss on a way from Redis to nodes over Redis PUB/SUB using additional sequence check and periodical synchronization. We also recommend modeling your applications in a way that users don't notice when Centrifugo does not work at all. Use graceful degradation. For example if your user posts a new comment over AJAX call to your application backend - you should not rely only on Centrifugo to get new comment form and display it - you should return new comment data in AJAX call response and render it. This way user that posts a comment will think that everything works just fine. Be careful to not draw comments twice in this case. Message order in channels guaranteed to be the same while you publish messages into channel one after another or publish them in one request. If you do parallel publishes into the same channel then Centrifugo can't guarantee message order. Should I create channels explicitly? \u00b6 No. By default, channels created automatically as soon as first client subscribed to it. And destroyed automatically when last client unsubscribes from a channel. When history is on then a window of last messages kept automatically during retention period. So client that comes later and subscribes to channel can retrieve those messages using call to history. What about best practices with number of channels? \u00b6 Channel is a very lightweight entity - Centrifugo can deal with lots of channels, don't be afraid to use many channels. But keep in mind that one client should not be subscribed to lots of channels at the same moment. Use channels for separate real-time features. Using no more than several channels for a client is what you should try to achieve. A good analogy is writing SQL queries \u2013 you need to make sure you return content using fixed amount of database queries, as soon as more entries on your page result in more queries - your pages start working very slow at some point. The same for channels - you better to deliver real-time events over fixed amount of channels. It takes a separate frame for a client to subscribe to single channel \u2013 more frames mean more heavy initial connection. Presence for chat apps - online status of your contacts \u00b6 While presence is a good feature it does not fit well some apps. For example if you make chat app - you may probably use single personal channel for each user. In this case you cannot find who is online at moment using builtin Centrifugo presence feature as users do not share a common channel. You can solve this using your own separate service that tracks online status of your users (for example in Redis) and has bulk API that returns online status approximation for a list of users. This way you will have efficient scalable way to deal with online statuses. Centrifugo stops accepting new connections, why? \u00b6 The most popular reason behind this is reaching open file limit. Just make it higher, we described how to do this nearby in this doc. Can I use Centrifugo without reverse-proxy like Nginx before it? \u00b6 Yes, you can - Go standard library designed to allow this. Though proxy before Centrifugo can be very useful for load balancing clients. Does Centrifugo work with HTTP/2? \u00b6 Yes, Centrifugo works with HTTP/2. You can disable HTTP/2 running Centrifugo server with GODEBUG environment variable: GODEBUG=\"http2server=0\" centrifugo -c config.json Is there a way to use single connection to Centrifugo from different browser tabs? \u00b6 If underlying transport is HTTP-based, and you use HTTP/2 then this will work automatically. For WebSocket each browser tab creates new connection. What if I need to send push notifications to mobile or web applications? \u00b6 Sometimes it's confusing to see a difference between real-time messages and push notifications. Centrifugo is a real-time messaging server. It can not send push notifications to devices - to Apple iOS devices via APNS, Android devices via GCM or browsers over Web Push API. This is a goal for another software. But the reasonable question here is how can I know when I need to send real-time message to client online or push notification to its device because application closed at client's device at moment. The solution is pretty simple. You can keep critical notifications for client in database. And when client read message send ack to your backend marking that notification as read by client, you save this ack too. Periodically you can check which notifications were sent to clients but they have not read it (no ack received). For such notification you can send push notification to its device using your own or another open-source solution. Look at Firebase for example! How can I know a message really delivered to a client? \u00b6 You can but Centrifugo does not have such API. What you have to do to ensure your client received message is sending confirmation ack from your client to your application backend as soon as client processed message coming from Centrifugo channel. Can I publish new messages over websocket connection from a client? \u00b6 Centrifugo designed to stream messages from server to client. Even though it's possible to publish messages into channels directly from a client (when publish channel option enabled) - we strongly discourage this in production usage as those messages will go through Centrifugo without any control. Theoretically Centrifugo could resend messages published from client to your application backend endpoint (i.e. having some sort of webhook built in) but it does not seem beneficial it terms of overall performance and application architecture at moment. And this will require extra layer of convetions about Centrifugo-to-backend communication. So in general when user generates an event it must be first delivered to your app backend using a convenient way (for example AJAX POST request for web application), processed on backend (validated, saved into main application database) and then published to Centrifugo using Centrifugo HTTP or GRPC API. Sometimes publishing from a client directly into a channel can be useful though - for personal projects, for demonstrations (like we do in our examples ) or if you trust your users and want to build application without backend. In all cases when you don't need any message control on your backend. Since Centrifugo v2.3.0 it's possible to utilize RPC proxy feature \u2013 in this case you can call RPC over Centrifugo WebSocket which will be translated to HTTP request to your backend. This way you can utilize WebSocket transport in bidirectional way. How to create secure channel for two users only (private chat case)? \u00b6 There are several ways to achieve it: use a private channel (starting with $ ) - every time user will try to subscribe on it your backend should provide sign to confirm that subscription request. Read more in special chapter about channels next is user limited channels (with # ) - you can create a channel with name like dialog#42,567 to limit subscribers only to user with id 42 and user with ID 567 finally, you can create hard to guess channel name (based on some secret key and user IDs or just generate and save this long unique name into your main app database) so other users won't know this channel to subscribe on it. This is the simplest but not the safest way - but can be reasonable to consider in many situations. What's the best way to organize channel configuration? \u00b6 In most situations your application need several real-time features. We suggest to use namespaces for every real-time feature if it requires some option enabled. For example if you need join/leave messages for chat app - create a special channel namespace with this join_leave option enabled. Otherwise, your other channels will receive join/leave messages too - increasing load and traffic in system but not actually used by clients. The same relates to other channel options. Can I rely on Centrifugo and its message history for guaranteed message delivery? \u00b6 The short answer is \u2013 no, you cannot. As you may know Centrifugo can keep message history for a while. But there are several caveats you need to know: Centrifugo with Memory engine will reset all history after restart When using Redis Engine history will inherit properties of your Redis setup Centrifugo client protocol designed to give a client a tip that some messages lost after reconnect (when message recovery feature is on) Does Centrifugo support webhooks? \u00b6 Centrifugo designed in a way where messages mostly flow one direction: from server to client. In idiomatic case you publish messages to your backend first, then after saving to your main database publish to channel over Centrifugo API to deliver a real-time message to all active channel subscribers. Now if you need any extra callbacks/webhooks you can call your application backend yourself from client side (for example just after connect event fired in client library). There are several reasons why we can't simply add webhooks \u2013 some of them described in this issue . A bit tricky thing are disconnects. It's pretty hard as there are no guarantee that disconnect code will have time to execute on client side (as client can just switch off its device or simply lose internet connection). If you need to know that client disconnected and program your business logic around this fact then a reasonable approach is periodically call your backend from client side and update user status somewhere on backend (use Redis maybe). This is a pretty robust solution where you can't occasionally miss disconnect event. HTTP proxy feature added in v2.3.0 allows integrating Centrifugo with your own session mechanism and provides a way to react on connection events. Also, it opens a road for bidirectional communication with RPC calls. But the note above about disconnects is still true - we can't simply call your app in case of client disconnects as loosing one such event can result in broken business logic inside your app. How scalable is presence and join/leave information? \u00b6 Presence is good for small channels with a reasonable number of subscribers, as soon as there are tons of subscribers presence info becomes very expensive in terms of bandwidth (as it contains information about all clients in channel). There is presence_stats API method that can be helpful if you only need to know a number of clients (or unique users) in a channel. But in case of Redis engine even presence stats not optimized for channels with more that several thousands active subscribers. You may consider using separate service to deal with presense status information that provides information in near real-time maybe with some reasonable approximation. The same is true for join/leave messages - as soon as you turn on join/leave events for a channel with many subscribers every join/leave event (which generally happen relatively frequently) result into many messages sent to each subscriber in a channel, drastically multiplying amount of messages travelling through the system. So be careful and estimate possible load. What is the difference between Centrifugo and Centrifuge \u00b6 Centrifugo is a server built on top of Centrifuge library for Go language. This documentation built to describe Centrifugo. Though many things said here can be considered as extra documentation for Centrifuge library. I have not found an answer on my question here: \u00b6 We have Gitter chat room and Telegram group - welcome! I want to contribute to this awesome project \u00b6 We have many things you can help with \u2013 just ask us in our chat rooms.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Answers on various questions here.","title":"Frequently Asked Questions"},{"location":"faq/#how-many-connections-can-one-centrifugo-instance-handle","text":"This depends on many factors. Hardware, message rate, size of messages, Centrifugo features enabled, client distribution over channels, websocket compression on/off etc. So no certain answer on this question exists. Common sense, tests and monitoring can help here. Generally we suggest to not put more than 50-100k clients on one node - but you should measure for your personal use case. You can find a benchmark we did in this docs chapter \u2013 though the point above is still valid, measure and monitor your own setup.","title":"How many connections can one Centrifugo instance handle?"},{"location":"faq/#memory-usage-per-connection","text":"Depending on features enabled an amount of RAM required per each connection can vary. At moment, you can expect that each connection will cost about 30-50 KB of RAM, thus a server with 1 GB of RAM can handle about 20-30k connections.","title":"Memory usage per connection?"},{"location":"faq/#can-centrifugo-scale-horizontally","text":"Yes, it can. It can do this using builtin Redis Engine. Redis is very fast \u2013 for example it can handle hundreds of thousands requests per second. This should be OK for most applications in an internet. if you approach Redis resource limits (CPU or memory) then it's possible to use Centrifugo consistent sharding support to balance queries between different Redis instances.","title":"Can Centrifugo scale horizontally?"},{"location":"faq/#message-delivery-model-and-message-order-guarantees","text":"The model of message delivery of Centrifugo server is at most once. With recovery feature it's possible to achieve at least once guarantee during retention period to recover missed messages after a temporary disconnect. So without recovery feature message you send to Centrifugo can be theoretically lost while moving towards your clients. Offline clients do not receive messages. Centrifugo tries to do the best effort to prevent message losses on a way to online clients, but you should be aware of possible message loss. Your application should tolerate this. As said above Centrifugo has an option to automatically recover messages that have been lost because of short network disconnections. Also, it prevents message loss on a way from Redis to nodes over Redis PUB/SUB using additional sequence check and periodical synchronization. We also recommend modeling your applications in a way that users don't notice when Centrifugo does not work at all. Use graceful degradation. For example if your user posts a new comment over AJAX call to your application backend - you should not rely only on Centrifugo to get new comment form and display it - you should return new comment data in AJAX call response and render it. This way user that posts a comment will think that everything works just fine. Be careful to not draw comments twice in this case. Message order in channels guaranteed to be the same while you publish messages into channel one after another or publish them in one request. If you do parallel publishes into the same channel then Centrifugo can't guarantee message order.","title":"Message delivery model and message order guarantees"},{"location":"faq/#should-i-create-channels-explicitly","text":"No. By default, channels created automatically as soon as first client subscribed to it. And destroyed automatically when last client unsubscribes from a channel. When history is on then a window of last messages kept automatically during retention period. So client that comes later and subscribes to channel can retrieve those messages using call to history.","title":"Should I create channels explicitly?"},{"location":"faq/#what-about-best-practices-with-number-of-channels","text":"Channel is a very lightweight entity - Centrifugo can deal with lots of channels, don't be afraid to use many channels. But keep in mind that one client should not be subscribed to lots of channels at the same moment. Use channels for separate real-time features. Using no more than several channels for a client is what you should try to achieve. A good analogy is writing SQL queries \u2013 you need to make sure you return content using fixed amount of database queries, as soon as more entries on your page result in more queries - your pages start working very slow at some point. The same for channels - you better to deliver real-time events over fixed amount of channels. It takes a separate frame for a client to subscribe to single channel \u2013 more frames mean more heavy initial connection.","title":"What about best practices with number of channels?"},{"location":"faq/#presence-for-chat-apps-online-status-of-your-contacts","text":"While presence is a good feature it does not fit well some apps. For example if you make chat app - you may probably use single personal channel for each user. In this case you cannot find who is online at moment using builtin Centrifugo presence feature as users do not share a common channel. You can solve this using your own separate service that tracks online status of your users (for example in Redis) and has bulk API that returns online status approximation for a list of users. This way you will have efficient scalable way to deal with online statuses.","title":"Presence for chat apps - online status of your contacts"},{"location":"faq/#centrifugo-stops-accepting-new-connections-why","text":"The most popular reason behind this is reaching open file limit. Just make it higher, we described how to do this nearby in this doc.","title":"Centrifugo stops accepting new connections, why?"},{"location":"faq/#can-i-use-centrifugo-without-reverse-proxy-like-nginx-before-it","text":"Yes, you can - Go standard library designed to allow this. Though proxy before Centrifugo can be very useful for load balancing clients.","title":"Can I use Centrifugo without reverse-proxy like Nginx before it?"},{"location":"faq/#does-centrifugo-work-with-http2","text":"Yes, Centrifugo works with HTTP/2. You can disable HTTP/2 running Centrifugo server with GODEBUG environment variable: GODEBUG=\"http2server=0\" centrifugo -c config.json","title":"Does Centrifugo work with HTTP/2?"},{"location":"faq/#is-there-a-way-to-use-single-connection-to-centrifugo-from-different-browser-tabs","text":"If underlying transport is HTTP-based, and you use HTTP/2 then this will work automatically. For WebSocket each browser tab creates new connection.","title":"Is there a way to use single connection to Centrifugo from different browser tabs?"},{"location":"faq/#what-if-i-need-to-send-push-notifications-to-mobile-or-web-applications","text":"Sometimes it's confusing to see a difference between real-time messages and push notifications. Centrifugo is a real-time messaging server. It can not send push notifications to devices - to Apple iOS devices via APNS, Android devices via GCM or browsers over Web Push API. This is a goal for another software. But the reasonable question here is how can I know when I need to send real-time message to client online or push notification to its device because application closed at client's device at moment. The solution is pretty simple. You can keep critical notifications for client in database. And when client read message send ack to your backend marking that notification as read by client, you save this ack too. Periodically you can check which notifications were sent to clients but they have not read it (no ack received). For such notification you can send push notification to its device using your own or another open-source solution. Look at Firebase for example!","title":"What if I need to send push notifications to mobile or web applications?"},{"location":"faq/#how-can-i-know-a-message-really-delivered-to-a-client","text":"You can but Centrifugo does not have such API. What you have to do to ensure your client received message is sending confirmation ack from your client to your application backend as soon as client processed message coming from Centrifugo channel.","title":"How can I know a message really delivered to a client?"},{"location":"faq/#can-i-publish-new-messages-over-websocket-connection-from-a-client","text":"Centrifugo designed to stream messages from server to client. Even though it's possible to publish messages into channels directly from a client (when publish channel option enabled) - we strongly discourage this in production usage as those messages will go through Centrifugo without any control. Theoretically Centrifugo could resend messages published from client to your application backend endpoint (i.e. having some sort of webhook built in) but it does not seem beneficial it terms of overall performance and application architecture at moment. And this will require extra layer of convetions about Centrifugo-to-backend communication. So in general when user generates an event it must be first delivered to your app backend using a convenient way (for example AJAX POST request for web application), processed on backend (validated, saved into main application database) and then published to Centrifugo using Centrifugo HTTP or GRPC API. Sometimes publishing from a client directly into a channel can be useful though - for personal projects, for demonstrations (like we do in our examples ) or if you trust your users and want to build application without backend. In all cases when you don't need any message control on your backend. Since Centrifugo v2.3.0 it's possible to utilize RPC proxy feature \u2013 in this case you can call RPC over Centrifugo WebSocket which will be translated to HTTP request to your backend. This way you can utilize WebSocket transport in bidirectional way.","title":"Can I publish new messages over websocket connection from a client?"},{"location":"faq/#how-to-create-secure-channel-for-two-users-only-private-chat-case","text":"There are several ways to achieve it: use a private channel (starting with $ ) - every time user will try to subscribe on it your backend should provide sign to confirm that subscription request. Read more in special chapter about channels next is user limited channels (with # ) - you can create a channel with name like dialog#42,567 to limit subscribers only to user with id 42 and user with ID 567 finally, you can create hard to guess channel name (based on some secret key and user IDs or just generate and save this long unique name into your main app database) so other users won't know this channel to subscribe on it. This is the simplest but not the safest way - but can be reasonable to consider in many situations.","title":"How to create secure channel for two users only (private chat case)?"},{"location":"faq/#whats-the-best-way-to-organize-channel-configuration","text":"In most situations your application need several real-time features. We suggest to use namespaces for every real-time feature if it requires some option enabled. For example if you need join/leave messages for chat app - create a special channel namespace with this join_leave option enabled. Otherwise, your other channels will receive join/leave messages too - increasing load and traffic in system but not actually used by clients. The same relates to other channel options.","title":"What's the best way to organize channel configuration?"},{"location":"faq/#can-i-rely-on-centrifugo-and-its-message-history-for-guaranteed-message-delivery","text":"The short answer is \u2013 no, you cannot. As you may know Centrifugo can keep message history for a while. But there are several caveats you need to know: Centrifugo with Memory engine will reset all history after restart When using Redis Engine history will inherit properties of your Redis setup Centrifugo client protocol designed to give a client a tip that some messages lost after reconnect (when message recovery feature is on)","title":"Can I rely on Centrifugo and its message history for guaranteed message delivery?"},{"location":"faq/#does-centrifugo-support-webhooks","text":"Centrifugo designed in a way where messages mostly flow one direction: from server to client. In idiomatic case you publish messages to your backend first, then after saving to your main database publish to channel over Centrifugo API to deliver a real-time message to all active channel subscribers. Now if you need any extra callbacks/webhooks you can call your application backend yourself from client side (for example just after connect event fired in client library). There are several reasons why we can't simply add webhooks \u2013 some of them described in this issue . A bit tricky thing are disconnects. It's pretty hard as there are no guarantee that disconnect code will have time to execute on client side (as client can just switch off its device or simply lose internet connection). If you need to know that client disconnected and program your business logic around this fact then a reasonable approach is periodically call your backend from client side and update user status somewhere on backend (use Redis maybe). This is a pretty robust solution where you can't occasionally miss disconnect event. HTTP proxy feature added in v2.3.0 allows integrating Centrifugo with your own session mechanism and provides a way to react on connection events. Also, it opens a road for bidirectional communication with RPC calls. But the note above about disconnects is still true - we can't simply call your app in case of client disconnects as loosing one such event can result in broken business logic inside your app.","title":"Does Centrifugo support webhooks?"},{"location":"faq/#how-scalable-is-presence-and-joinleave-information","text":"Presence is good for small channels with a reasonable number of subscribers, as soon as there are tons of subscribers presence info becomes very expensive in terms of bandwidth (as it contains information about all clients in channel). There is presence_stats API method that can be helpful if you only need to know a number of clients (or unique users) in a channel. But in case of Redis engine even presence stats not optimized for channels with more that several thousands active subscribers. You may consider using separate service to deal with presense status information that provides information in near real-time maybe with some reasonable approximation. The same is true for join/leave messages - as soon as you turn on join/leave events for a channel with many subscribers every join/leave event (which generally happen relatively frequently) result into many messages sent to each subscriber in a channel, drastically multiplying amount of messages travelling through the system. So be careful and estimate possible load.","title":"How scalable is presence and join/leave information?"},{"location":"faq/#what-is-the-difference-between-centrifugo-and-centrifuge","text":"Centrifugo is a server built on top of Centrifuge library for Go language. This documentation built to describe Centrifugo. Though many things said here can be considered as extra documentation for Centrifuge library.","title":"What is the difference between Centrifugo and Centrifuge"},{"location":"faq/#i-have-not-found-an-answer-on-my-question-here","text":"We have Gitter chat room and Telegram group - welcome!","title":"I have not found an answer on my question here:"},{"location":"faq/#i-want-to-contribute-to-this-awesome-project","text":"We have many things you can help with \u2013 just ask us in our chat rooms.","title":"I want to contribute to this awesome project"},{"location":"getting_started/","text":"What is Centrifugo \u00b6 Centrifugo is a scalable real-time messaging server in language-agnostic way. The term language-agnostic in this definition means that it does not matter which programming language your application uses on a frontend or backend sides \u2013 Centrifugo works in conjunction with any. Centrifugo is fast and scales well to support millions of client connections. Real-time messages are messages delivered to your application users almost immediately after some event happened - think live comments, chat apps, real-time charts, dynamic counters and multiplayer games. There are several real-time messaging transports Centrifugo supports at moment: Websocket with JSON or binary Protobuf protocols SockJS \u2013 library that tries to establish Websocket connection first and then falls back to HTTP transports (Server-Sent Events, XHR-streaming, XHR-polling etc) automatically in case of problems with Websocket connection Join community \u00b6 We have rooms in Telegram and Discord \u2013 welcome! Motivation of project \u00b6 Centrifugo was originally born to help applications with server side written in language or framework without built-in concurrency support. In this case Centrifugo is a very straightforward and non-obtrusive way to introduce real-time updates. For example frameworks like Django, Flask, Yii, Laravel, Ruby on Rails etc has poor or not very performant support of working with many persistent connections. Centrifugo aims to help with this and continue to write a backend with your favorite language or favorite framework. Centrifugo also has some features (performance, scalability, connection management, message recovery on reconnect etc) that can simplify your life as a developer even if you are writing backend in asynchronous concurrent language. Concepts \u00b6 Centrifugo runs as standalone server and takes care of handling persistent connections from your application users. Your application backend and frontend can be written in any programming language. Your clients connect to Centrifugo from a frontend and subscribe to channels. As soon as some event happens your application backend can publish a message with event into a channel using Centrifugo API. That message will be delivered to all clients currently subscribed on a channel. So actually Centrifugo is a user-facing PUB/SUB server. Here is a simplified scheme: Highlights \u00b6 Here is a list with main Centrifugo highlights: Centrifugo is fast and capable to scale to millions of simultaneous connections Simple integration with any application \u2013 works as separate service Simple server API (HTTP or GRPC) Client-side libraries for popular frontend environments JSON and binary Protobuf Websocket client protocol based on strict schema SockJS polyfill for web browsers without Websocket support User authentication with JWT or over connection request proxy to configured HTTP endpoint Proper connection management and expiration control Various types of channels: private, user-limited Various types of subscriptions: client-side or server-side Transform RPC calls over WebSocket/SockJS to configured HTTP endpoint call Presence information for channels (show all active clients in channel) History information for channels (last messages published into channel) Join/leave events for channels (client goes online/offline) Automatic recovery of missed messages between client reconnects over configured retention period Built-in administrative web panel Cross platform \u2013 works on Linux, MacOS and Windows Ready to deploy (Docker, RPM/DEB packages, automatic Let's Encrypt TLS certificates, Prometheus/Graphite monitoring) MIT license","title":"What is Centrifugo"},{"location":"getting_started/#what-is-centrifugo","text":"Centrifugo is a scalable real-time messaging server in language-agnostic way. The term language-agnostic in this definition means that it does not matter which programming language your application uses on a frontend or backend sides \u2013 Centrifugo works in conjunction with any. Centrifugo is fast and scales well to support millions of client connections. Real-time messages are messages delivered to your application users almost immediately after some event happened - think live comments, chat apps, real-time charts, dynamic counters and multiplayer games. There are several real-time messaging transports Centrifugo supports at moment: Websocket with JSON or binary Protobuf protocols SockJS \u2013 library that tries to establish Websocket connection first and then falls back to HTTP transports (Server-Sent Events, XHR-streaming, XHR-polling etc) automatically in case of problems with Websocket connection","title":"What is Centrifugo"},{"location":"getting_started/#join-community","text":"We have rooms in Telegram and Discord \u2013 welcome!","title":"Join community"},{"location":"getting_started/#motivation-of-project","text":"Centrifugo was originally born to help applications with server side written in language or framework without built-in concurrency support. In this case Centrifugo is a very straightforward and non-obtrusive way to introduce real-time updates. For example frameworks like Django, Flask, Yii, Laravel, Ruby on Rails etc has poor or not very performant support of working with many persistent connections. Centrifugo aims to help with this and continue to write a backend with your favorite language or favorite framework. Centrifugo also has some features (performance, scalability, connection management, message recovery on reconnect etc) that can simplify your life as a developer even if you are writing backend in asynchronous concurrent language.","title":"Motivation of project"},{"location":"getting_started/#concepts","text":"Centrifugo runs as standalone server and takes care of handling persistent connections from your application users. Your application backend and frontend can be written in any programming language. Your clients connect to Centrifugo from a frontend and subscribe to channels. As soon as some event happens your application backend can publish a message with event into a channel using Centrifugo API. That message will be delivered to all clients currently subscribed on a channel. So actually Centrifugo is a user-facing PUB/SUB server. Here is a simplified scheme:","title":"Concepts"},{"location":"getting_started/#highlights","text":"Here is a list with main Centrifugo highlights: Centrifugo is fast and capable to scale to millions of simultaneous connections Simple integration with any application \u2013 works as separate service Simple server API (HTTP or GRPC) Client-side libraries for popular frontend environments JSON and binary Protobuf Websocket client protocol based on strict schema SockJS polyfill for web browsers without Websocket support User authentication with JWT or over connection request proxy to configured HTTP endpoint Proper connection management and expiration control Various types of channels: private, user-limited Various types of subscriptions: client-side or server-side Transform RPC calls over WebSocket/SockJS to configured HTTP endpoint call Presence information for channels (show all active clients in channel) History information for channels (last messages published into channel) Join/leave events for channels (client goes online/offline) Automatic recovery of missed messages between client reconnects over configured retention period Built-in administrative web panel Cross platform \u2013 works on Linux, MacOS and Windows Ready to deploy (Docker, RPM/DEB packages, automatic Let's Encrypt TLS certificates, Prometheus/Graphite monitoring) MIT license","title":"Highlights"},{"location":"guide/","text":"Centrifugo integration guide \u00b6 This chapter aims to help you get started with Centrifugo. We will look at a step-by-step workflow of integrating your application with Centrifugo providing links to relevant parts of this documentation. As Centrifugo is language-agnostic and can be used together with any language/framework we won't be specific here about any backend or frontend technology your application can be built with. Only abstract steps which you can extrapolate to your application stack. So first of all let's look again at a simplified scheme: There are three parts involved into idiomatic Centrifugo usage scenario: your clients (frontend application), your application backend and Centrifugo. It's possible to use Centrifugo without any application backend involved but here we won't consider this use case. Here let's suppose you already have 2 of 3 elements: clients and backend. Now you want to add Centrifugo to receive real-time events on client side. 0. Install \u00b6 First you need to do is download/install Centrifugo server. See install chapter for details. 1. Configure Centrifugo \u00b6 Create basic configuration file with token_hmac_secret_key (or token_rsa_public_key ) and api_key set and then run Centrifugo. See this chapter for details about token_hmac_secret_key / token_rsa_public_key and chapter about API for API description. The simplest way to do this automatically is by using genconfig command: ./centrifugo genconfig \u2013 which will generate config.json file for you with all required fields. A generated configuration also includes v3_use_offset option set to true . This is an option that enables using actual offset field in client-server protocol and will be used by default in Centrifugo v3. This option available since Centrifugo v2.5.0 and described in detail in v2.5.0 release notes . 2. Configure your backend \u00b6 In configuration file of your application backend register several variables: Centrifugo secret and Centrifugo API key you set on a previous step and Centrifugo API address. By default, API address is http://localhost:8000/api . You must never reveal token secret and API key to your users . 3. Connect to Centrifugo \u00b6 Now your users can start connecting to Centrifugo. You should get client library (see list of available client libraries ) for your application frontend. Every library has method to connect to Centrifugo. See information about Centrifugo connection endpoints here . Every client should provide connection token (JWT) on connect. You must generate this token on your backend side using Centrifugo secret key you set to backend configuration (note that in case of RSA tokens you are generating JWT with private key). See how to generate this JWT in special chapter . You pass this token from backend to your frontend app (pass it in template context or use separate request from client side to get user specific JWT from backend side). And use this token when connecting to Centrifugo (for example browser client has special method setToken ). Since Centrifugo v2.3.0 there is a way to authenticate connections without using JWT - see chapter about proxying to backend . 4. Subscribe to channels \u00b6 After connecting to Centrifugo subscribe clients to channels they are interested in. See more about channels in special chapter . All client libraries provide a way to handle messages coming to client from a channel after subscribing to it. Since Centrifugo v2.4.0 there is a way to subscribe connection to a list of channels on server side at the moment of connection establishment. See chapter about server-side subscriptions . 5. Publish to channel \u00b6 So everything should work now \u2013 as soon as user opens some page of your application it must successfully connect to Centrifugo and subscribe to a channel (or channels). Now let's imagine you want to send a real-time message to users subscribed on a specific channel. This message can be a reaction on some event happened in your app: someone posted new comment, administrator just created new post, user pressed like button etc. Anyway this is an event your backend just got, and you want to immediately share it with interested users. You can do this using Centrifugo HTTP API . To simplify your life we have several API libraries for different languages. You can publish message into channel using one of those libraries or you can simply follow API description to construct API request yourself - this is very simple. Also Centrifugo supports GRPC API . As soon as you published message to channel it must be delivered to your client. 6. Deploy to production \u00b6 To put this all into production you need to deploy Centrifugo on your production server. To help you with this we have many things like Docker image, rpm and deb packages, Nginx configuration. You can find more information in Deploy section of this doc. See OS tuning chapter for some actions you have to do to prepare your server machine for handling many persistent connections. 7. Monitor Centrifugo \u00b6 Don't forget to monitor your production Centrifugo setup. 8. Scale Centrifugo \u00b6 As soon as you are close to machine resource limits you may want to scale Centrifugo \u2013 you can run many Centrifugo instances and load-balance clients between them using Redis engine . 9. Read FAQ \u00b6 That's all for basics. Documentation actually covers lots of other concepts Centrifugo server has: like scalability, private channels, admin web interface, SockJS fallback, Protobuf support and more. And don't forget to read our FAQ .","title":"Integration Guide"},{"location":"guide/#centrifugo-integration-guide","text":"This chapter aims to help you get started with Centrifugo. We will look at a step-by-step workflow of integrating your application with Centrifugo providing links to relevant parts of this documentation. As Centrifugo is language-agnostic and can be used together with any language/framework we won't be specific here about any backend or frontend technology your application can be built with. Only abstract steps which you can extrapolate to your application stack. So first of all let's look again at a simplified scheme: There are three parts involved into idiomatic Centrifugo usage scenario: your clients (frontend application), your application backend and Centrifugo. It's possible to use Centrifugo without any application backend involved but here we won't consider this use case. Here let's suppose you already have 2 of 3 elements: clients and backend. Now you want to add Centrifugo to receive real-time events on client side.","title":"Centrifugo integration guide"},{"location":"guide/#0-install","text":"First you need to do is download/install Centrifugo server. See install chapter for details.","title":"0. Install"},{"location":"guide/#1-configure-centrifugo","text":"Create basic configuration file with token_hmac_secret_key (or token_rsa_public_key ) and api_key set and then run Centrifugo. See this chapter for details about token_hmac_secret_key / token_rsa_public_key and chapter about API for API description. The simplest way to do this automatically is by using genconfig command: ./centrifugo genconfig \u2013 which will generate config.json file for you with all required fields. A generated configuration also includes v3_use_offset option set to true . This is an option that enables using actual offset field in client-server protocol and will be used by default in Centrifugo v3. This option available since Centrifugo v2.5.0 and described in detail in v2.5.0 release notes .","title":"1. Configure Centrifugo"},{"location":"guide/#2-configure-your-backend","text":"In configuration file of your application backend register several variables: Centrifugo secret and Centrifugo API key you set on a previous step and Centrifugo API address. By default, API address is http://localhost:8000/api . You must never reveal token secret and API key to your users .","title":"2. Configure your backend"},{"location":"guide/#3-connect-to-centrifugo","text":"Now your users can start connecting to Centrifugo. You should get client library (see list of available client libraries ) for your application frontend. Every library has method to connect to Centrifugo. See information about Centrifugo connection endpoints here . Every client should provide connection token (JWT) on connect. You must generate this token on your backend side using Centrifugo secret key you set to backend configuration (note that in case of RSA tokens you are generating JWT with private key). See how to generate this JWT in special chapter . You pass this token from backend to your frontend app (pass it in template context or use separate request from client side to get user specific JWT from backend side). And use this token when connecting to Centrifugo (for example browser client has special method setToken ). Since Centrifugo v2.3.0 there is a way to authenticate connections without using JWT - see chapter about proxying to backend .","title":"3. Connect to Centrifugo"},{"location":"guide/#4-subscribe-to-channels","text":"After connecting to Centrifugo subscribe clients to channels they are interested in. See more about channels in special chapter . All client libraries provide a way to handle messages coming to client from a channel after subscribing to it. Since Centrifugo v2.4.0 there is a way to subscribe connection to a list of channels on server side at the moment of connection establishment. See chapter about server-side subscriptions .","title":"4. Subscribe to channels"},{"location":"guide/#5-publish-to-channel","text":"So everything should work now \u2013 as soon as user opens some page of your application it must successfully connect to Centrifugo and subscribe to a channel (or channels). Now let's imagine you want to send a real-time message to users subscribed on a specific channel. This message can be a reaction on some event happened in your app: someone posted new comment, administrator just created new post, user pressed like button etc. Anyway this is an event your backend just got, and you want to immediately share it with interested users. You can do this using Centrifugo HTTP API . To simplify your life we have several API libraries for different languages. You can publish message into channel using one of those libraries or you can simply follow API description to construct API request yourself - this is very simple. Also Centrifugo supports GRPC API . As soon as you published message to channel it must be delivered to your client.","title":"5. Publish to channel"},{"location":"guide/#6-deploy-to-production","text":"To put this all into production you need to deploy Centrifugo on your production server. To help you with this we have many things like Docker image, rpm and deb packages, Nginx configuration. You can find more information in Deploy section of this doc. See OS tuning chapter for some actions you have to do to prepare your server machine for handling many persistent connections.","title":"6. Deploy to production"},{"location":"guide/#7-monitor-centrifugo","text":"Don't forget to monitor your production Centrifugo setup.","title":"7. Monitor Centrifugo"},{"location":"guide/#8-scale-centrifugo","text":"As soon as you are close to machine resource limits you may want to scale Centrifugo \u2013 you can run many Centrifugo instances and load-balance clients between them using Redis engine .","title":"8. Scale Centrifugo"},{"location":"guide/#9-read-faq","text":"That's all for basics. Documentation actually covers lots of other concepts Centrifugo server has: like scalability, private channels, admin web interface, SockJS fallback, Protobuf support and more. And don't forget to read our FAQ .","title":"9. Read FAQ"},{"location":"install/","text":"Server overview and installation \u00b6 Centrifugo server written in Go language. It's an open-source software, the source code is available on Github . Centrifugo is built around Centrifuge library for Go language. That library defines custom protocol and message types which must be sent over various transports (Websocket, SockJS). Server clients use that protocol internally and provide simple API to features - making persistent connection, subscribing on channels, calling RPC commands and more. Server documentation covers a lot of server concepts in detail. Here we start with ways to install Centrifugo on your system. Install from binary release \u00b6 Binary releases available on Github. Download latest release for your operating system, unpack it and you are done. Centrifugo is pre-built for: Linux 64-bit (linux_amd64) Linux 32-bit (linux_386) MacOS (darwin_amd64) Windows (windows_amd64) FreeBSD (freebsd_amd64) ARM v6 (linux_armv6) Archives contain a single statically compiled binary centrifugo file which is ready to run: ./centrifugo -h See version of Centrifugo: ./centrifugo version Centrifugo server node requires configuration file with some secret keys. If you are new to Centrifugo then there is genconfig command which generates minimal required configuration file: ./centrifugo genconfig It generates secret keys automatically and creates configuration file config.json in a current directory (by default) so you can finally run Centrifugo instance: ./centrifugo --config = config.json We will talk about a configuration in detail in next sections. You can also put or symlink centrifugo into your bin OS directory and run it from anywhere: centrifugo --config = config.json Docker image \u00b6 Centrifugo server has docker image available on Docker Hub . docker pull centrifugo/centrifugo Run: docker run --ulimit nofile = 65536 :65536 -v /host/dir/with/config/file:/centrifugo -p 8000 :8000 centrifugo/centrifugo centrifugo -c config.json Note that docker allows setting nofile limits in command-line arguments which is pretty important to handle lots of simultaneous persistent connections and not run out of open file limit (each connection requires one file descriptor). See also OS tuning chapter . RPM and DEB packages for Linux \u00b6 Every time we make new Centrifugo release we upload rpm and deb packages for popular linux distributions on packagecloud.io . At moment, we support versions of the following distributions: 64-bit Debian 8 Jessie 64-bit Debian 9 Stretch 64-bit Debian 10 Buster 64-bit Ubuntu 16.04 Xenial 64-bit Ubuntu 18.04 Bionic 64-bit Ubuntu 20.04 Focal Fossa 64-bit Centos 7 64-bit Centos 8 See full list of available packages and installation instructions . Centrifugo also works on 32-bit architecture, but we don't support packaging for it since 64-bit is more convenient for servers today. With brew on MacOS \u00b6 If you are developing on MacOS then you can install Centrifugo over brew : brew tap centrifugal/centrifugo brew install centrifugo Using go get \u00b6 If you are Go developer then you can quickly try Centrifugo with go get : go get github.com/centrifugal/centrifugo centrifugo This will install Centrifugo to your Go bin directory. Build from source \u00b6 You need Go language installed: git clone https://github.com/centrifugal/centrifugo.git cd centrifugo go build ./centrifugo","title":"Installation"},{"location":"install/#server-overview-and-installation","text":"Centrifugo server written in Go language. It's an open-source software, the source code is available on Github . Centrifugo is built around Centrifuge library for Go language. That library defines custom protocol and message types which must be sent over various transports (Websocket, SockJS). Server clients use that protocol internally and provide simple API to features - making persistent connection, subscribing on channels, calling RPC commands and more. Server documentation covers a lot of server concepts in detail. Here we start with ways to install Centrifugo on your system.","title":"Server overview and installation"},{"location":"install/#install-from-binary-release","text":"Binary releases available on Github. Download latest release for your operating system, unpack it and you are done. Centrifugo is pre-built for: Linux 64-bit (linux_amd64) Linux 32-bit (linux_386) MacOS (darwin_amd64) Windows (windows_amd64) FreeBSD (freebsd_amd64) ARM v6 (linux_armv6) Archives contain a single statically compiled binary centrifugo file which is ready to run: ./centrifugo -h See version of Centrifugo: ./centrifugo version Centrifugo server node requires configuration file with some secret keys. If you are new to Centrifugo then there is genconfig command which generates minimal required configuration file: ./centrifugo genconfig It generates secret keys automatically and creates configuration file config.json in a current directory (by default) so you can finally run Centrifugo instance: ./centrifugo --config = config.json We will talk about a configuration in detail in next sections. You can also put or symlink centrifugo into your bin OS directory and run it from anywhere: centrifugo --config = config.json","title":"Install from binary release"},{"location":"install/#docker-image","text":"Centrifugo server has docker image available on Docker Hub . docker pull centrifugo/centrifugo Run: docker run --ulimit nofile = 65536 :65536 -v /host/dir/with/config/file:/centrifugo -p 8000 :8000 centrifugo/centrifugo centrifugo -c config.json Note that docker allows setting nofile limits in command-line arguments which is pretty important to handle lots of simultaneous persistent connections and not run out of open file limit (each connection requires one file descriptor). See also OS tuning chapter .","title":"Docker image"},{"location":"install/#rpm-and-deb-packages-for-linux","text":"Every time we make new Centrifugo release we upload rpm and deb packages for popular linux distributions on packagecloud.io . At moment, we support versions of the following distributions: 64-bit Debian 8 Jessie 64-bit Debian 9 Stretch 64-bit Debian 10 Buster 64-bit Ubuntu 16.04 Xenial 64-bit Ubuntu 18.04 Bionic 64-bit Ubuntu 20.04 Focal Fossa 64-bit Centos 7 64-bit Centos 8 See full list of available packages and installation instructions . Centrifugo also works on 32-bit architecture, but we don't support packaging for it since 64-bit is more convenient for servers today.","title":"RPM and DEB packages for Linux"},{"location":"install/#with-brew-on-macos","text":"If you are developing on MacOS then you can install Centrifugo over brew : brew tap centrifugal/centrifugo brew install centrifugo","title":"With brew on MacOS"},{"location":"install/#using-go-get","text":"If you are Go developer then you can quickly try Centrifugo with go get : go get github.com/centrifugal/centrifugo centrifugo This will install Centrifugo to your Go bin directory.","title":"Using go get"},{"location":"install/#build-from-source","text":"You need Go language installed: git clone https://github.com/centrifugal/centrifugo.git cd centrifugo go build ./centrifugo","title":"Build from source"},{"location":"quick_start/","text":"Quick start \u00b6 Here we will build a very simple browser application with Centrifugo. It works in a way that users connect to Centrifugo over WebSocket, subscribe to channel and start receiving all messages published to that channel. In our case we will send a counter value to all channel subscribers to update it in all open browser tabs in real-time. Hope you installed Centrifugo . We can generate minimal required configuration file with the following command: ./centrifugo genconfig It will generate config.json file in the same directory with content like this: { \"v3_use_offset\": true, \"token_hmac_secret_key\": \"46b38493-147e-4e3f-86e0-dc5ec54f5133\", \"admin_password\": \"ad0dff75-3131-4a02-8d64-9279b4f1c57b\", \"admin_secret\": \"583bc4b7-0fa5-4c4a-8566-16d3ce4ad401\", \"api_key\": \"aaaf202f-b5f8-4b34-bf88-f6c03a1ecda6\" } Now we can start server, and let's start it with built-in admin web interface: ./centrifugo --config=config.json --admin Now open http://localhost:8000 \u2013 and you should see Centrifugo admin web panel. Enter admin_password from configuration file to log in. Inside admin panel you should see that one Centrifugo node is running, and it does not have connected clients. Now let's create index.html file with our simple app: < html > < head > < title > Centrifugo quick start </ title > </ head > < body > < div id = \"counter\" > - </ div > < script src = \"https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.6.2/dist/centrifuge.min.js\" ></ script > < script type = \"text/javascript\" > const container = document . getElementById ( 'counter' ) const centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" ); centrifuge . setToken ( \"<TOKEN>\" ); centrifuge . on ( 'connect' , function ( ctx ) { console . log ( \"connected\" , ctx ); }); centrifuge . on ( 'disconnect' , function ( ctx ) { console . log ( \"disconnected\" , ctx ); }); centrifuge . subscribe ( \"channel\" , function ( ctx ) { container . innerHTML = ctx . data . value ; document . title = ctx . data . value ; }); centrifuge . connect (); </ script > </ body > </ html > Note that we are using centrifuge-js 2.6.2 in this example, you better use its latest version for a moment of reading this. We create an instance of client providing it Centrifugo default WebSocket endpoint address, then we subscribe to channel channel and provide callback function to process real-time messages. Then we call connect method to create WebSocket connection. You need to open this file in a browser, for example on MacOS: open index.html Or just enter sth like file:///path/to/index.html to browser address bar. In real application you will serve your HTML files with a proper web server \u2013 but for this simple example we don't need it. Now if you look at browser developer tools or in Centrifugo logs you will notice that connection not successfully established: 2020-05-16 01:19:59 [INF] invalid connection token error=\"jwt: token format is not valid\" client=45a1b8f4-d6dc-4679-9927-93e41c14ad93 2020-05-16 01:19:59 [INF] disconnect after handling command client=45a1b8f4-d6dc-4679-9927-93e41c14ad93 command=\"id:1 params:\\\"{\\\\\\\"token\\\\\\\":\\\\\\\"<TOKEN>\\\\\\\"}\\\" \" reason=\"invalid token\" user= That's because client should provide valid JWT (JSON Web Token) to authenticate itself. This token must be generated on your backend and passed to client side. Since in our simple example we don't have application backend we can quickly generate example token for a user using centrifugo sub-command gentoken . Like this: ./centrifugo gentoken -u 123722 \u2013 where -u flag sets user ID. The output should be like: HMAC SHA-256 JWT for user 123722 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE1OTAxODYzMTZ9.YMJVJsQbK_p1fYFWkcoKBYr718AeavAk3MAYvxcMk0M \u2013 you will have another token value since this one based on randomly generated token_hmac_secret_key from configuration file we created in the beginning of this tutorial. Now we can copy generated HMAC SHA-256 JWT and paste it into centrifuge.setToken call instead of <TOKEN> placeholder in index.html file. I.e.: centrifuge.setToken(\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE1OTAxODYzMTZ9.YMJVJsQbK_p1fYFWkcoKBYr718AeavAk3MAYvxcMk0M\"); That's it! Now if you reload your browser tab \u2013 connection will be successfully established and client will subscribe to channel. If you open developer tools and look at WebSocket frames panel you should see sth like this: OK, the last thing we need to do here is publish new counter value to channel and make sure our app works properly. We can do this over Centrifugo API sending HTTP request to default API endpoint http://localhost:8000/api , but let's do this over admin web panel first. Open Centrifugo admin web panel in another browser tab and go to Actions section. Select publish action, insert channel name that you want to publish to \u2013 in our case this is string channel and insert into data area JSON like this: { \"value\" : 1 } Click Submit button and check out application browser tab \u2013 counter value must be immediately received and displayed. Open several browser tabs with our app and make sure all tabs receive message as soon as you publish it. BTW, let's also look at how you can publish data to channel over Centrifugo API from a terminal using curl tool: curl --header \"Content-Type: application/json\" \\ --header \"Authorization: apikey aaaf202f-b5f8-4b34-bf88-f6c03a1ecda6\" \\ --request POST \\ --data '{\"method\": \"publish\", \"params\": {\"channel\": \"channel\", \"data\": {\"value\": 2}}}' \\ http://localhost:8000/api \u2013 where for Authorization header we set api_key value from Centrifugo config file generated above. We did it! We built the simplest app with Centrifugo and its Javascript client. It does not have backend, it's not very useful to be honest, but it should give you an insight on how to start working with Centrifugo server. Read more about Centrifugo server in next documentations chapters \u2013 it can do much-much more than we just showed here. Integration guide describes a process of idiomatic Centrifugo integration with your application backend. More examples \u00b6 Several more examples located on Github \u2013 check out this repo","title":"Quick start"},{"location":"quick_start/#quick-start","text":"Here we will build a very simple browser application with Centrifugo. It works in a way that users connect to Centrifugo over WebSocket, subscribe to channel and start receiving all messages published to that channel. In our case we will send a counter value to all channel subscribers to update it in all open browser tabs in real-time. Hope you installed Centrifugo . We can generate minimal required configuration file with the following command: ./centrifugo genconfig It will generate config.json file in the same directory with content like this: { \"v3_use_offset\": true, \"token_hmac_secret_key\": \"46b38493-147e-4e3f-86e0-dc5ec54f5133\", \"admin_password\": \"ad0dff75-3131-4a02-8d64-9279b4f1c57b\", \"admin_secret\": \"583bc4b7-0fa5-4c4a-8566-16d3ce4ad401\", \"api_key\": \"aaaf202f-b5f8-4b34-bf88-f6c03a1ecda6\" } Now we can start server, and let's start it with built-in admin web interface: ./centrifugo --config=config.json --admin Now open http://localhost:8000 \u2013 and you should see Centrifugo admin web panel. Enter admin_password from configuration file to log in. Inside admin panel you should see that one Centrifugo node is running, and it does not have connected clients. Now let's create index.html file with our simple app: < html > < head > < title > Centrifugo quick start </ title > </ head > < body > < div id = \"counter\" > - </ div > < script src = \"https://cdn.jsdelivr.net/gh/centrifugal/centrifuge-js@2.6.2/dist/centrifuge.min.js\" ></ script > < script type = \"text/javascript\" > const container = document . getElementById ( 'counter' ) const centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" ); centrifuge . setToken ( \"<TOKEN>\" ); centrifuge . on ( 'connect' , function ( ctx ) { console . log ( \"connected\" , ctx ); }); centrifuge . on ( 'disconnect' , function ( ctx ) { console . log ( \"disconnected\" , ctx ); }); centrifuge . subscribe ( \"channel\" , function ( ctx ) { container . innerHTML = ctx . data . value ; document . title = ctx . data . value ; }); centrifuge . connect (); </ script > </ body > </ html > Note that we are using centrifuge-js 2.6.2 in this example, you better use its latest version for a moment of reading this. We create an instance of client providing it Centrifugo default WebSocket endpoint address, then we subscribe to channel channel and provide callback function to process real-time messages. Then we call connect method to create WebSocket connection. You need to open this file in a browser, for example on MacOS: open index.html Or just enter sth like file:///path/to/index.html to browser address bar. In real application you will serve your HTML files with a proper web server \u2013 but for this simple example we don't need it. Now if you look at browser developer tools or in Centrifugo logs you will notice that connection not successfully established: 2020-05-16 01:19:59 [INF] invalid connection token error=\"jwt: token format is not valid\" client=45a1b8f4-d6dc-4679-9927-93e41c14ad93 2020-05-16 01:19:59 [INF] disconnect after handling command client=45a1b8f4-d6dc-4679-9927-93e41c14ad93 command=\"id:1 params:\\\"{\\\\\\\"token\\\\\\\":\\\\\\\"<TOKEN>\\\\\\\"}\\\" \" reason=\"invalid token\" user= That's because client should provide valid JWT (JSON Web Token) to authenticate itself. This token must be generated on your backend and passed to client side. Since in our simple example we don't have application backend we can quickly generate example token for a user using centrifugo sub-command gentoken . Like this: ./centrifugo gentoken -u 123722 \u2013 where -u flag sets user ID. The output should be like: HMAC SHA-256 JWT for user 123722 with expiration TTL 168h0m0s: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE1OTAxODYzMTZ9.YMJVJsQbK_p1fYFWkcoKBYr718AeavAk3MAYvxcMk0M \u2013 you will have another token value since this one based on randomly generated token_hmac_secret_key from configuration file we created in the beginning of this tutorial. Now we can copy generated HMAC SHA-256 JWT and paste it into centrifuge.setToken call instead of <TOKEN> placeholder in index.html file. I.e.: centrifuge.setToken(\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM3MjIiLCJleHAiOjE1OTAxODYzMTZ9.YMJVJsQbK_p1fYFWkcoKBYr718AeavAk3MAYvxcMk0M\"); That's it! Now if you reload your browser tab \u2013 connection will be successfully established and client will subscribe to channel. If you open developer tools and look at WebSocket frames panel you should see sth like this: OK, the last thing we need to do here is publish new counter value to channel and make sure our app works properly. We can do this over Centrifugo API sending HTTP request to default API endpoint http://localhost:8000/api , but let's do this over admin web panel first. Open Centrifugo admin web panel in another browser tab and go to Actions section. Select publish action, insert channel name that you want to publish to \u2013 in our case this is string channel and insert into data area JSON like this: { \"value\" : 1 } Click Submit button and check out application browser tab \u2013 counter value must be immediately received and displayed. Open several browser tabs with our app and make sure all tabs receive message as soon as you publish it. BTW, let's also look at how you can publish data to channel over Centrifugo API from a terminal using curl tool: curl --header \"Content-Type: application/json\" \\ --header \"Authorization: apikey aaaf202f-b5f8-4b34-bf88-f6c03a1ecda6\" \\ --request POST \\ --data '{\"method\": \"publish\", \"params\": {\"channel\": \"channel\", \"data\": {\"value\": 2}}}' \\ http://localhost:8000/api \u2013 where for Authorization header we set api_key value from Centrifugo config file generated above. We did it! We built the simplest app with Centrifugo and its Javascript client. It does not have backend, it's not very useful to be honest, but it should give you an insight on how to start working with Centrifugo server. Read more about Centrifugo server in next documentations chapters \u2013 it can do much-much more than we just showed here. Integration guide describes a process of idiomatic Centrifugo integration with your application backend.","title":"Quick start"},{"location":"quick_start/#more-examples","text":"Several more examples located on Github \u2013 check out this repo","title":"More examples"},{"location":"deploy/monitoring/","text":"Monitoring \u00b6 Centrifugo supports reporting metrics in Prometheus format and can automatically export metrics to Graphite. Prometheus \u00b6 To enable Prometheus endpoint start Centrifugo with prometheus option on: { ... \"prometheus\" : true } ./centrifugo --config=config.json This will enable /metrics endpoint so Centrifugo instance can be monitored by your Prometheus server. Graphite \u00b6 To enable automatic export to Graphite (via TCP): { \"graphite\" : true , \"graphite_host\" : \"localhost\" , \"graphite_port\" : 2003 } By default stats will be aggregated over 10 seconds interval inside Centrifugo and then pushed to Graphite over TCP connection. If you need to change this aggregation interval use graphite_interval option (in seconds, default 10 ). This option available since v2.1.0","title":"Monitoring"},{"location":"deploy/monitoring/#monitoring","text":"Centrifugo supports reporting metrics in Prometheus format and can automatically export metrics to Graphite.","title":"Monitoring"},{"location":"deploy/monitoring/#prometheus","text":"To enable Prometheus endpoint start Centrifugo with prometheus option on: { ... \"prometheus\" : true } ./centrifugo --config=config.json This will enable /metrics endpoint so Centrifugo instance can be monitored by your Prometheus server.","title":"Prometheus"},{"location":"deploy/monitoring/#graphite","text":"To enable automatic export to Graphite (via TCP): { \"graphite\" : true , \"graphite_host\" : \"localhost\" , \"graphite_port\" : 2003 } By default stats will be aggregated over 10 seconds interval inside Centrifugo and then pushed to Graphite over TCP connection. If you need to change this aggregation interval use graphite_interval option (in seconds, default 10 ). This option available since v2.1.0","title":"Graphite"},{"location":"deploy/nginx/","text":"Nginx configuration \u00b6 Although it's possible to use Centrifugo without any reverse proxy before it, it's still a good idea to keep Centrifugo behind mature reverse proxy to deal with edge cases when handling HTTP/Websocket connections from the wild. Also you probably want some sort of load balancing eventually between Centrifugo nodes so that proxy can be such a balancer too. In this section we will look at Nginx configuration to deploy Centrifugo. Minimal Nginx version \u2013 1.3.13 because it was the first version that can proxy Websocket connections. There are 2 ways: running Centrifugo server as separate service on its own domain or embed it to a location of your web site (for example to /centrifugo ). Separate domain for Centrifugo \u00b6 upstream centrifugo { # Enumerate all upstream servers here #sticky; ip_hash; server 127.0.0.1:8000; #server 127.0.0.1:8001; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } #server { # listen 80; # server_name centrifugo.example.com; # rewrite ^(.*) https://$server_name$1 permanent; #} server { server_name centrifugo.example.com; listen 80; #listen 443; #ssl on; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_ciphers AES128-SHA:AES256-SHA:RC4-SHA:DES-CBC3-SHA:RC4-MD5; #ssl_certificate /etc/nginx/ssl/wildcard.example.com.crt; #ssl_certificate_key /etc/nginx/ssl/wildcard.example.com.key; #ssl_session_cache shared:SSL:10m;ssl_session_timeout 10m; include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_min_length 1000; gzip_proxied any; # Only retry if there was a communication error, not a timeout # on the Tornado server (to avoid propagating \"queries of death\" # to all frontends) proxy_next_upstream error; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; location /connection { proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_http_version 1.1; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location / { proxy_pass http://centrifugo; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Embed to a location of web site \u00b6 upstream centrifugo { # Enumerate all the Tornado servers here #sticky; ip_hash; server 127.0.0.1:8000; #server 127.0.0.1:8001; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { # ... your web site Nginx config location /centrifugo/ { rewrite ^/centrifugo/(.*) /$1 break; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://centrifugo; } location /centrifugo/connection { rewrite ^/centrifugo(.*) $1 break; proxy_next_upstream error; gzip on; gzip_min_length 1000; gzip_proxied any; proxy_buffering off; keepalive_timeout 65; proxy_pass http://centrifugo; proxy_read_timeout 60s; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } sticky \u00b6 You may be noticed commented sticky; directive in nginx upstream configuration. When using SockJS and client connects to Centrifugo - SockJS session created - and to communicate client must send all next requests to the same upstream backend. In this configuration we use ip_hash; directive to proxy clients with the same ip address to the same upstream backend. But ip_hash; is not the best choice in this case, because there could be situations where a lot of different browsers are coming with the same IP address (behind proxies) and the load balancing system won't be fair. Also fair load balancing does not work during development - when all clients connecting from localhost. So the best solution would be using something like nginx-sticky-module which uses setting a special cookie to track the upstream server for client. worker_connections \u00b6 You may also need to update worker_connections option of Nginx: events { worker_connections 40000; } Upstream keepalive \u00b6 See chapter about operating system tuning for more details.","title":"Nginx configuration"},{"location":"deploy/nginx/#nginx-configuration","text":"Although it's possible to use Centrifugo without any reverse proxy before it, it's still a good idea to keep Centrifugo behind mature reverse proxy to deal with edge cases when handling HTTP/Websocket connections from the wild. Also you probably want some sort of load balancing eventually between Centrifugo nodes so that proxy can be such a balancer too. In this section we will look at Nginx configuration to deploy Centrifugo. Minimal Nginx version \u2013 1.3.13 because it was the first version that can proxy Websocket connections. There are 2 ways: running Centrifugo server as separate service on its own domain or embed it to a location of your web site (for example to /centrifugo ).","title":"Nginx configuration"},{"location":"deploy/nginx/#separate-domain-for-centrifugo","text":"upstream centrifugo { # Enumerate all upstream servers here #sticky; ip_hash; server 127.0.0.1:8000; #server 127.0.0.1:8001; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } #server { # listen 80; # server_name centrifugo.example.com; # rewrite ^(.*) https://$server_name$1 permanent; #} server { server_name centrifugo.example.com; listen 80; #listen 443; #ssl on; #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #ssl_ciphers AES128-SHA:AES256-SHA:RC4-SHA:DES-CBC3-SHA:RC4-MD5; #ssl_certificate /etc/nginx/ssl/wildcard.example.com.crt; #ssl_certificate_key /etc/nginx/ssl/wildcard.example.com.key; #ssl_session_cache shared:SSL:10m;ssl_session_timeout 10m; include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_min_length 1000; gzip_proxied any; # Only retry if there was a communication error, not a timeout # on the Tornado server (to avoid propagating \"queries of death\" # to all frontends) proxy_next_upstream error; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; location /connection { proxy_pass http://centrifugo; proxy_buffering off; keepalive_timeout 65; proxy_read_timeout 60s; proxy_http_version 1.1; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location / { proxy_pass http://centrifugo; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } }","title":"Separate domain for Centrifugo"},{"location":"deploy/nginx/#embed-to-a-location-of-web-site","text":"upstream centrifugo { # Enumerate all the Tornado servers here #sticky; ip_hash; server 127.0.0.1:8000; #server 127.0.0.1:8001; } map $http_upgrade $connection_upgrade { default upgrade; '' close; } server { # ... your web site Nginx config location /centrifugo/ { rewrite ^/centrifugo/(.*) /$1 break; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://centrifugo; } location /centrifugo/connection { rewrite ^/centrifugo(.*) $1 break; proxy_next_upstream error; gzip on; gzip_min_length 1000; gzip_proxied any; proxy_buffering off; keepalive_timeout 65; proxy_pass http://centrifugo; proxy_read_timeout 60s; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }","title":"Embed to a location of web site"},{"location":"deploy/nginx/#sticky","text":"You may be noticed commented sticky; directive in nginx upstream configuration. When using SockJS and client connects to Centrifugo - SockJS session created - and to communicate client must send all next requests to the same upstream backend. In this configuration we use ip_hash; directive to proxy clients with the same ip address to the same upstream backend. But ip_hash; is not the best choice in this case, because there could be situations where a lot of different browsers are coming with the same IP address (behind proxies) and the load balancing system won't be fair. Also fair load balancing does not work during development - when all clients connecting from localhost. So the best solution would be using something like nginx-sticky-module which uses setting a special cookie to track the upstream server for client.","title":"sticky"},{"location":"deploy/nginx/#worker_connections","text":"You may also need to update worker_connections option of Nginx: events { worker_connections 40000; }","title":"worker_connections"},{"location":"deploy/nginx/#upstream-keepalive","text":"See chapter about operating system tuning for more details.","title":"Upstream keepalive"},{"location":"deploy/tls/","text":"TLS \u00b6 TLS/SSL layer is very important not only for securing your connections but also to increase a chance to establish Websocket connection. In most situations you will put TLS termination task on your reverse proxy/load balancing software such as Nginx . There are situations though when you want to serve secure connections by Centrifugo itself. There are two ways to do this: using TLS certificate cert and key files that you've got from your CA provider or using automatic certificate handling via ACME provider (only Let's Encrypt at this moment). Using crt and key files \u00b6 In first way you already have cert and key files. For development you can create self-signed certificate - see this instruction as example. Then to start Centrifugo use the following command: ./centrifugo --config=config.json --tls --tls_key=server.key --tls_cert=server.crt Or just use configuration file: { ... \"tls\" : true , \"tls_key\" : \"server.key\" , \"tls_cert\" : \"server.crt\" } And run: ./centrifugo --config=config.json Automatic certificates \u00b6 For automatic certificates from Let's Encrypt add into configuration file: { ... \"tls_autocert\": true, \"tls_autocert_host_whitelist\": \"www.example.com\", \"tls_autocert_cache_dir\": \"/tmp/certs\", \"tls_autocert_email\": \"user@example.com\", \"tls_autocert_http\": true, \"tls_autocert_http_addr\": \":80\" } tls_autocert (boolean) says Centrifugo that you want automatic certificate handling using ACME provider. tls_autocert_host_whitelist (string) is a string with your app domain address. This can be comma-separated list. It's optional but recommended for extra security. tls_autocert_cache_dir (string) is a path to a folder to cache issued certificate files. This is optional but will increase performance. tls_autocert_email (string) is optional - it's an email address ACME provider will send notifications about problems with your certificates. tls_autocert_http (boolean) is an option to handle http_01 ACME challenge on non-TLS port. tls_autocert_http_addr (string) can be used to set address for handling http_01 ACME challenge (default is :80 ) When configured correctly and your domain is valid ( localhost will not work) - certificates will be retrieved on first request to Centrifugo. Also Let's Encrypt certificates will be automatically renewed. There are tho options that allow Centrifugo to support TLS client connections from older browsers such as Chrome 49 on Windows XP and IE8 on XP: tls_autocert_force_rsa - this is a boolean option, by default false . When enabled it forces autocert manager generate certificates with 2048-bit RSA keys. tls_autocert_server_name - string option, allows to set server name for client handshake hello. This can be useful to deal with old browsers without SNI support - see comment grpc_api_tls_disable boolean flag allows to disable TLS for GRPC API server but keep it on for HTTP endpoints. Custom TLS for GRPC API \u00b6 Starting from Centrifugo v2.2.5 you can provide custom certificate files to configure TLS for GRPC API server in custom way. grpc_api_tls boolean flag enables TLS for GRPC API server, requires an X509 certificate and a key file grpc_api_tls_cert string provides a path to an X509 certificate file for GRPC API server grpc_api_tls_key string provides a path to an X509 certificate key for GRPC API server","title":"TLS"},{"location":"deploy/tls/#tls","text":"TLS/SSL layer is very important not only for securing your connections but also to increase a chance to establish Websocket connection. In most situations you will put TLS termination task on your reverse proxy/load balancing software such as Nginx . There are situations though when you want to serve secure connections by Centrifugo itself. There are two ways to do this: using TLS certificate cert and key files that you've got from your CA provider or using automatic certificate handling via ACME provider (only Let's Encrypt at this moment).","title":"TLS"},{"location":"deploy/tls/#using-crt-and-key-files","text":"In first way you already have cert and key files. For development you can create self-signed certificate - see this instruction as example. Then to start Centrifugo use the following command: ./centrifugo --config=config.json --tls --tls_key=server.key --tls_cert=server.crt Or just use configuration file: { ... \"tls\" : true , \"tls_key\" : \"server.key\" , \"tls_cert\" : \"server.crt\" } And run: ./centrifugo --config=config.json","title":"Using crt and key files"},{"location":"deploy/tls/#automatic-certificates","text":"For automatic certificates from Let's Encrypt add into configuration file: { ... \"tls_autocert\": true, \"tls_autocert_host_whitelist\": \"www.example.com\", \"tls_autocert_cache_dir\": \"/tmp/certs\", \"tls_autocert_email\": \"user@example.com\", \"tls_autocert_http\": true, \"tls_autocert_http_addr\": \":80\" } tls_autocert (boolean) says Centrifugo that you want automatic certificate handling using ACME provider. tls_autocert_host_whitelist (string) is a string with your app domain address. This can be comma-separated list. It's optional but recommended for extra security. tls_autocert_cache_dir (string) is a path to a folder to cache issued certificate files. This is optional but will increase performance. tls_autocert_email (string) is optional - it's an email address ACME provider will send notifications about problems with your certificates. tls_autocert_http (boolean) is an option to handle http_01 ACME challenge on non-TLS port. tls_autocert_http_addr (string) can be used to set address for handling http_01 ACME challenge (default is :80 ) When configured correctly and your domain is valid ( localhost will not work) - certificates will be retrieved on first request to Centrifugo. Also Let's Encrypt certificates will be automatically renewed. There are tho options that allow Centrifugo to support TLS client connections from older browsers such as Chrome 49 on Windows XP and IE8 on XP: tls_autocert_force_rsa - this is a boolean option, by default false . When enabled it forces autocert manager generate certificates with 2048-bit RSA keys. tls_autocert_server_name - string option, allows to set server name for client handshake hello. This can be useful to deal with old browsers without SNI support - see comment grpc_api_tls_disable boolean flag allows to disable TLS for GRPC API server but keep it on for HTTP endpoints.","title":"Automatic certificates"},{"location":"deploy/tls/#custom-tls-for-grpc-api","text":"Starting from Centrifugo v2.2.5 you can provide custom certificate files to configure TLS for GRPC API server in custom way. grpc_api_tls boolean flag enables TLS for GRPC API server, requires an X509 certificate and a key file grpc_api_tls_cert string provides a path to an X509 certificate file for GRPC API server grpc_api_tls_key string provides a path to an X509 certificate key for GRPC API server","title":"Custom TLS for GRPC API"},{"location":"deploy/tuning/","text":"Tuning operating system \u00b6 As Centrifugo/Centrifuge deals with lots of persistent connections your operating system must be ready for it. open files limit \u00b6 First of all you should increase a max number of open files your processes can open. To get you current open files limit run: ulimit -n The result shows approximately how many clients your server can handle. See this document to get more info on how to increase this number. If you install Centrifugo using RPM from repo then it automatically sets max open files limit to 65536. You may also need to increase max open files for Nginx. lots of sockets in TIME_WAIT state. \u00b6 Look how many socket descriptors in TIME_WAIT state. netstat -an |grep TIME_WAIT | grep CENTRIFUGO_PID | wc -l Under load when lots of connections and disconnection happen lots of used socket descriptors can stay in TIME_WAIT state. Those descriptors can not be reused for a while. So you can get various errors when using Centrifugo. For example something like (99: Cannot assign requested address) while connecting to upstream in Nginx error log and 502 on client side. In this case there are several advices that can help. Nice article about TIME_WAIT sockets: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html There is a perfect article about operating system tuning for lots of connections: https://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads . To summarize: Increase ip_local_port_range If you are using Nginx set keepalive directive in upstream. upstream centrifugo { #sticky; ip_hash; server 127.0.0.1:8000; keepalive 512; } And finally if the problem is not gone away consider trying to enable net.ipv4.tcp_tw_reuse","title":"OS tuning"},{"location":"deploy/tuning/#tuning-operating-system","text":"As Centrifugo/Centrifuge deals with lots of persistent connections your operating system must be ready for it.","title":"Tuning operating system"},{"location":"deploy/tuning/#open-files-limit","text":"First of all you should increase a max number of open files your processes can open. To get you current open files limit run: ulimit -n The result shows approximately how many clients your server can handle. See this document to get more info on how to increase this number. If you install Centrifugo using RPM from repo then it automatically sets max open files limit to 65536. You may also need to increase max open files for Nginx.","title":"open files limit"},{"location":"deploy/tuning/#lots-of-sockets-in-time_wait-state","text":"Look how many socket descriptors in TIME_WAIT state. netstat -an |grep TIME_WAIT | grep CENTRIFUGO_PID | wc -l Under load when lots of connections and disconnection happen lots of used socket descriptors can stay in TIME_WAIT state. Those descriptors can not be reused for a while. So you can get various errors when using Centrifugo. For example something like (99: Cannot assign requested address) while connecting to upstream in Nginx error log and 502 on client side. In this case there are several advices that can help. Nice article about TIME_WAIT sockets: http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html There is a perfect article about operating system tuning for lots of connections: https://engineering.gosquared.com/optimising-nginx-node-js-and-networking-for-heavy-workloads . To summarize: Increase ip_local_port_range If you are using Nginx set keepalive directive in upstream. upstream centrifugo { #sticky; ip_hash; server 127.0.0.1:8000; keepalive 512; } And finally if the problem is not gone away consider trying to enable net.ipv4.tcp_tw_reuse","title":"lots of sockets in TIME_WAIT state."},{"location":"libraries/api/","text":"HTTP API clients \u00b6 If you look at server API docs you will find that sending API request to Centrifugo is a very simple task to do in any programming language - this is just a POST request with JSON payload in body and Authorization header. See more in special chapter in server section. We have several official client libraries for different languages so you don't have to construct proper HTTP requests manually: cent for Python phpcent for PHP gocent for Go rubycent for Ruby ( not available for Centrifugo v2 yet ) jscent for NodeJS ( not available for Centrifugo v2 yet ) Also there are libraries supported by community: laracent for Laravel framework crystalcent for Crystal language CentrifugoBundle for Symfony framework Also, keep in mind that Centrifugo has GRPC API so you can automatically generate client API code for your language.","title":"API libraries"},{"location":"libraries/api/#http-api-clients","text":"If you look at server API docs you will find that sending API request to Centrifugo is a very simple task to do in any programming language - this is just a POST request with JSON payload in body and Authorization header. See more in special chapter in server section. We have several official client libraries for different languages so you don't have to construct proper HTTP requests manually: cent for Python phpcent for PHP gocent for Go rubycent for Ruby ( not available for Centrifugo v2 yet ) jscent for NodeJS ( not available for Centrifugo v2 yet ) Also there are libraries supported by community: laracent for Laravel framework crystalcent for Crystal language CentrifugoBundle for Symfony framework Also, keep in mind that Centrifugo has GRPC API so you can automatically generate client API code for your language.","title":"HTTP API clients"},{"location":"libraries/client/","text":"Client libraries \u00b6 These libraries allow your users to connect to Centrifugo from application frontend. centrifuge-js \u2013 for browser, NodeJS and React Native centrifuge-go - for Go language centrifuge-mobile - for iOS and Android using centrifuge-go as basis and gomobile project to create bindings centrifuge-dart - for Dart and Flutter centrifuge-swift \u2013 for native iOS development centrifuge-java \u2013 for native Android development and general Java","title":"Client libraries"},{"location":"libraries/client/#client-libraries","text":"These libraries allow your users to connect to Centrifugo from application frontend. centrifuge-js \u2013 for browser, NodeJS and React Native centrifuge-go - for Go language centrifuge-mobile - for iOS and Android using centrifuge-go as basis and gomobile project to create bindings centrifuge-dart - for Dart and Flutter centrifuge-swift \u2013 for native iOS development centrifuge-java \u2013 for native Android development and general Java","title":"Client libraries"},{"location":"misc/benchmark/","text":"Benchmark \u00b6 In order to get an understanding about possible hardware requirements for reasonably massive Centrifugo setup we made a test stand inside Kubernetes. Our goal was to run server based on Centrifuge library (the core of Centrifugo server) with one million WebSocket connections and send many messages to connected clients. While sending many messages we have been looking at delivery time latency. In fact we will see that about 30 million messages per minute (500k messages per second) will be delivered to connected clients and latency won't be larger than 200ms in 99 percentile. Server nodes have been run on machines with the following configuration: CPU Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz Linux Debian 4.9.65-3+deb9u1 (2017-12-23) x86_64 GNU/Linux Some sysctl values: fs.file-max = 3276750 fs.nr_open = 1048576 net.ipv4.tcp_mem = 3086496 4115330 6172992 net.ipv4.tcp_rmem = 8192 8388608 16777216 net.ipv4.tcp_wmem = 4096 4194394 16777216 net.core.rmem_max = 33554432 net.core.wmem_max = 33554432 Kubernetes used these machines as its nodes. We started 20 Centrifuge-based server pods. Our clients connected to server pods using Centrifuge Protobuf protocol. To scale horizontally we used Redis Engine and sharded it to 5 different Redis instances (each Redis instance consumes 1 CPU max). To achieve many client connections we used 100 Kubernetes pods each generating about 10k client connections to server. Here are some numbers we achieved: 1 million WebSocket connections Each connection subscribed to 2 channels: one personal channel and one group channel (with 10 subscribers in it), i.e. we had about 1.1 million active channels at each moment. 28 million messages per minute (about 500k per second) delivered to clients 200k per minute constant connect/disconnect rate to simulate real-life situation where clients connect/disconnect from server 200ms delivery latency in 99 percentile The size of each published message was about 100kb And here are some numbers about final resource usage on server side (we don't actually interested in client side resource usage here): 40 CPU total for server nodes when load achieved values claimed above (20 pods, ~2 CPU each) 27 GB of RAM used mostly to handle 1 mln WebSocket connections, i.e. about 30kb RAM per connection 0.32 CPU usage on every Redis instance 100 mbit/sec rx \u0438 150 mbit/sec tx of network used on each server pod The picture that demonstrates experiment (better to open image in new tab): This also demonstrates that to handle one million of WebSocket connections without many messages sent to clients you need about 10 CPU total for server nodes and about 5% of CPU on each of Redis instances. In this case CPU mostly spent on connect/disconnect flow, ping/pong frames, subscriptions to channels. If we enable history and history message recovery features we see an increased Redis CPU usage: 64% instead of 32% on the same workload. Other resources usage is pretty the same. The results mean that one can theoretically achieve the comparable numbers on single modern server machine. But numbers can vary a lot in case of different load scenarios. In this benchmark we looked at basic use case where we only connect many clients and send Publications to them. There are many features in Centrifuge library and in Centrifugo not covered by this artificial experiment. Also note that though benchmark was made for Centrifuge library for Centrifugo you can expect similar results. Read and write buffer sizes of websocket connections were set to 512 kb on server side (sizes of buffers affect memory usage), with Centrifugo this means that to reproduce the same configuration you need to set: { ... \"websocket_read_buffer_size\" : 512 , \"websocket_write_buffer_size\" : 512 }","title":"Benchmarking server"},{"location":"misc/benchmark/#benchmark","text":"In order to get an understanding about possible hardware requirements for reasonably massive Centrifugo setup we made a test stand inside Kubernetes. Our goal was to run server based on Centrifuge library (the core of Centrifugo server) with one million WebSocket connections and send many messages to connected clients. While sending many messages we have been looking at delivery time latency. In fact we will see that about 30 million messages per minute (500k messages per second) will be delivered to connected clients and latency won't be larger than 200ms in 99 percentile. Server nodes have been run on machines with the following configuration: CPU Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz Linux Debian 4.9.65-3+deb9u1 (2017-12-23) x86_64 GNU/Linux Some sysctl values: fs.file-max = 3276750 fs.nr_open = 1048576 net.ipv4.tcp_mem = 3086496 4115330 6172992 net.ipv4.tcp_rmem = 8192 8388608 16777216 net.ipv4.tcp_wmem = 4096 4194394 16777216 net.core.rmem_max = 33554432 net.core.wmem_max = 33554432 Kubernetes used these machines as its nodes. We started 20 Centrifuge-based server pods. Our clients connected to server pods using Centrifuge Protobuf protocol. To scale horizontally we used Redis Engine and sharded it to 5 different Redis instances (each Redis instance consumes 1 CPU max). To achieve many client connections we used 100 Kubernetes pods each generating about 10k client connections to server. Here are some numbers we achieved: 1 million WebSocket connections Each connection subscribed to 2 channels: one personal channel and one group channel (with 10 subscribers in it), i.e. we had about 1.1 million active channels at each moment. 28 million messages per minute (about 500k per second) delivered to clients 200k per minute constant connect/disconnect rate to simulate real-life situation where clients connect/disconnect from server 200ms delivery latency in 99 percentile The size of each published message was about 100kb And here are some numbers about final resource usage on server side (we don't actually interested in client side resource usage here): 40 CPU total for server nodes when load achieved values claimed above (20 pods, ~2 CPU each) 27 GB of RAM used mostly to handle 1 mln WebSocket connections, i.e. about 30kb RAM per connection 0.32 CPU usage on every Redis instance 100 mbit/sec rx \u0438 150 mbit/sec tx of network used on each server pod The picture that demonstrates experiment (better to open image in new tab): This also demonstrates that to handle one million of WebSocket connections without many messages sent to clients you need about 10 CPU total for server nodes and about 5% of CPU on each of Redis instances. In this case CPU mostly spent on connect/disconnect flow, ping/pong frames, subscriptions to channels. If we enable history and history message recovery features we see an increased Redis CPU usage: 64% instead of 32% on the same workload. Other resources usage is pretty the same. The results mean that one can theoretically achieve the comparable numbers on single modern server machine. But numbers can vary a lot in case of different load scenarios. In this benchmark we looked at basic use case where we only connect many clients and send Publications to them. There are many features in Centrifuge library and in Centrifugo not covered by this artificial experiment. Also note that though benchmark was made for Centrifuge library for Centrifugo you can expect similar results. Read and write buffer sizes of websocket connections were set to 512 kb on server side (sizes of buffers affect memory usage), with Centrifugo this means that to reproduce the same configuration you need to set: { ... \"websocket_read_buffer_size\" : 512 , \"websocket_write_buffer_size\" : 512 }","title":"Benchmark"},{"location":"misc/insecure_modes/","text":"Insecure modes \u00b6 This chapter describes several insecure options that enable several insecure modes in Centrifugo. Insecure client connection \u00b6 The boolean option client_insecure (default false ) allows to connect to Centrifugo without JWT token. This means there is no user authentication involved. This mode can be useful to demo projects based on Centrifugo, personal projects or real-time application prototyping. Insecure API mode \u00b6 This mode can be enabled using boolean option api_insecure (default false ). When on there is no need to provide API key in HTTP requests. When using this mode everyone that has access to /api endpoint can send any command to server. Enabling this option can be reasonable if /api endpoint protected by firewall rules. The option is also useful in development to simplify sending API commands to Centrifugo using CURL for example without specifying Authorization header in requests. Insecure admin mode \u00b6 This mode can be enabled using boolean option admin_insecure (default false ). When on there is no authentication in admin web interface. Again - this is not secure but can be justified if you protected admin interface by firewall rules or you want to use basic authentication for Centrifugo admin interface.","title":"Insecure modes"},{"location":"misc/insecure_modes/#insecure-modes","text":"This chapter describes several insecure options that enable several insecure modes in Centrifugo.","title":"Insecure modes"},{"location":"misc/insecure_modes/#insecure-client-connection","text":"The boolean option client_insecure (default false ) allows to connect to Centrifugo without JWT token. This means there is no user authentication involved. This mode can be useful to demo projects based on Centrifugo, personal projects or real-time application prototyping.","title":"Insecure client connection"},{"location":"misc/insecure_modes/#insecure-api-mode","text":"This mode can be enabled using boolean option api_insecure (default false ). When on there is no need to provide API key in HTTP requests. When using this mode everyone that has access to /api endpoint can send any command to server. Enabling this option can be reasonable if /api endpoint protected by firewall rules. The option is also useful in development to simplify sending API commands to Centrifugo using CURL for example without specifying Authorization header in requests.","title":"Insecure API mode"},{"location":"misc/insecure_modes/#insecure-admin-mode","text":"This mode can be enabled using boolean option admin_insecure (default false ). When on there is no authentication in admin web interface. Again - this is not secure but can be justified if you protected admin interface by firewall rules or you want to use basic authentication for Centrifugo admin interface.","title":"Insecure admin mode"},{"location":"misc/migrate/","text":"Migration notes from Centrifugo v1 \u00b6 In version 2 of Centrifugo many things changed in backwards incompatible way comparing to version 1. This document aims to help Centrifugo v1 users to migrate their projects to version 2 (if they want to). New client protocol and client libraries \u00b6 In Centrifugo v2 internal client-server protocol changed meaning that old client library version won't work with new server. So first step in migrating - update client libraries to new version with Centrifugo v2 support. While refactoring client's API changed a bit so you have to adapt your code to those changes. For the moment of this writing we have no native mobile libraries for Centrifugo v2. So if you are using centrifuge-ios or centrifuge-android then you can't migrate to v2 until those libraries will be ported. Migrate communication with API \u00b6 Centrifugo v2 simplified communication with API - requests should not be signed with secret key anymore thus you can simply integrate your backend with Centrifugo without using any of our helper libraries - just send JSON API command as POST request to api endpoint. Don't forget to use api key and protect API endpoint with TLS (more information in server API description document). Centrifugo v1 could process messages published in Redis queue. In v2 this possibility was removed because this technique is not good in terms of error handling and non-deterministic delay before message will be processed by Centrifugo node worker. Migrate to using HTTP or GRPC API. Use JWT instead of hand-crafted connection token \u00b6 In Centrifugo v2 you must use JWT instead of hand-crafted tokens of v1. This means that you need to download JWT library for your language (there are plenty of them \u2013 see jwt.io) and build connection token with it. See dedicated docs chapter to see how token can be built. All connection information will be passed inside this single token string. This means you only need to pass one string to your frontend. No need to pass user , timestamp , info anymore. This also means that you will have less problems with escaping features of template engines - because JWT is safe base64 string. Connection expiration (connection check mechanism) now based on exp claim of JWT \u2013 you don't need to enable it globally in configuration. Use JWT instead of hand-crafted signature for private subscriptions \u00b6 Read chapter about private subscriptions to find how you should now use JWT for private channel subscriptions. Channel options changed \u00b6 Channel option recover now called history_recover . There is no watch channel option anymore - in Centrifugo v2 admin websocket connection was removed as it made code base much more overhelmed for almost nothing. SockJS endpoint changed \u00b6 It's now /connection/sockjs instead of /connection New way to export metrics \u00b6 Centrifugo is now uses Prometheus primitives internally so if you are using Prometheus you can simply configure it to monitor Centrifugo. Also Centrifugo is able to automatically convert and export metrics to Graphite. See special Monitoring chapter in server docs. Previously you have to periodically call stats command and export metrics manually. This is gone in Centrifugo v2. Options renamed \u00b6 Some of advanced options have been renamed \u2013 if you are using advanced configuration then refer to documentation to find actual option names. No client limited channels anymore \u00b6 That was a pretty useless feature of Centrifugo v1. New reserved symbols in channel name \u00b6 Symbols * and / in channel name are now reserved for Centrifugo future needs - please do not use it in channels. Centrifugo v1 repos \u00b6 Here some links for those who still use Centrifugo v1 Centrifugo v1 source code Centrifugo v1 documentation centrifuge-js v1 centrifuge-go centrifuge-mobile examples","title":"Migrate from Centrifugo v1"},{"location":"misc/migrate/#migration-notes-from-centrifugo-v1","text":"In version 2 of Centrifugo many things changed in backwards incompatible way comparing to version 1. This document aims to help Centrifugo v1 users to migrate their projects to version 2 (if they want to).","title":"Migration notes from Centrifugo v1"},{"location":"misc/migrate/#new-client-protocol-and-client-libraries","text":"In Centrifugo v2 internal client-server protocol changed meaning that old client library version won't work with new server. So first step in migrating - update client libraries to new version with Centrifugo v2 support. While refactoring client's API changed a bit so you have to adapt your code to those changes. For the moment of this writing we have no native mobile libraries for Centrifugo v2. So if you are using centrifuge-ios or centrifuge-android then you can't migrate to v2 until those libraries will be ported.","title":"New client protocol and client libraries"},{"location":"misc/migrate/#migrate-communication-with-api","text":"Centrifugo v2 simplified communication with API - requests should not be signed with secret key anymore thus you can simply integrate your backend with Centrifugo without using any of our helper libraries - just send JSON API command as POST request to api endpoint. Don't forget to use api key and protect API endpoint with TLS (more information in server API description document). Centrifugo v1 could process messages published in Redis queue. In v2 this possibility was removed because this technique is not good in terms of error handling and non-deterministic delay before message will be processed by Centrifugo node worker. Migrate to using HTTP or GRPC API.","title":"Migrate communication with API"},{"location":"misc/migrate/#use-jwt-instead-of-hand-crafted-connection-token","text":"In Centrifugo v2 you must use JWT instead of hand-crafted tokens of v1. This means that you need to download JWT library for your language (there are plenty of them \u2013 see jwt.io) and build connection token with it. See dedicated docs chapter to see how token can be built. All connection information will be passed inside this single token string. This means you only need to pass one string to your frontend. No need to pass user , timestamp , info anymore. This also means that you will have less problems with escaping features of template engines - because JWT is safe base64 string. Connection expiration (connection check mechanism) now based on exp claim of JWT \u2013 you don't need to enable it globally in configuration.","title":"Use JWT instead of hand-crafted connection token"},{"location":"misc/migrate/#use-jwt-instead-of-hand-crafted-signature-for-private-subscriptions","text":"Read chapter about private subscriptions to find how you should now use JWT for private channel subscriptions.","title":"Use JWT instead of hand-crafted signature for private subscriptions"},{"location":"misc/migrate/#channel-options-changed","text":"Channel option recover now called history_recover . There is no watch channel option anymore - in Centrifugo v2 admin websocket connection was removed as it made code base much more overhelmed for almost nothing.","title":"Channel options changed"},{"location":"misc/migrate/#sockjs-endpoint-changed","text":"It's now /connection/sockjs instead of /connection","title":"SockJS endpoint changed"},{"location":"misc/migrate/#new-way-to-export-metrics","text":"Centrifugo is now uses Prometheus primitives internally so if you are using Prometheus you can simply configure it to monitor Centrifugo. Also Centrifugo is able to automatically convert and export metrics to Graphite. See special Monitoring chapter in server docs. Previously you have to periodically call stats command and export metrics manually. This is gone in Centrifugo v2.","title":"New way to export metrics"},{"location":"misc/migrate/#options-renamed","text":"Some of advanced options have been renamed \u2013 if you are using advanced configuration then refer to documentation to find actual option names.","title":"Options renamed"},{"location":"misc/migrate/#no-client-limited-channels-anymore","text":"That was a pretty useless feature of Centrifugo v1.","title":"No client limited channels anymore"},{"location":"misc/migrate/#new-reserved-symbols-in-channel-name","text":"Symbols * and / in channel name are now reserved for Centrifugo future needs - please do not use it in channels.","title":"New reserved symbols in channel name"},{"location":"misc/migrate/#centrifugo-v1-repos","text":"Here some links for those who still use Centrifugo v1 Centrifugo v1 source code Centrifugo v1 documentation centrifuge-js v1 centrifuge-go centrifuge-mobile examples","title":"Centrifugo v1 repos"},{"location":"server/admin/","text":"Admin web interface \u00b6 Centrifugo comes with builtin admin web interface. It can: show current server general information and statistics from server nodes. call publish , broadcast , unsubscribe , disconnect , history , presence , presence_stats , channels , info server API commands. For publish command Ace JSON editor helps to write JSON to send into channel. To enable admin interface you must run centrifugo with --admin and provide some security options in configuration file. centrifugo --config = config.json --admin Also you must set two options in config: admin_password and admin_secret : { ..., \"admin_password\" : \"<PASSWORD>\" , \"admin_secret\" : \"<SECRET>\" } admin_password \u2013 this is a password to log into admin web interface admin_secret - this is a secret key for authentication token set on successful login. Make both strong and keep in secret. After setting this in config go to http://localhost:8000 (by default) - and you should see web interface. Although there is password based authentication a good advice is to protect web interface by firewall rules in production. If you don't want to use embedded web interface you can specify path to your own custom web interface directory: { ..., \"admin_password\" : \"<PASSWORD>\" , \"admin_secret\" : \"<SECRET>\" , \"admin_web_path\" : \"<PATH>\" } This can be useful if you want to modify official web interface code in some way. There is also an option to run Centrifugo in insecure admin mode - in this case you don't need to set admin_password and admin_secret in config \u2013 in web interface you will be logged in automatically without any password. Note that this is only an option for production if you protected admin web interface with firewall rules. Otherwise anyone in internet will have full access to admin functionality described above. To start Centrifugo with admin web interface in insecure admin mode run: centrifugo --config=config.json --admin --admin_insecure","title":"Admin web interface"},{"location":"server/admin/#admin-web-interface","text":"Centrifugo comes with builtin admin web interface. It can: show current server general information and statistics from server nodes. call publish , broadcast , unsubscribe , disconnect , history , presence , presence_stats , channels , info server API commands. For publish command Ace JSON editor helps to write JSON to send into channel. To enable admin interface you must run centrifugo with --admin and provide some security options in configuration file. centrifugo --config = config.json --admin Also you must set two options in config: admin_password and admin_secret : { ..., \"admin_password\" : \"<PASSWORD>\" , \"admin_secret\" : \"<SECRET>\" } admin_password \u2013 this is a password to log into admin web interface admin_secret - this is a secret key for authentication token set on successful login. Make both strong and keep in secret. After setting this in config go to http://localhost:8000 (by default) - and you should see web interface. Although there is password based authentication a good advice is to protect web interface by firewall rules in production. If you don't want to use embedded web interface you can specify path to your own custom web interface directory: { ..., \"admin_password\" : \"<PASSWORD>\" , \"admin_secret\" : \"<SECRET>\" , \"admin_web_path\" : \"<PATH>\" } This can be useful if you want to modify official web interface code in some way. There is also an option to run Centrifugo in insecure admin mode - in this case you don't need to set admin_password and admin_secret in config \u2013 in web interface you will be logged in automatically without any password. Note that this is only an option for production if you protected admin web interface with firewall rules. Otherwise anyone in internet will have full access to admin functionality described above. To start Centrifugo with admin web interface in insecure admin mode run: centrifugo --config=config.json --admin --admin_insecure","title":"Admin web interface"},{"location":"server/authentication/","text":"Authentication \u00b6 When you are using centrifuge library from Go language you can implement any user authentication using middleware. In Centrifugo case you need to tell server who is connecting in well-known predefined way. This chapter will describe this mechanism. When connecting to Centrifugo client must provide connection JWT token with several predefined credential claims. If you've never heard about JWT before - refer to jwt.io page. At moment **the only supported JWT algorithms are HMAC and RSA ** - i.e. HS256, HS384, HS512, RSA256, RSA384, RSA512. This can be extended later. RSA algorithm is available since v2.3.0 release. We will use Javascript Centrifugo client here for example snippets for client side and PyJWT Python library to generate connection token on backend side. To add HMAC secret key to Centrifugo add token_hmac_secret_key to configuration file: { \"token_hmac_secret_key\" : \"<YOUR-SECRET-STRING-HERE>\" , ... } To add RSA public key (must be PEM encoded string) add token_rsa_public_key option, ex: { \"token_rsa_public_key\" : \"-----BEGIN PUBLIC KEY-----\\nMFwwDQYJKoZ...\" , ... } Claims \u00b6 Centrifugo uses the following claims in JWT: sub , exp , info and b64info . What do they mean? Let's describe in detail. sub \u00b6 This is a standard JWT claim which must contain an ID of current application user ( as string ). If your user is not currently authenticated in your application but you want to let him connect to Centrifugo anyway you can use empty string as user ID in this sub claim. This is called anonymous access. In this case anonymous option must be enabled in Centrifugo configuration for channels that client will subscribe to. exp \u00b6 This is an a UNIX timestamp seconds when token will expire. This is standard JWT claim - all JWT libraries for different languages provide an API to set it. If exp claim not provided then Centrifugo won't expire any connections. When provided special algorithm will find connections with exp in the past and activate connection refresh mechanism. Refresh mechanism allows connection to survive and be prolonged. In case of refresh failure client connection will be eventually closed by Centrifugo and won't be accepted until new valid and actual credentials provided in connection token. You can use connection expiration mechanism in cases when you don't want users of your app be subscribed on channels after being banned/deactivated in application. Or to protect your users from token leak (providing reasonably small time of expiration). Choose exp value wisely, you don't need small values because refresh mechanism will hit your application often with refresh requests. But setting this value too large can lead to non very fast user connection deactivation. This is a trade off. Read more about connection expiration in special chapter. info \u00b6 This claim is optional - this is additional information about client connection that can be provided for Centrifugo. This information will be included in presence information, join/leave events and in channel publication message if it was published from client side. b64info \u00b6 If you are using binary protobuf protocol you may want info to be custom bytes. Use this field in this case. This field contains a base64 representation of your bytes. After receiving Centrifugo will decode base64 back to bytes and will embed result into various places described above. channels \u00b6 New in v2.4.0 Optional array of strings with server-side channels. See more details about server-side subscriptions . Examples \u00b6 Let's look how to generate connection HS256 JWT in Python: Simplest token \u00b6 import jwt token = jwt . encode ({ \"sub\" : \"42\" }, \"secret\" ) . decode () print ( token ) Note that we use the value of secret from Centrifugo config here (in this case secret value is just secret ). The only two who must know secret key is your application backend which generates JWT and Centrifugo itself. You should never show secret key to your users. Then you can pass this token to your client side and use it when connecting to Centrifugo: var centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" ); centrifuge . setToken ( token ); centrifuge . connect (); Token with expiration \u00b6 Token that will be valid for 5 minutes: import jwt import time claims = { \"sub\" : \"42\" , \"exp\" : int ( time . time ()) + 5 * 60 } token = jwt . encode ( claims , \"secret\" , algorithm = \"HS256\" ) . decode () print ( token ) Token with additional connection info \u00b6 import jwt claims = { \"sub\" : \"42\" , \"info\" : { \"name\" : \"Alexander Emelin\" }} token = jwt . encode ( claims , \"secret\" , algorithm = \"HS256\" ) . decode () print ( token ) Investigating problems with JWT \u00b6 You can use jwt.io site to investigate contents of your tokens. Also server logs usually contain some useful information.","title":"Authentication"},{"location":"server/authentication/#authentication","text":"When you are using centrifuge library from Go language you can implement any user authentication using middleware. In Centrifugo case you need to tell server who is connecting in well-known predefined way. This chapter will describe this mechanism. When connecting to Centrifugo client must provide connection JWT token with several predefined credential claims. If you've never heard about JWT before - refer to jwt.io page. At moment **the only supported JWT algorithms are HMAC and RSA ** - i.e. HS256, HS384, HS512, RSA256, RSA384, RSA512. This can be extended later. RSA algorithm is available since v2.3.0 release. We will use Javascript Centrifugo client here for example snippets for client side and PyJWT Python library to generate connection token on backend side. To add HMAC secret key to Centrifugo add token_hmac_secret_key to configuration file: { \"token_hmac_secret_key\" : \"<YOUR-SECRET-STRING-HERE>\" , ... } To add RSA public key (must be PEM encoded string) add token_rsa_public_key option, ex: { \"token_rsa_public_key\" : \"-----BEGIN PUBLIC KEY-----\\nMFwwDQYJKoZ...\" , ... }","title":"Authentication"},{"location":"server/authentication/#claims","text":"Centrifugo uses the following claims in JWT: sub , exp , info and b64info . What do they mean? Let's describe in detail.","title":"Claims"},{"location":"server/authentication/#sub","text":"This is a standard JWT claim which must contain an ID of current application user ( as string ). If your user is not currently authenticated in your application but you want to let him connect to Centrifugo anyway you can use empty string as user ID in this sub claim. This is called anonymous access. In this case anonymous option must be enabled in Centrifugo configuration for channels that client will subscribe to.","title":"sub"},{"location":"server/authentication/#exp","text":"This is an a UNIX timestamp seconds when token will expire. This is standard JWT claim - all JWT libraries for different languages provide an API to set it. If exp claim not provided then Centrifugo won't expire any connections. When provided special algorithm will find connections with exp in the past and activate connection refresh mechanism. Refresh mechanism allows connection to survive and be prolonged. In case of refresh failure client connection will be eventually closed by Centrifugo and won't be accepted until new valid and actual credentials provided in connection token. You can use connection expiration mechanism in cases when you don't want users of your app be subscribed on channels after being banned/deactivated in application. Or to protect your users from token leak (providing reasonably small time of expiration). Choose exp value wisely, you don't need small values because refresh mechanism will hit your application often with refresh requests. But setting this value too large can lead to non very fast user connection deactivation. This is a trade off. Read more about connection expiration in special chapter.","title":"exp"},{"location":"server/authentication/#info","text":"This claim is optional - this is additional information about client connection that can be provided for Centrifugo. This information will be included in presence information, join/leave events and in channel publication message if it was published from client side.","title":"info"},{"location":"server/authentication/#b64info","text":"If you are using binary protobuf protocol you may want info to be custom bytes. Use this field in this case. This field contains a base64 representation of your bytes. After receiving Centrifugo will decode base64 back to bytes and will embed result into various places described above.","title":"b64info"},{"location":"server/authentication/#channels","text":"New in v2.4.0 Optional array of strings with server-side channels. See more details about server-side subscriptions .","title":"channels"},{"location":"server/authentication/#examples","text":"Let's look how to generate connection HS256 JWT in Python:","title":"Examples"},{"location":"server/authentication/#simplest-token","text":"import jwt token = jwt . encode ({ \"sub\" : \"42\" }, \"secret\" ) . decode () print ( token ) Note that we use the value of secret from Centrifugo config here (in this case secret value is just secret ). The only two who must know secret key is your application backend which generates JWT and Centrifugo itself. You should never show secret key to your users. Then you can pass this token to your client side and use it when connecting to Centrifugo: var centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" ); centrifuge . setToken ( token ); centrifuge . connect ();","title":"Simplest token"},{"location":"server/authentication/#token-with-expiration","text":"Token that will be valid for 5 minutes: import jwt import time claims = { \"sub\" : \"42\" , \"exp\" : int ( time . time ()) + 5 * 60 } token = jwt . encode ( claims , \"secret\" , algorithm = \"HS256\" ) . decode () print ( token )","title":"Token with expiration"},{"location":"server/authentication/#token-with-additional-connection-info","text":"import jwt claims = { \"sub\" : \"42\" , \"info\" : { \"name\" : \"Alexander Emelin\" }} token = jwt . encode ( claims , \"secret\" , algorithm = \"HS256\" ) . decode () print ( token )","title":"Token with additional connection info"},{"location":"server/authentication/#investigating-problems-with-jwt","text":"You can use jwt.io site to investigate contents of your tokens. Also server logs usually contain some useful information.","title":"Investigating problems with JWT"},{"location":"server/channels/","text":"Channels \u00b6 Channel is a route for publication messages. Clients can subscribe to a channel to receive messages published to this channel \u2013 new publications, join/leave events (if enabled for a channel namespace) etc. Channel subscriber can also ask for channel presence or channel history information (if enabled for a channel namespace). Channel is just a string - news , comments are valid channel names. Channel is an ephemeral entity \u2013 you don't need to create it explicitly . Channel created automatically by Centrifugo as soon as first client subscribes to it. As soon as last subscriber leaves channel - it's automatically cleaned up. Channel name rules \u00b6 Only ASCII symbols must be used in channel string . Channel name length limited by 255 characters by default (can be changed via configuration file option channel_max_length ). Several symbols in channel names reserved for Centrifugo internal needs: : \u2013 for namespace channel boundary (see below) $ \u2013 for private channel prefix (see below) # \u2013 for user channel boundary (see below) * \u2013 for future Centrifugo needs & \u2013 for future Centrifugo needs / \u2013 for future Centrifugo needs namespace channel boundary (:) \u00b6 : \u2013 is a channel namespace boundary. Namespaces used to set custom options to a group of channels. Each channel belonging to the same namespace will have the same channel options. Read more about available channel options in configuration chapter . If channel is public:chat - then Centrifugo will apply options to this channel from channel namespace with name public . private channel prefix ($) \u00b6 If channel starts with $ then it is considered private . Subscription on a private channel must be properly signed by your backend. Use private channels if you pass sensitive data inside channel and want to control access permissions on your backend. For example $secrets is a private channel, $public:chat - is a private channel that belongs namespace public . Subscription request to private channels requires additional JWT from your backend. Read detailed chapter about private channels . If you need a personal channel for a single user (or maybe channel for short and stable set of users) then consider using user-limited channel (see below) as a simpler alternative which does not require additional subscription token from your backend. user channel boundary (#) \u00b6 # \u2013 is a user channel boundary. This is a separator to create personal channels for users (we call this user-limited channels ) without need to provide subscription token. For example if channel is news#42 then only user with ID 42 can subscribe on this channel (Centrifugo knows user ID because clients provide it in connection credentials with connection JWT). Moreover, you can provide several user IDs in channel name separated by a comma: dialog#42,43 \u2013 in this case only user with ID 42 and user with ID 43 will be able to subscribe on this channel. This is useful for channels with static list of allowed users, for example for single user personal messages channel, for dialog channel between certainly defined users. As soon as you need dynamic user access to channel this channel type does not suit well.","title":"Channels"},{"location":"server/channels/#channels","text":"Channel is a route for publication messages. Clients can subscribe to a channel to receive messages published to this channel \u2013 new publications, join/leave events (if enabled for a channel namespace) etc. Channel subscriber can also ask for channel presence or channel history information (if enabled for a channel namespace). Channel is just a string - news , comments are valid channel names. Channel is an ephemeral entity \u2013 you don't need to create it explicitly . Channel created automatically by Centrifugo as soon as first client subscribes to it. As soon as last subscriber leaves channel - it's automatically cleaned up.","title":"Channels"},{"location":"server/channels/#channel-name-rules","text":"Only ASCII symbols must be used in channel string . Channel name length limited by 255 characters by default (can be changed via configuration file option channel_max_length ). Several symbols in channel names reserved for Centrifugo internal needs: : \u2013 for namespace channel boundary (see below) $ \u2013 for private channel prefix (see below) # \u2013 for user channel boundary (see below) * \u2013 for future Centrifugo needs & \u2013 for future Centrifugo needs / \u2013 for future Centrifugo needs","title":"Channel name rules"},{"location":"server/channels/#namespace-channel-boundary","text":": \u2013 is a channel namespace boundary. Namespaces used to set custom options to a group of channels. Each channel belonging to the same namespace will have the same channel options. Read more about available channel options in configuration chapter . If channel is public:chat - then Centrifugo will apply options to this channel from channel namespace with name public .","title":"namespace channel boundary (:)"},{"location":"server/channels/#private-channel-prefix","text":"If channel starts with $ then it is considered private . Subscription on a private channel must be properly signed by your backend. Use private channels if you pass sensitive data inside channel and want to control access permissions on your backend. For example $secrets is a private channel, $public:chat - is a private channel that belongs namespace public . Subscription request to private channels requires additional JWT from your backend. Read detailed chapter about private channels . If you need a personal channel for a single user (or maybe channel for short and stable set of users) then consider using user-limited channel (see below) as a simpler alternative which does not require additional subscription token from your backend.","title":"private channel prefix ($)"},{"location":"server/channels/#user-channel-boundary","text":"# \u2013 is a user channel boundary. This is a separator to create personal channels for users (we call this user-limited channels ) without need to provide subscription token. For example if channel is news#42 then only user with ID 42 can subscribe on this channel (Centrifugo knows user ID because clients provide it in connection credentials with connection JWT). Moreover, you can provide several user IDs in channel name separated by a comma: dialog#42,43 \u2013 in this case only user with ID 42 and user with ID 43 will be able to subscribe on this channel. This is useful for channels with static list of allowed users, for example for single user personal messages channel, for dialog channel between certainly defined users. As soon as you need dynamic user access to channel this channel type does not suit well.","title":"user channel boundary (#)"},{"location":"server/configuration/","text":"Configuration \u00b6 Here we will look at how Centrifugo can be configured. Getting help \u00b6 First let's look at all available command-line options: centrifugo -h You should see something like this as output: Centrifugo \u2013 scalable real-time messaging server in language-agnostic way Usage: [flags] [command] Available Commands: checkconfig Check configuration file checktoken Check connection JWT genconfig Generate minimal configuration file to start with gentoken Generate sample connection JWT for user help Help about any command version Centrifugo version information Flags: -a, --address string interface address to listen on --admin enable admin web interface --admin_external enable admin web interface on external port --admin_insecure use insecure admin mode \u2013 no auth required for admin socket --api_insecure use insecure API mode --client_insecure start in insecure client mode -c, --config string path to config file (default \"config.json\") --debug enable debug endpoints -e, --engine string engine to use: memory or redis (default \"memory\") --grpc_api enable GRPC API server --grpc_api_port int port to bind GRPC API server to (default 10000) --grpc_api_tls enable TLS for GRPC API server, requires an X509 certificate and a key file --grpc_api_tls_cert string path to an X509 certificate file for GRPC API server --grpc_api_tls_disable disable general TLS for GRPC API server --grpc_api_tls_key string path to an X509 certificate key for GRPC API server --health enable health check endpoint -h, --help help for this command --internal_address string custom interface address to listen on for internal endpoints --internal_port string custom port for internal endpoints --log_file string optional log file - if not specified logs go to STDOUT --log_level string set the log level: debug, info, error, fatal or none (default \"info\") -n, --name string unique node name --pid_file string optional path to create PID file -p, --port string port to bind HTTP server to (default \"8000\") --prometheus enable Prometheus metrics endpoint --redis_db int Redis database (Redis engine) --redis_host string Redis host (Redis engine) (default \"127.0.0.1\") --redis_master_name string name of Redis master Sentinel monitors (Redis engine) --redis_password string Redis auth password (Redis engine) --redis_port string Redis port (Redis engine) (default \"6379\") --redis_sentinels string comma-separated list of Sentinel addresses (Redis engine) --redis_tls enable Redis TLS connection --redis_tls_skip_verify disable Redis TLS host verification --redis_url string Redis connection URL in format redis://:password@hostname:port/db (Redis engine) --tls enable TLS, requires an X509 certificate and a key file --tls_cert string path to an X509 certificate file --tls_external enable TLS only for external endpoints --tls_key string path to an X509 certificate key Not all available Centrifugo options available to be set over command-line flags \u2013 here we can see only some frequently used. Note All command-line options of Centrifugo can be set via configuration file with the same name (without -- prefix of course). Also all available options can be set over environment variables in format CENTRIFUGO_<OPTION_NAME> . Config file formats \u00b6 Centrifugo supports different configuration file formats. JSON config format \u00b6 Centrifugo requires configuration file on start. As was mentioned earlier it must be a file with valid JSON. This is a minimal Centrifugo configuration file: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"<YOUR-SECRET-STRING-HERE>\" , \"api_key\" : \"<YOUR-API-KEY-HERE>\" , } The only two fields required are token_hmac_secret_key and api_key . token_hmac_secret_key used to check JWT signature (more about JWT in authentication chapter ). API key used for Centrifugo API endpoint authorization, see more in chapter about server HTTP API . Keep both values in secret and never reveal to clients. The option v3_use_offset turns on using latest client-server protocol offset field (will be used by default in Centrifugo v3 so better to use it from start). TOML config format \u00b6 Centrifugo also supports TOML format for configuration file: centrifugo --config=config.toml Where config.toml contains: v3_use_offset = true token_hmac_secret_key = \"<YOUR-SECRET-STRING-HERE>\" api_key = \"<YOUR-API-KEY-HERE>\" log_level = \"debug\" I.e. the same configuration as JSON file above with one extra option to define logging level. YAML config format \u00b6 And YAML config also supported. config.yaml : v3_use_offset: true token_hmac_secret_key: \"<YOUR-SECRET-STRING-HERE>\" api_key: \"<YOUR-API-KEY-HERE>\" log_level: debug With YAML remember to use spaces, not tabs when writing configuration file. version command \u00b6 To show Centrifugo version and exit run: centrifugo version checkconfig command \u00b6 Centrifugo has special command to check configuration file checkconfig : centrifugo checkconfig --config = config.json If any errors found during validation \u2013 program will exit with error message and exit code 1. genconfig command \u00b6 Another command is genconfig : centrifugo genconfig -c config.json It will automatically generate the minimal required configuration file. If any errors happen \u2013 program will exit with error message and exit code 1. gentoken command \u00b6 Another command is gentoken : centrifugo gentoken -c config.json -u 28282 It will automatically generate HMAC SHA-256 based token for user with ID 28282 (which expires in 1 week). You can change token TTL with -t flag (number of seconds): centrifugo gentoken -c config.json -u 28282 -t 3600 This way generated token will be valid for 1 hour. If any errors happen \u2013 program will exit with error message and exit code 1. checktoken command \u00b6 One more command is checktoken : centrifugo checktoken -c config.json <TOKEN> It will validate your connection JWT, so you can test it before using while developing application. If any errors happen or validation failed \u2013 program will exit with error message and exit code 1. Important options \u00b6 Some of the most important options you can configure when running Centrifugo: address \u2013 bind your Centrifugo to specific interface address (by default \"\" ) port \u2013 port to bind Centrifugo to (by default 8000 ) engine \u2013 engine to use - memory or redis (by default memory ). Read more about engines in special chapter . Note that some options can be set via command-line. Command-line options are more valuable when set than configuration file's options. See description of viper \u2013 to see more details about configuration options priority. Channel options \u00b6 Let's look at options related to channels. Channel is an entity to which clients can subscribe to receive messages published into that channel. Channel is just a string (several symbols has special meaning in Centrifugo - see special chapter to find more information about channels). The following options will affect channel behaviour: publish (boolean, default false ) \u2013 allow clients to publish messages into channels directly (from client side). Your application will never receive those messages. In idiomatic case all messages must be published to Centrifugo by your application backend using Centrifugo API. But this option can be useful when you want to build something without backend-side validation and saving into database. This option can also be useful for demos and prototyping real-time ideas. By default it's false . subscribe_to_publish (boolean, default false ) - when publish option enabled client can publish into channel without being subscribed to it. This option enables automatic check that client subscribed on channel before allowing client to publish into channel. anonymous (boolean, default false ) \u2013 this option enables anonymous access (with empty sub claim in connection token). In most situations your application works with authenticated users so every user has its own unique id. But if you provide real-time features for public access you may need unauthorized access to some channels. Turn on this option and use empty string as user ID. presence (boolean, default false ) \u2013 enable/disable presence information. Presence is an information about clients currently subscribed on channel. By default this option is off so no presence information will be available for channels. presence_disable_for_client (boolean, default false , available since v2.2.3) \u2013 allows making presence calls available only for server side API. By default presence information is available for both client and server side APIs. join_leave (boolean, default false ) \u2013 enable/disable sending join(leave) messages when client subscribes on a channel (unsubscribes from channel). history_size (integer, default 0 ) \u2013 history size (amount of messages) for channels. As Centrifugo keeps all history messages in memory it's very important to limit maximum amount of messages in channel history to reasonable value. history_size defines maximum amount of messages that Centrifugo will keep for each channel in namespace during history lifetime (see below). By default history size is 0 - this means that channels will have no history messages at all. history_lifetime (integer, default 0 ) \u2013 interval in seconds how long to keep channel history messages. As all history is storing in memory it is also very important to get rid of old history data for unused (inactive for a long time) channels. By default history lifetime is 0 \u2013 this means that channels will have no history messages at all. So to turn on keeping history messages you should wisely configure both history_size and history_lifetime options . history_recover (boolean, default false ) \u2013 when enabled Centrifugo will try to recover missed publications while client was disconnected for some reason (bad internet connection for example). By default this feature is off. This option must be used in conjunction with reasonably configured message history for channel i.e. history_size and history_lifetime must be set (because Centrifugo uses channel history to recover messages). Also note that not all real-time events require this feature turned on so think wisely when you need this. When this option turned on your application should be designed in a way to tolerate duplicate messages coming from channel (currently Centrifugo returns recovered publications in order and without duplicates but this is implementation detail that can be theoretically changed in future). See more details about how recovery works in special chapter . history_disable_for_client (boolean, default false , available since v2.2.3) \u2013 allows making history available only for server side API. By default false \u2013 i.e. history calls are available for both client and server side APIs. History recovery mechanism if enabled will continue to work for clients anyway even if history_disable_for_client is on. server_side (boolean, default false , available since v2.4.0) \u2013 when enabled then all client-side subscription requests to channels in namespace will be rejected with PermissionDenied error. Let's look how to set some of these options in config: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"my-secret-key\" , \"api_key\" : \"secret-api-key\" , \"anonymous\" : true , \"publish\" : true , \"subscribe_to_publish\" : true , \"presence\" : true , \"join_leave\" : true , \"history_size\" : 10 , \"history_lifetime\" : 300 , \"history_recover\" : true } And the last channel specific option is namespaces . namespaces are optional and if set must be an array of namespace objects. Namespace allows to configure custom options for channels starting with namespace name. This provides a great control over channel behaviour. Namespace has a name and the same channel options (with same defaults) as described above. name - unique namespace name (name must consist of letters, numbers, underscores or hyphens and be more than 2 symbols length i.e. satisfy regexp ^[-a-zA-Z0-9_]{2,}$ ). If you want to use namespace options for channel - you must include namespace name into channel name with : as separator: public:messages gossips:messages Where public and gossips are namespace names from project namespaces . All things together here is an example of config.json which includes registered project with all options set and 2 additional namespaces in it: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"very-long-secret-key\" , \"api_key\" : \"secret-api-key\" , \"anonymous\" : true , \"publish\" : true , \"presence\" : true , \"join_leave\" : true , \"history_size\" : 10 , \"history_lifetime\" : 30 , \"namespaces\" : [ { \"name\" : \"public\" , \"publish\" : true , \"anonymous\" : true , \"history_size\" : 10 , \"history_lifetime\" : 300 , \"history_recover\" : true }, { \"name\" : \"gossips\" , \"presence\" : true , \"join_leave\" : true } ] } Channel news will use globally defined channel options. Channel public:news will use public namespace's options. Channel gossips:news will use gossips namespace's options. There is no inheritance in channel options and namespaces \u2013 so if for example you defined presence: true on top level of configuration and then defined namespace \u2013 that namespace won't have presence enabled - you must enable it for namespace explicitly. Advanced configuration \u00b6 Centrifugo has some options for which default values make sense for most applications. In many case you don't need (and you really should not) change them. This chapter is about such options. client_channel_limit \u00b6 Default: 128 Sets maximum number of different channel subscriptions single client can have. channel_max_length \u00b6 Default: 255 Sets maximum length of channel name. client_user_connection_limit \u00b6 Default: 0 Maximum number of connections from user (with known user ID) to Centrifugo node. By default - unlimited. client_request_max_size \u00b6 Default: 65536 Maximum allowed size of request from client in bytes. client_queue_max_size \u00b6 Default: 10485760 Maximum client message queue size in bytes to close slow reader connections. By default - 10mb. client_anonymous \u00b6 Default: false Enable mode when all clients can connect to Centrifugo without JWT connection token. In this case all connections without token will be treated as anonymous (i.e. with empty user ID) and only can subscribe to channels with anonymous option enabled. sockjs_heartbeat_delay \u00b6 Default: 25 Interval in seconds how often to send SockJS h-frames to client. websocket_compression \u00b6 Default: false Enable websocket compression, see chapter about websocket transport for more details. gomaxprocs \u00b6 Default: 0 By default Centrifugo runs on all available CPU cores. If you want to limit amount of cores Centrifugo can utilize in one moment use this option. Advanced endpoint configuration. \u00b6 After you started Centrifugo you have several endpoints available. As soon as you have not provided any extra options you have 3 endpoints by default. Default endpoints. \u00b6 The main endpoint is raw Websocket endpoint to serve client connections that use pure Websocket protocol: ws://localhost:8000/connection/websocket Then there is SockJS endpoint - it's needed to serve client connections that use SockJS library: http://localhost:8000/connection/sockjs And finally you have API endpoint to publish messages to channels (and execute other available API commands): http://localhost:8000/api By default all endpoints work on port 8000 . You can change it using port option: { \"port\": 9000 } In production setup you will have your domain name in endpoint addresses above instead of localhost . Also if your Centrifugo will be behind proxy or load balancer software you most probably won't have ports in your endpoint addresses. What will always be the same as shown above are URL paths: /connection/sockjs , /connection/websocket , /api . Let's look at possibilities to tweak available endpoints. Admin endpoints. \u00b6 First is enabling admin endpoints: { ... \"admin\": true, \"admin_password\": \"password\", \"admin_secret\": \"secret\" } This makes the following endpoint available: http://localhost:8000 At this address you will see admin web interface. You can log into it using admin_password value shown above. Debug endpoints. \u00b6 Next, when Centrifugo started in debug mode some extra debug endpoints become available. To start in debug mode add debug option to config: { ... \"debug\": true } And endpoint: http://localhost:8000/debug/pprof/ \u2013 will show you useful info about internal state of Centrifugo instance. This info is especially helpful when troubleshooting. See wiki page for more info. Healthcheck endpoint \u00b6 New in v2.1.0 Use health boolean option (by default false ) to enable healthcheck endpoint which will be available on path /health . Also available over command-line flag: ./centrifugo -c config.json --health Custom internal ports \u00b6 We strongly recommend to not expose API, admin, debug and prometheus endpoints to Internet. The following Centrifugo endpoints are considered internal: API endpoint ( /api ) - for HTTP API requests Admin web interface endpoints ( / , /admin/auth , /admin/api ) - used by web interface Prometheus endpoint ( /metrics ) - used for exposing server metrics in Prometheus format Healthcheck endpoint ( /health ) - used to do healthchecks Debug endpoints ( /debug/pprof ) - used to inspect internal server state It's a good practice to protect those endpoints with firewall. For example you can do this in location section of Nginx configuration. Though sometimes you don't have access to per-location configuration in your proxy/load balancer software. For example when using Amazon ELB. In this case you can change ports on which your internal endpoints work. To run internal endpoints on custom port use internal_port option: { ... \"internal_port\": 9000 } So admin web interface will work on address: http://localhost:9000 Also debug page will be available on new custom port too: http://localhost:9000/debug/pprof/ The same for API and prometheus endpoint. Disable default endpoints \u00b6 These options available since v2.4.0 To disable websocket endpoint set websocket_disable boolean option to true . To disable SockJS endpoint set sockjs_disable boolean option to true . To disable API endpoint set api_disable boolean option to true . Customize handler endpoinds \u00b6 Starting from Centrifugo v2.2.5 it's possible to customize server HTTP handler endpoints. To do this Centrifugo supports several options: admin_handler_prefix (default \"\" ) - to control Admin panel URL prefix websocket_handler_prefix (default \"/connection/websocket\" ) - to control WebSocket URL prefix sockjs_handler_prefix (default \"/connection/sockjs\" ) - to control SockJS URL prefix api_handler_prefix (default \"/api\" ) - to control HTTP API URL prefix prometheus_handler_prefix (default \"/metrics\" ) - to control Prometheus URL prefix health_handler_prefix (default \"/health\" ) - to control health check URL prefix","title":"Configuration"},{"location":"server/configuration/#configuration","text":"Here we will look at how Centrifugo can be configured.","title":"Configuration"},{"location":"server/configuration/#getting-help","text":"First let's look at all available command-line options: centrifugo -h You should see something like this as output: Centrifugo \u2013 scalable real-time messaging server in language-agnostic way Usage: [flags] [command] Available Commands: checkconfig Check configuration file checktoken Check connection JWT genconfig Generate minimal configuration file to start with gentoken Generate sample connection JWT for user help Help about any command version Centrifugo version information Flags: -a, --address string interface address to listen on --admin enable admin web interface --admin_external enable admin web interface on external port --admin_insecure use insecure admin mode \u2013 no auth required for admin socket --api_insecure use insecure API mode --client_insecure start in insecure client mode -c, --config string path to config file (default \"config.json\") --debug enable debug endpoints -e, --engine string engine to use: memory or redis (default \"memory\") --grpc_api enable GRPC API server --grpc_api_port int port to bind GRPC API server to (default 10000) --grpc_api_tls enable TLS for GRPC API server, requires an X509 certificate and a key file --grpc_api_tls_cert string path to an X509 certificate file for GRPC API server --grpc_api_tls_disable disable general TLS for GRPC API server --grpc_api_tls_key string path to an X509 certificate key for GRPC API server --health enable health check endpoint -h, --help help for this command --internal_address string custom interface address to listen on for internal endpoints --internal_port string custom port for internal endpoints --log_file string optional log file - if not specified logs go to STDOUT --log_level string set the log level: debug, info, error, fatal or none (default \"info\") -n, --name string unique node name --pid_file string optional path to create PID file -p, --port string port to bind HTTP server to (default \"8000\") --prometheus enable Prometheus metrics endpoint --redis_db int Redis database (Redis engine) --redis_host string Redis host (Redis engine) (default \"127.0.0.1\") --redis_master_name string name of Redis master Sentinel monitors (Redis engine) --redis_password string Redis auth password (Redis engine) --redis_port string Redis port (Redis engine) (default \"6379\") --redis_sentinels string comma-separated list of Sentinel addresses (Redis engine) --redis_tls enable Redis TLS connection --redis_tls_skip_verify disable Redis TLS host verification --redis_url string Redis connection URL in format redis://:password@hostname:port/db (Redis engine) --tls enable TLS, requires an X509 certificate and a key file --tls_cert string path to an X509 certificate file --tls_external enable TLS only for external endpoints --tls_key string path to an X509 certificate key Not all available Centrifugo options available to be set over command-line flags \u2013 here we can see only some frequently used. Note All command-line options of Centrifugo can be set via configuration file with the same name (without -- prefix of course). Also all available options can be set over environment variables in format CENTRIFUGO_<OPTION_NAME> .","title":"Getting help"},{"location":"server/configuration/#config-file-formats","text":"Centrifugo supports different configuration file formats.","title":"Config file formats"},{"location":"server/configuration/#json-config-format","text":"Centrifugo requires configuration file on start. As was mentioned earlier it must be a file with valid JSON. This is a minimal Centrifugo configuration file: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"<YOUR-SECRET-STRING-HERE>\" , \"api_key\" : \"<YOUR-API-KEY-HERE>\" , } The only two fields required are token_hmac_secret_key and api_key . token_hmac_secret_key used to check JWT signature (more about JWT in authentication chapter ). API key used for Centrifugo API endpoint authorization, see more in chapter about server HTTP API . Keep both values in secret and never reveal to clients. The option v3_use_offset turns on using latest client-server protocol offset field (will be used by default in Centrifugo v3 so better to use it from start).","title":"JSON config format"},{"location":"server/configuration/#toml-config-format","text":"Centrifugo also supports TOML format for configuration file: centrifugo --config=config.toml Where config.toml contains: v3_use_offset = true token_hmac_secret_key = \"<YOUR-SECRET-STRING-HERE>\" api_key = \"<YOUR-API-KEY-HERE>\" log_level = \"debug\" I.e. the same configuration as JSON file above with one extra option to define logging level.","title":"TOML config format"},{"location":"server/configuration/#yaml-config-format","text":"And YAML config also supported. config.yaml : v3_use_offset: true token_hmac_secret_key: \"<YOUR-SECRET-STRING-HERE>\" api_key: \"<YOUR-API-KEY-HERE>\" log_level: debug With YAML remember to use spaces, not tabs when writing configuration file.","title":"YAML config format"},{"location":"server/configuration/#version-command","text":"To show Centrifugo version and exit run: centrifugo version","title":"version command"},{"location":"server/configuration/#checkconfig-command","text":"Centrifugo has special command to check configuration file checkconfig : centrifugo checkconfig --config = config.json If any errors found during validation \u2013 program will exit with error message and exit code 1.","title":"checkconfig command"},{"location":"server/configuration/#genconfig-command","text":"Another command is genconfig : centrifugo genconfig -c config.json It will automatically generate the minimal required configuration file. If any errors happen \u2013 program will exit with error message and exit code 1.","title":"genconfig command"},{"location":"server/configuration/#gentoken-command","text":"Another command is gentoken : centrifugo gentoken -c config.json -u 28282 It will automatically generate HMAC SHA-256 based token for user with ID 28282 (which expires in 1 week). You can change token TTL with -t flag (number of seconds): centrifugo gentoken -c config.json -u 28282 -t 3600 This way generated token will be valid for 1 hour. If any errors happen \u2013 program will exit with error message and exit code 1.","title":"gentoken command"},{"location":"server/configuration/#checktoken-command","text":"One more command is checktoken : centrifugo checktoken -c config.json <TOKEN> It will validate your connection JWT, so you can test it before using while developing application. If any errors happen or validation failed \u2013 program will exit with error message and exit code 1.","title":"checktoken command"},{"location":"server/configuration/#important-options","text":"Some of the most important options you can configure when running Centrifugo: address \u2013 bind your Centrifugo to specific interface address (by default \"\" ) port \u2013 port to bind Centrifugo to (by default 8000 ) engine \u2013 engine to use - memory or redis (by default memory ). Read more about engines in special chapter . Note that some options can be set via command-line. Command-line options are more valuable when set than configuration file's options. See description of viper \u2013 to see more details about configuration options priority.","title":"Important options"},{"location":"server/configuration/#channel-options","text":"Let's look at options related to channels. Channel is an entity to which clients can subscribe to receive messages published into that channel. Channel is just a string (several symbols has special meaning in Centrifugo - see special chapter to find more information about channels). The following options will affect channel behaviour: publish (boolean, default false ) \u2013 allow clients to publish messages into channels directly (from client side). Your application will never receive those messages. In idiomatic case all messages must be published to Centrifugo by your application backend using Centrifugo API. But this option can be useful when you want to build something without backend-side validation and saving into database. This option can also be useful for demos and prototyping real-time ideas. By default it's false . subscribe_to_publish (boolean, default false ) - when publish option enabled client can publish into channel without being subscribed to it. This option enables automatic check that client subscribed on channel before allowing client to publish into channel. anonymous (boolean, default false ) \u2013 this option enables anonymous access (with empty sub claim in connection token). In most situations your application works with authenticated users so every user has its own unique id. But if you provide real-time features for public access you may need unauthorized access to some channels. Turn on this option and use empty string as user ID. presence (boolean, default false ) \u2013 enable/disable presence information. Presence is an information about clients currently subscribed on channel. By default this option is off so no presence information will be available for channels. presence_disable_for_client (boolean, default false , available since v2.2.3) \u2013 allows making presence calls available only for server side API. By default presence information is available for both client and server side APIs. join_leave (boolean, default false ) \u2013 enable/disable sending join(leave) messages when client subscribes on a channel (unsubscribes from channel). history_size (integer, default 0 ) \u2013 history size (amount of messages) for channels. As Centrifugo keeps all history messages in memory it's very important to limit maximum amount of messages in channel history to reasonable value. history_size defines maximum amount of messages that Centrifugo will keep for each channel in namespace during history lifetime (see below). By default history size is 0 - this means that channels will have no history messages at all. history_lifetime (integer, default 0 ) \u2013 interval in seconds how long to keep channel history messages. As all history is storing in memory it is also very important to get rid of old history data for unused (inactive for a long time) channels. By default history lifetime is 0 \u2013 this means that channels will have no history messages at all. So to turn on keeping history messages you should wisely configure both history_size and history_lifetime options . history_recover (boolean, default false ) \u2013 when enabled Centrifugo will try to recover missed publications while client was disconnected for some reason (bad internet connection for example). By default this feature is off. This option must be used in conjunction with reasonably configured message history for channel i.e. history_size and history_lifetime must be set (because Centrifugo uses channel history to recover messages). Also note that not all real-time events require this feature turned on so think wisely when you need this. When this option turned on your application should be designed in a way to tolerate duplicate messages coming from channel (currently Centrifugo returns recovered publications in order and without duplicates but this is implementation detail that can be theoretically changed in future). See more details about how recovery works in special chapter . history_disable_for_client (boolean, default false , available since v2.2.3) \u2013 allows making history available only for server side API. By default false \u2013 i.e. history calls are available for both client and server side APIs. History recovery mechanism if enabled will continue to work for clients anyway even if history_disable_for_client is on. server_side (boolean, default false , available since v2.4.0) \u2013 when enabled then all client-side subscription requests to channels in namespace will be rejected with PermissionDenied error. Let's look how to set some of these options in config: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"my-secret-key\" , \"api_key\" : \"secret-api-key\" , \"anonymous\" : true , \"publish\" : true , \"subscribe_to_publish\" : true , \"presence\" : true , \"join_leave\" : true , \"history_size\" : 10 , \"history_lifetime\" : 300 , \"history_recover\" : true } And the last channel specific option is namespaces . namespaces are optional and if set must be an array of namespace objects. Namespace allows to configure custom options for channels starting with namespace name. This provides a great control over channel behaviour. Namespace has a name and the same channel options (with same defaults) as described above. name - unique namespace name (name must consist of letters, numbers, underscores or hyphens and be more than 2 symbols length i.e. satisfy regexp ^[-a-zA-Z0-9_]{2,}$ ). If you want to use namespace options for channel - you must include namespace name into channel name with : as separator: public:messages gossips:messages Where public and gossips are namespace names from project namespaces . All things together here is an example of config.json which includes registered project with all options set and 2 additional namespaces in it: { \"v3_use_offset\" : true , \"token_hmac_secret_key\" : \"very-long-secret-key\" , \"api_key\" : \"secret-api-key\" , \"anonymous\" : true , \"publish\" : true , \"presence\" : true , \"join_leave\" : true , \"history_size\" : 10 , \"history_lifetime\" : 30 , \"namespaces\" : [ { \"name\" : \"public\" , \"publish\" : true , \"anonymous\" : true , \"history_size\" : 10 , \"history_lifetime\" : 300 , \"history_recover\" : true }, { \"name\" : \"gossips\" , \"presence\" : true , \"join_leave\" : true } ] } Channel news will use globally defined channel options. Channel public:news will use public namespace's options. Channel gossips:news will use gossips namespace's options. There is no inheritance in channel options and namespaces \u2013 so if for example you defined presence: true on top level of configuration and then defined namespace \u2013 that namespace won't have presence enabled - you must enable it for namespace explicitly.","title":"Channel options"},{"location":"server/configuration/#advanced-configuration","text":"Centrifugo has some options for which default values make sense for most applications. In many case you don't need (and you really should not) change them. This chapter is about such options.","title":"Advanced configuration"},{"location":"server/configuration/#client_channel_limit","text":"Default: 128 Sets maximum number of different channel subscriptions single client can have.","title":"client_channel_limit"},{"location":"server/configuration/#channel_max_length","text":"Default: 255 Sets maximum length of channel name.","title":"channel_max_length"},{"location":"server/configuration/#client_user_connection_limit","text":"Default: 0 Maximum number of connections from user (with known user ID) to Centrifugo node. By default - unlimited.","title":"client_user_connection_limit"},{"location":"server/configuration/#client_request_max_size","text":"Default: 65536 Maximum allowed size of request from client in bytes.","title":"client_request_max_size"},{"location":"server/configuration/#client_queue_max_size","text":"Default: 10485760 Maximum client message queue size in bytes to close slow reader connections. By default - 10mb.","title":"client_queue_max_size"},{"location":"server/configuration/#client_anonymous","text":"Default: false Enable mode when all clients can connect to Centrifugo without JWT connection token. In this case all connections without token will be treated as anonymous (i.e. with empty user ID) and only can subscribe to channels with anonymous option enabled.","title":"client_anonymous"},{"location":"server/configuration/#sockjs_heartbeat_delay","text":"Default: 25 Interval in seconds how often to send SockJS h-frames to client.","title":"sockjs_heartbeat_delay"},{"location":"server/configuration/#websocket_compression","text":"Default: false Enable websocket compression, see chapter about websocket transport for more details.","title":"websocket_compression"},{"location":"server/configuration/#gomaxprocs","text":"Default: 0 By default Centrifugo runs on all available CPU cores. If you want to limit amount of cores Centrifugo can utilize in one moment use this option.","title":"gomaxprocs"},{"location":"server/configuration/#advanced-endpoint-configuration","text":"After you started Centrifugo you have several endpoints available. As soon as you have not provided any extra options you have 3 endpoints by default.","title":"Advanced endpoint configuration."},{"location":"server/configuration/#default-endpoints","text":"The main endpoint is raw Websocket endpoint to serve client connections that use pure Websocket protocol: ws://localhost:8000/connection/websocket Then there is SockJS endpoint - it's needed to serve client connections that use SockJS library: http://localhost:8000/connection/sockjs And finally you have API endpoint to publish messages to channels (and execute other available API commands): http://localhost:8000/api By default all endpoints work on port 8000 . You can change it using port option: { \"port\": 9000 } In production setup you will have your domain name in endpoint addresses above instead of localhost . Also if your Centrifugo will be behind proxy or load balancer software you most probably won't have ports in your endpoint addresses. What will always be the same as shown above are URL paths: /connection/sockjs , /connection/websocket , /api . Let's look at possibilities to tweak available endpoints.","title":"Default endpoints."},{"location":"server/configuration/#admin-endpoints","text":"First is enabling admin endpoints: { ... \"admin\": true, \"admin_password\": \"password\", \"admin_secret\": \"secret\" } This makes the following endpoint available: http://localhost:8000 At this address you will see admin web interface. You can log into it using admin_password value shown above.","title":"Admin endpoints."},{"location":"server/configuration/#debug-endpoints","text":"Next, when Centrifugo started in debug mode some extra debug endpoints become available. To start in debug mode add debug option to config: { ... \"debug\": true } And endpoint: http://localhost:8000/debug/pprof/ \u2013 will show you useful info about internal state of Centrifugo instance. This info is especially helpful when troubleshooting. See wiki page for more info.","title":"Debug endpoints."},{"location":"server/configuration/#healthcheck-endpoint","text":"New in v2.1.0 Use health boolean option (by default false ) to enable healthcheck endpoint which will be available on path /health . Also available over command-line flag: ./centrifugo -c config.json --health","title":"Healthcheck endpoint"},{"location":"server/configuration/#custom-internal-ports","text":"We strongly recommend to not expose API, admin, debug and prometheus endpoints to Internet. The following Centrifugo endpoints are considered internal: API endpoint ( /api ) - for HTTP API requests Admin web interface endpoints ( / , /admin/auth , /admin/api ) - used by web interface Prometheus endpoint ( /metrics ) - used for exposing server metrics in Prometheus format Healthcheck endpoint ( /health ) - used to do healthchecks Debug endpoints ( /debug/pprof ) - used to inspect internal server state It's a good practice to protect those endpoints with firewall. For example you can do this in location section of Nginx configuration. Though sometimes you don't have access to per-location configuration in your proxy/load balancer software. For example when using Amazon ELB. In this case you can change ports on which your internal endpoints work. To run internal endpoints on custom port use internal_port option: { ... \"internal_port\": 9000 } So admin web interface will work on address: http://localhost:9000 Also debug page will be available on new custom port too: http://localhost:9000/debug/pprof/ The same for API and prometheus endpoint.","title":"Custom internal ports"},{"location":"server/configuration/#disable-default-endpoints","text":"These options available since v2.4.0 To disable websocket endpoint set websocket_disable boolean option to true . To disable SockJS endpoint set sockjs_disable boolean option to true . To disable API endpoint set api_disable boolean option to true .","title":"Disable default endpoints"},{"location":"server/configuration/#customize-handler-endpoinds","text":"Starting from Centrifugo v2.2.5 it's possible to customize server HTTP handler endpoints. To do this Centrifugo supports several options: admin_handler_prefix (default \"\" ) - to control Admin panel URL prefix websocket_handler_prefix (default \"/connection/websocket\" ) - to control WebSocket URL prefix sockjs_handler_prefix (default \"/connection/sockjs\" ) - to control SockJS URL prefix api_handler_prefix (default \"/api\" ) - to control HTTP API URL prefix prometheus_handler_prefix (default \"/metrics\" ) - to control Prometheus URL prefix health_handler_prefix (default \"/health\" ) - to control health check URL prefix","title":"Customize handler endpoinds"},{"location":"server/connection_expiration/","text":"Connection expiration \u00b6 In authentication chapter we mentioned exp claim in connection token that allows to expire client connection at some point of time. In this chapter we will look at details on what happens when Centrifugo detects that connection is going to expire. So first you should do is enable client expiration mechanism in Centrifugo providing connection token with expiration: import jwt import time token = jwt . encode ({ \"sub\" : \"42\" , \"exp\" : int ( time . time ()) + 10 * 60 }, \"secret\" ) . decode () print ( token ) Let's suppose that you set exp field to timestamp that will expire in 10 minutes and client connected to Centrifugo with this token. During 10 mins connection will be kept by Centrifugo. When this time passed Centrifugo gives connection some time (configured, 25 seconds by default) to refresh its credentials and provide new valid token with new exp . When client first connects to Centrifugo it receives ttl value in connect reply. That ttl value contains number of seconds after which client must send refresh command with new credentials to Centrifugo. Centrifugo clients must handle this ttl field and automatically start refresh process. For example Javascript browser client will send AJAX POST request to your application when it's time to refresh credentials. By default this request goes to /centrifuge/refresh url endpoint. In response your server must return JSON with new connection token: { \"token\" : token } So you must just return the same connection token for your user when rendering page initially. But with actual valid exp . Javascript client will then send them to Centrifugo server and connection will be refreshed for a time you set in exp . In this case you know which user want to refresh its connection because this is just a general request to your app - so your session mechanism will tell you about the user. If you don't want to refresh connection for this user - just return 403 Forbidden on refresh request to your application backend. Javascript client also has options to hook into refresh mechanism to implement your custom way of refreshing. Other Centrifugo clients also should have hooks to refresh credentials but depending on client API for this can be different - see specific client docs.","title":"Connection expiration"},{"location":"server/connection_expiration/#connection-expiration","text":"In authentication chapter we mentioned exp claim in connection token that allows to expire client connection at some point of time. In this chapter we will look at details on what happens when Centrifugo detects that connection is going to expire. So first you should do is enable client expiration mechanism in Centrifugo providing connection token with expiration: import jwt import time token = jwt . encode ({ \"sub\" : \"42\" , \"exp\" : int ( time . time ()) + 10 * 60 }, \"secret\" ) . decode () print ( token ) Let's suppose that you set exp field to timestamp that will expire in 10 minutes and client connected to Centrifugo with this token. During 10 mins connection will be kept by Centrifugo. When this time passed Centrifugo gives connection some time (configured, 25 seconds by default) to refresh its credentials and provide new valid token with new exp . When client first connects to Centrifugo it receives ttl value in connect reply. That ttl value contains number of seconds after which client must send refresh command with new credentials to Centrifugo. Centrifugo clients must handle this ttl field and automatically start refresh process. For example Javascript browser client will send AJAX POST request to your application when it's time to refresh credentials. By default this request goes to /centrifuge/refresh url endpoint. In response your server must return JSON with new connection token: { \"token\" : token } So you must just return the same connection token for your user when rendering page initially. But with actual valid exp . Javascript client will then send them to Centrifugo server and connection will be refreshed for a time you set in exp . In this case you know which user want to refresh its connection because this is just a general request to your app - so your session mechanism will tell you about the user. If you don't want to refresh connection for this user - just return 403 Forbidden on refresh request to your application backend. Javascript client also has options to hook into refresh mechanism to implement your custom way of refreshing. Other Centrifugo clients also should have hooks to refresh credentials but depending on client API for this can be different - see specific client docs.","title":"Connection expiration"},{"location":"server/engines/","text":"Engines \u00b6 Memory engine Redis engine Engine in Centrifugo is responsible for publishing messages between nodes, handle PUB/SUB broker subscriptions, save/retrieve presence and history data. By default, Centrifugo uses Memory engine. There is also Redis engine available. The difference between them - with Memory engine you can start only one node of Centrifugo, while Redis engine allows running several nodes on different machines and they will be connected via Redis, will know about each other due to Redis and will also keep history and presence data in Redis instead of Centrifugo node process memory so this data can be accessed from each node. To set engine you can use engine configuration option. Available values are memory and redis . Default value is memory . For example to work with Redis engine: centrifugo --config=config.json --engine=redis Or just set engine in config: { ... \"engine\" : \"redis\" } Memory engine \u00b6 Supports only one node. Nice choice to start with. Supports all features keeping everything in Centrifugo node process memory. You don't need to install Redis when using this engine. Advantages: fast does not require separate Redis setup Disadvantages: does not allow scaling nodes (actually you still can scale Centrifugo with Memory engine but you have to publish data into each Centrifugo node and you won't have consistent state of presence) does not persist message history in channels between Centrifugo restarts Several configuration options related to Memory engine: memory_history_meta_ttl (int, default 0 ) - sets a time in seconds of history stream metadata expiration. Stream metadata is an information about current offset number in channel and epoch value. By default, metadata for channels does not expire. Though in some cases \u2013 when channels created for \u0430 short time and then not used anymore \u2013 created metadata can stay in memory while not actually useful. For example, you can have a personal user channel but after using your app for a while user left it forever. In long-term perspective this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than history retention period) can help. In this case unused channel metadata will eventually expire. Available since v2.5.0 Redis engine \u00b6 Allows scaling Centrifugo nodes to different machines. Nodes will use Redis as message broker. Redis engine keeps presence and history data in Redis, uses Redis PUB/SUB for internal node communication. Minimal Redis version is 3.2.0 Several configuration options related to Redis engine: redis_host (string, default \"127.0.0.1\" ) - Redis server host redis_port (int, default 6379 ) - Redis server port redis_url (string, default \"\" ) - optional Redis connection URL redis_password (string, default \"\" ) - Redis password redis_db (int, default 0 ) - number of Redis db to use redis_tls (boolean, default false ) - enable Redis TLS connection (new in v2.0.2) redis_tls_skip_verify (boolean, default false ) - disable Redis TLS host verification (new in v2.0.2) redis_sentinels (string, default \"\" ) - comma separated list of Sentinels for HA redis_master_name (string, default \"\" ) - name of Redis master Sentinel monitors redis_prefix (string, default \"centrifugo\" ) \u2013 custom prefix to use for channels and keys in Redis redis_history_meta_ttl (int, default 0 ) - sets a time in seconds of history stream metadata expiration in Redis Engine. Meta key in Redis is a HASH that contains current offset number in channel and epoch value. By default, metadata for channels does not expire. Though in some cases \u2013 when channels created for \u0430 short time and then not used anymore \u2013 created stream metadata can stay in memory while not actually useful. For example, you can have a personal user channel but after using your app for a while user left it forever. In long-term perspective this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than history retention period) can help. In this case unused channel metadata will eventually expire. Available since v2.3.0 redis_streams (boolean, default false ) \u2013 turns on using Redis Streams instead of List data structure for keeping history All of these options can be set over configuration file. Some of them can be set over command-line arguments (see centrifugo -h output). Let's describe a bit more redis_url option. redis_url allows to set Redis connection parameters in a form of URL in format redis://:password@hostname:port/db_number . When redis_url set Centrifugo will use URL instead of values provided in redis_host , redis_port , redis_password , redis_db options. Scaling with Redis tutorial \u00b6 Let's see how to start several Centrifugo nodes using Redis engine. We will start 3 Centrifugo nodes and all those nodes will be connected via Redis. First, you should have Redis running. As soon as it's running - we can launch 3 Centrifugo instances. Open your terminal and start first one: centrifugo --config=config.json --port=8000 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 If your Redis on the same machine and runs on its default port you can omit redis_host and redis_port options in command above. Then open another terminal and start another Centrifugo instance: centrifugo --config=config.json --port=8001 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 Note that we use another port number ( 8001 ) as port 8000 already busy by our first Centrifugo instance. If you are starting Centrifugo instances on different machines then you most probably can use the same port number ( 8000 or whatever you want) for all instances. And finally let's start third instance: centrifugo --config=config.json --port=8002 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 Now you have 3 Centrifugo instances running on ports 8000, 8001, 8002 and clients can connect to any of them. You can also send API requests to any of those nodes \u2013 as all nodes connected over Redis PUB/SUB message will be delivered to all interested clients on all nodes. To load balance clients between nodes you can use Nginx \u2013 you can find its configuration here in documentation. Redis Sentinel for high availability \u00b6 Centrifugo supports official way to add high availability to Redis - Redis Sentinel . For this you only need to utilize 2 Redis Engine options: redis_master_name and redis_sentinels . redis_master_name - is a name of master your Sentinels monitor. redis_sentinels - comma-separated addresses of Sentinel servers. At least one known server required. So you can start Centrifugo which will use Sentinels to discover redis master instance like this: centrifugo --config=config.json --engine=redis --redis_master_name=mymaster --redis_sentinels=\":26379\" Sentinel configuration files can look like this: port 26379 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 10000 sentinel failover-timeout mymaster 60000 You can find how to properly setup Sentinels in official documentation . Note that when your redis master instance down there will be small downtime interval until Sentinels discover a problem and come to quorum decision about new master. The length of this period depends on Sentinel configuration. Haproxy instead of Sentinel configuration \u00b6 Alternatively you can use Haproxy between Centrifugo and Redis to let it properly balance traffic to Redis master. In this case you still need to configure Sentinels but you can omit Sentinel specifics from Centrifugo configuration and just use Redis address as in simple non-HA case. For example you can use something like this in Haproxy config: listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:16379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin And then just point Centrifugo to this Haproxy: centrifugo --config=config.json --engine=redis --redis_host=localhost --redis_port=16379 Redis sharding \u00b6 Centrifugo has a built-in Redis sharding support. This resolves situation when Redis becoming a bottleneck on large Centrifugo setup. Redis is single-threaded server, it's very fast but it's power is not infinite so when your Redis approaches 100% CPU usage then sharding feature can help your application to scale. At moment Centrifugo supports simple comma-based approach to configuring Redis shards. Let's just look on examples. To start Centrifugo with 2 Redis shards on localhost running on port 6379 and port 6380: centrifugo --config=config.json --engine=redis --redis_port=6379,6380 To start Centrifugo with Redis instances on different hosts: centrifugo --config=config.json --engine=redis --redis_host=192.168.1.34,192.168.1.35 If you also need to customize AUTH password, Redis DB number then you can use redis_url option. Note, that due to how Redis PUB/SUB work it's not possible (and it's pretty useless anyway) to run shards in one Redis instances using different Redis DB numbers. When sharding enabled Centrifugo will spread channels and history/presence keys over configured Redis instances using consistent hashing algorithm. At moment we use Jump consistent hash algorithm (see paper and implementation ) Redis cluster \u00b6 Redis cluster supported since Centrifugo v2.5.0 Running Centrifugo with Redis cluster is simple and can be achieved using redis_cluster_addrs option. This is an array of strings. Each element of array is a comma-separated Redis cluster seed nodes. For example: { ... \"redis_cluster_addrs\" : [ \"localhost:30001,localhost:30002,localhost:30003\" ] } Actually you don't need to list all Redis cluster nodes in config \u2013 only several working nodes is enough to start. To set the same over environment variable: CENTRIFUGO_REDIS_CLUSTER_ADDRS = \"localhost:30001\" CENTRIFUGO_ENGINE = redis ./centrifugo If you need to shard data between several Redis clusters then simply add one more string with seed nodes of another cluster to this array: { ... \"redis_cluster_addrs\" : [ \"localhost:30001,localhost:30002,localhost:30003\" , \"localhost:30101,localhost:30102,localhost:30103\" ] } Sharding between different Redis clusters can make sense due to the fact how PUB/SUB works in Redis cluster. It does not scale linearly when adding nodes as all PUB/SUB messages got copied to every cluster node. See this discussion for more information on topic. To spread data between different Redis clusters Centrifugo uses the same consistent hashing algorithm described above (i.e. Jump ). To reproduce the same over environment variable use space to separate different clusters: CENTRIFUGO_REDIS_CLUSTER_ADDRS = \"localhost:30001,localhost:30002 localhost:30101,localhost:30102\" CENTRIFUGO_ENGINE = redis ./centrifugo Redis Streams \u00b6 Since Centrifugo v2.5.0 it's possible to use Redis Stream data structure to keep Publication history inside channels. Redis streams can help to reduce number of memory allocations Centrifugo does during message recovery process upon client reconnect inside large history streams. This can be critical for massive Centrifugo deployments that utilize recovery feature. To turn on using Redis streams use boolean option redis_streams , default false . Redis Streams can become default data structure to keep Publication history in Centrifugo v3. KeyDB \u00b6 Centrifugo Redis engine seamlessly works with KeyDB . KeyDB server is compatible with Redis and provides several additional features beyond. Though we can't give any promises about compatibility with KeyDB in future Centrifugo releases - while KeyDB is fully compatible with Redis things should work fine. That's why we consider this as EXPERIMENTAL feature. Use KeyDB instead of Redis only if you are really sure you need it. Nothing stops you from running several Redis instances per each core you have, configure sharding and obtain even better performance that KeyDB can provide (due to lack of synchronization between threads in Redis). In order to run Centrifugo with KeyDB all you need to do is use redis engine option and run KeyDB server instead of Redis.","title":"Engines"},{"location":"server/engines/#engines","text":"Memory engine Redis engine Engine in Centrifugo is responsible for publishing messages between nodes, handle PUB/SUB broker subscriptions, save/retrieve presence and history data. By default, Centrifugo uses Memory engine. There is also Redis engine available. The difference between them - with Memory engine you can start only one node of Centrifugo, while Redis engine allows running several nodes on different machines and they will be connected via Redis, will know about each other due to Redis and will also keep history and presence data in Redis instead of Centrifugo node process memory so this data can be accessed from each node. To set engine you can use engine configuration option. Available values are memory and redis . Default value is memory . For example to work with Redis engine: centrifugo --config=config.json --engine=redis Or just set engine in config: { ... \"engine\" : \"redis\" }","title":"Engines"},{"location":"server/engines/#memory-engine","text":"Supports only one node. Nice choice to start with. Supports all features keeping everything in Centrifugo node process memory. You don't need to install Redis when using this engine. Advantages: fast does not require separate Redis setup Disadvantages: does not allow scaling nodes (actually you still can scale Centrifugo with Memory engine but you have to publish data into each Centrifugo node and you won't have consistent state of presence) does not persist message history in channels between Centrifugo restarts Several configuration options related to Memory engine: memory_history_meta_ttl (int, default 0 ) - sets a time in seconds of history stream metadata expiration. Stream metadata is an information about current offset number in channel and epoch value. By default, metadata for channels does not expire. Though in some cases \u2013 when channels created for \u0430 short time and then not used anymore \u2013 created metadata can stay in memory while not actually useful. For example, you can have a personal user channel but after using your app for a while user left it forever. In long-term perspective this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than history retention period) can help. In this case unused channel metadata will eventually expire. Available since v2.5.0","title":"Memory engine"},{"location":"server/engines/#redis-engine","text":"Allows scaling Centrifugo nodes to different machines. Nodes will use Redis as message broker. Redis engine keeps presence and history data in Redis, uses Redis PUB/SUB for internal node communication. Minimal Redis version is 3.2.0 Several configuration options related to Redis engine: redis_host (string, default \"127.0.0.1\" ) - Redis server host redis_port (int, default 6379 ) - Redis server port redis_url (string, default \"\" ) - optional Redis connection URL redis_password (string, default \"\" ) - Redis password redis_db (int, default 0 ) - number of Redis db to use redis_tls (boolean, default false ) - enable Redis TLS connection (new in v2.0.2) redis_tls_skip_verify (boolean, default false ) - disable Redis TLS host verification (new in v2.0.2) redis_sentinels (string, default \"\" ) - comma separated list of Sentinels for HA redis_master_name (string, default \"\" ) - name of Redis master Sentinel monitors redis_prefix (string, default \"centrifugo\" ) \u2013 custom prefix to use for channels and keys in Redis redis_history_meta_ttl (int, default 0 ) - sets a time in seconds of history stream metadata expiration in Redis Engine. Meta key in Redis is a HASH that contains current offset number in channel and epoch value. By default, metadata for channels does not expire. Though in some cases \u2013 when channels created for \u0430 short time and then not used anymore \u2013 created stream metadata can stay in memory while not actually useful. For example, you can have a personal user channel but after using your app for a while user left it forever. In long-term perspective this can be an unwanted memory leak. Setting a reasonable value to this option (usually much bigger than history retention period) can help. In this case unused channel metadata will eventually expire. Available since v2.3.0 redis_streams (boolean, default false ) \u2013 turns on using Redis Streams instead of List data structure for keeping history All of these options can be set over configuration file. Some of them can be set over command-line arguments (see centrifugo -h output). Let's describe a bit more redis_url option. redis_url allows to set Redis connection parameters in a form of URL in format redis://:password@hostname:port/db_number . When redis_url set Centrifugo will use URL instead of values provided in redis_host , redis_port , redis_password , redis_db options.","title":"Redis engine"},{"location":"server/engines/#scaling-with-redis-tutorial","text":"Let's see how to start several Centrifugo nodes using Redis engine. We will start 3 Centrifugo nodes and all those nodes will be connected via Redis. First, you should have Redis running. As soon as it's running - we can launch 3 Centrifugo instances. Open your terminal and start first one: centrifugo --config=config.json --port=8000 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 If your Redis on the same machine and runs on its default port you can omit redis_host and redis_port options in command above. Then open another terminal and start another Centrifugo instance: centrifugo --config=config.json --port=8001 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 Note that we use another port number ( 8001 ) as port 8000 already busy by our first Centrifugo instance. If you are starting Centrifugo instances on different machines then you most probably can use the same port number ( 8000 or whatever you want) for all instances. And finally let's start third instance: centrifugo --config=config.json --port=8002 --engine=redis --redis_host=127.0.0.1 --redis_port=6379 Now you have 3 Centrifugo instances running on ports 8000, 8001, 8002 and clients can connect to any of them. You can also send API requests to any of those nodes \u2013 as all nodes connected over Redis PUB/SUB message will be delivered to all interested clients on all nodes. To load balance clients between nodes you can use Nginx \u2013 you can find its configuration here in documentation.","title":"Scaling with Redis tutorial"},{"location":"server/engines/#redis-sentinel-for-high-availability","text":"Centrifugo supports official way to add high availability to Redis - Redis Sentinel . For this you only need to utilize 2 Redis Engine options: redis_master_name and redis_sentinels . redis_master_name - is a name of master your Sentinels monitor. redis_sentinels - comma-separated addresses of Sentinel servers. At least one known server required. So you can start Centrifugo which will use Sentinels to discover redis master instance like this: centrifugo --config=config.json --engine=redis --redis_master_name=mymaster --redis_sentinels=\":26379\" Sentinel configuration files can look like this: port 26379 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 10000 sentinel failover-timeout mymaster 60000 You can find how to properly setup Sentinels in official documentation . Note that when your redis master instance down there will be small downtime interval until Sentinels discover a problem and come to quorum decision about new master. The length of this period depends on Sentinel configuration.","title":"Redis Sentinel for high availability"},{"location":"server/engines/#haproxy-instead-of-sentinel-configuration","text":"Alternatively you can use Haproxy between Centrifugo and Redis to let it properly balance traffic to Redis master. In this case you still need to configure Sentinels but you can omit Sentinel specifics from Centrifugo configuration and just use Redis address as in simple non-HA case. For example you can use something like this in Haproxy config: listen redis server redis-01 127.0.0.1:6380 check port 6380 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 server redis-02 127.0.0.1:6381 check port 6381 check inter 2s weight 1 inter 2s downinter 5s rise 10 fall 2 backup bind *:16379 mode tcp option tcpka option tcplog option tcp-check tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK balance roundrobin And then just point Centrifugo to this Haproxy: centrifugo --config=config.json --engine=redis --redis_host=localhost --redis_port=16379","title":"Haproxy instead of Sentinel configuration"},{"location":"server/engines/#redis-sharding","text":"Centrifugo has a built-in Redis sharding support. This resolves situation when Redis becoming a bottleneck on large Centrifugo setup. Redis is single-threaded server, it's very fast but it's power is not infinite so when your Redis approaches 100% CPU usage then sharding feature can help your application to scale. At moment Centrifugo supports simple comma-based approach to configuring Redis shards. Let's just look on examples. To start Centrifugo with 2 Redis shards on localhost running on port 6379 and port 6380: centrifugo --config=config.json --engine=redis --redis_port=6379,6380 To start Centrifugo with Redis instances on different hosts: centrifugo --config=config.json --engine=redis --redis_host=192.168.1.34,192.168.1.35 If you also need to customize AUTH password, Redis DB number then you can use redis_url option. Note, that due to how Redis PUB/SUB work it's not possible (and it's pretty useless anyway) to run shards in one Redis instances using different Redis DB numbers. When sharding enabled Centrifugo will spread channels and history/presence keys over configured Redis instances using consistent hashing algorithm. At moment we use Jump consistent hash algorithm (see paper and implementation )","title":"Redis sharding"},{"location":"server/engines/#redis-cluster","text":"Redis cluster supported since Centrifugo v2.5.0 Running Centrifugo with Redis cluster is simple and can be achieved using redis_cluster_addrs option. This is an array of strings. Each element of array is a comma-separated Redis cluster seed nodes. For example: { ... \"redis_cluster_addrs\" : [ \"localhost:30001,localhost:30002,localhost:30003\" ] } Actually you don't need to list all Redis cluster nodes in config \u2013 only several working nodes is enough to start. To set the same over environment variable: CENTRIFUGO_REDIS_CLUSTER_ADDRS = \"localhost:30001\" CENTRIFUGO_ENGINE = redis ./centrifugo If you need to shard data between several Redis clusters then simply add one more string with seed nodes of another cluster to this array: { ... \"redis_cluster_addrs\" : [ \"localhost:30001,localhost:30002,localhost:30003\" , \"localhost:30101,localhost:30102,localhost:30103\" ] } Sharding between different Redis clusters can make sense due to the fact how PUB/SUB works in Redis cluster. It does not scale linearly when adding nodes as all PUB/SUB messages got copied to every cluster node. See this discussion for more information on topic. To spread data between different Redis clusters Centrifugo uses the same consistent hashing algorithm described above (i.e. Jump ). To reproduce the same over environment variable use space to separate different clusters: CENTRIFUGO_REDIS_CLUSTER_ADDRS = \"localhost:30001,localhost:30002 localhost:30101,localhost:30102\" CENTRIFUGO_ENGINE = redis ./centrifugo","title":"Redis cluster"},{"location":"server/engines/#redis-streams","text":"Since Centrifugo v2.5.0 it's possible to use Redis Stream data structure to keep Publication history inside channels. Redis streams can help to reduce number of memory allocations Centrifugo does during message recovery process upon client reconnect inside large history streams. This can be critical for massive Centrifugo deployments that utilize recovery feature. To turn on using Redis streams use boolean option redis_streams , default false . Redis Streams can become default data structure to keep Publication history in Centrifugo v3.","title":"Redis Streams"},{"location":"server/engines/#keydb","text":"Centrifugo Redis engine seamlessly works with KeyDB . KeyDB server is compatible with Redis and provides several additional features beyond. Though we can't give any promises about compatibility with KeyDB in future Centrifugo releases - while KeyDB is fully compatible with Redis things should work fine. That's why we consider this as EXPERIMENTAL feature. Use KeyDB instead of Redis only if you are really sure you need it. Nothing stops you from running several Redis instances per each core you have, configure sharding and obtain even better performance that KeyDB can provide (due to lack of synchronization between threads in Redis). In order to run Centrifugo with KeyDB all you need to do is use redis engine option and run KeyDB server instead of Redis.","title":"KeyDB"},{"location":"server/grpc_api/","text":"Server GRPC API \u00b6 Centrifugo also supports GRPC API. With GRPC it's possible to communicate with Centrifugo using more compact binary representation of commands and use the power of HTTP/2 which is the transport behind GRPC. GRPC API is also useful if you want to publish binary data to Centrifugo channels. You can enable GRPC API in Centrifugo using grpc_api option: ./centrifugo --config=config.json --grpc_api As always in Centrifugo you can just add grpc_api option to configuration file: { ... \"grpc_api\" : true } By default, GRPC will be served on port 10000 but you can change it using grpc_api_port option. Now as soon as Centrifugo started you can send GRPC commands to it. To do this get our API Protocol Buffer definitions from this file . Then see GRPC docs specific to your language to find out how to generate client code from definitions and use generated code to communicate with Centrifugo. Example for Python \u00b6 For example for Python you need to run sth like this according to GRPC docs: python -m grpc_tools.protoc -I../../protos --python_out=. --grpc_python_out=. api.proto As soon as you run command you will have 2 generated files: api_pb2.py and api_pb2_grpc.py . Now all you need is to write simple program that uses generated code and sends GRPC requests to Centrifugo: import grpc import api_pb2_grpc as api_grpc import api_pb2 as api_pb channel = grpc . insecure_channel ( 'localhost:10000' ) stub = api_grpc . CentrifugoStub ( channel ) try : resp = stub . Info ( api_pb . InfoRequest ()) except grpc . RpcError as err : # GRPC level error. print ( err . code (), err . details ()) else : if resp . error . code : # Centrifugo server level error. print ( resp . error . code , resp . error . message ) else : print ( resp . result ) Note that you need to explicitly handle Centrifugo API level error which is not transformed automatically into GRPC protocol level error. Example for Go \u00b6 Here is a simple example on how to run Centrifugo with GRPC Go client. First start Centrifugo itself: centrifugo --config config.json --grpc_api In another terminal tab: cd ~ mkdir centrifugo_grpc_example cd centrifugo_grpc_example/ wget https://raw.githubusercontent.com/centrifugal/centrifugo/master/misc/proto/api.proto -O api.proto protoc api.proto --go_out = plugins = grpc,import_path = main:./ touch main.go Put the following code to main.go file (created on last step above): package main import ( \"context\" \"log\" \"time\" \"google.golang.org/grpc\" ) func main () { conn , err := grpc . Dial ( \"localhost:10000\" , grpc . WithInsecure ()) if err != nil { log . Fatalln ( err ) } defer conn . Close () client := NewCentrifugoClient ( conn ) for { resp , err := client . Publish ( context . Background (), & PublishRequest { Channel : \"chat:index\" , Data : [] byte ( `{\"input\": \"hello from GRPC\"}` ), }) if err != nil { log . Printf ( \"Transport level error: %v\" , err ) } else { if resp . GetError () != nil { respError := resp . GetError () log . Printf ( \"Error %d (%s)\" , respError . Code , respError . Message ) } else { log . Println ( \"Successfully published\" ) } } time . Sleep ( time . Second ) } } Then run: GO111MODULE = on go run *.go The program starts and periodically publishes the same payload into chat:index channel.","title":"Server GRPC API"},{"location":"server/grpc_api/#server-grpc-api","text":"Centrifugo also supports GRPC API. With GRPC it's possible to communicate with Centrifugo using more compact binary representation of commands and use the power of HTTP/2 which is the transport behind GRPC. GRPC API is also useful if you want to publish binary data to Centrifugo channels. You can enable GRPC API in Centrifugo using grpc_api option: ./centrifugo --config=config.json --grpc_api As always in Centrifugo you can just add grpc_api option to configuration file: { ... \"grpc_api\" : true } By default, GRPC will be served on port 10000 but you can change it using grpc_api_port option. Now as soon as Centrifugo started you can send GRPC commands to it. To do this get our API Protocol Buffer definitions from this file . Then see GRPC docs specific to your language to find out how to generate client code from definitions and use generated code to communicate with Centrifugo.","title":"Server GRPC API"},{"location":"server/grpc_api/#example-for-python","text":"For example for Python you need to run sth like this according to GRPC docs: python -m grpc_tools.protoc -I../../protos --python_out=. --grpc_python_out=. api.proto As soon as you run command you will have 2 generated files: api_pb2.py and api_pb2_grpc.py . Now all you need is to write simple program that uses generated code and sends GRPC requests to Centrifugo: import grpc import api_pb2_grpc as api_grpc import api_pb2 as api_pb channel = grpc . insecure_channel ( 'localhost:10000' ) stub = api_grpc . CentrifugoStub ( channel ) try : resp = stub . Info ( api_pb . InfoRequest ()) except grpc . RpcError as err : # GRPC level error. print ( err . code (), err . details ()) else : if resp . error . code : # Centrifugo server level error. print ( resp . error . code , resp . error . message ) else : print ( resp . result ) Note that you need to explicitly handle Centrifugo API level error which is not transformed automatically into GRPC protocol level error.","title":"Example for Python"},{"location":"server/grpc_api/#example-for-go","text":"Here is a simple example on how to run Centrifugo with GRPC Go client. First start Centrifugo itself: centrifugo --config config.json --grpc_api In another terminal tab: cd ~ mkdir centrifugo_grpc_example cd centrifugo_grpc_example/ wget https://raw.githubusercontent.com/centrifugal/centrifugo/master/misc/proto/api.proto -O api.proto protoc api.proto --go_out = plugins = grpc,import_path = main:./ touch main.go Put the following code to main.go file (created on last step above): package main import ( \"context\" \"log\" \"time\" \"google.golang.org/grpc\" ) func main () { conn , err := grpc . Dial ( \"localhost:10000\" , grpc . WithInsecure ()) if err != nil { log . Fatalln ( err ) } defer conn . Close () client := NewCentrifugoClient ( conn ) for { resp , err := client . Publish ( context . Background (), & PublishRequest { Channel : \"chat:index\" , Data : [] byte ( `{\"input\": \"hello from GRPC\"}` ), }) if err != nil { log . Printf ( \"Transport level error: %v\" , err ) } else { if resp . GetError () != nil { respError := resp . GetError () log . Printf ( \"Error %d (%s)\" , respError . Code , respError . Message ) } else { log . Println ( \"Successfully published\" ) } } time . Sleep ( time . Second ) } } Then run: GO111MODULE = on go run *.go The program starts and periodically publishes the same payload into chat:index channel.","title":"Example for Go"},{"location":"server/http_api/","text":"Server HTTP API \u00b6 HTTP API is a way to send commands to Centrifugo. Why we need API? If you look at configuration options you see an option called publish defined on configuration top-level and for a channel namespace. When turned on this option allows browser clients to publish into channels directly. If client publishes a message into channel directly \u2013 your application will not receive that message (it just goes through Centrifugo towards subscribed clients). This pattern can be useful sometimes but in most cases you first need to send new event from client to backend over non-Centrifugo transport (for example via AJAX request in web application), then process it on application backend side \u2013 probably validate, save into main app database \u2013 and then publish into Centrifugo using HTTP API so Centrifugo broadcast message to all clients subscribed on a channel. Server API works on /api endpoint. It's very simple to use: you just have to send POST request with JSON command to this endpoint. In this chapter we will look at API protocol internals - for new API client library authors and just if you are curious how existing API clients work. API request is a POST HTTP request with application/json Content-Type and JSON payload in request body. API protected by api_key set in Centrifugo configuration. I.e. api_key must be added to config, like: { ... \"api_key\" : \"<YOUR API KEY>\" } This API key must be set in request Authorization header in this way: Authorization: apikey <KEY> Starting from Centrifugo v2.2.7 it's also possible to pass API key over URL query param. This solves some edge cases where it's not possible to use Authorization header. Simply add ?api_key=<YOUR API KEY> query param to API endpoint. Keep in mind that passing API key in Authorization header is a recommended way. It's possible to disable API key check on Centrifugo side using api_insecure configuration option. Be sure to protect API endpoint by firewall rules in this case to prevent anyone in internet to send commands over your unprotected Centrifugo API. API key auth is not very safe for man-in-the-middle so recommended way is running Centrifugo with TLS (we are in 2018 in the end). Command is a JSON object with two properties: method and params . method is a name of command you want to call. params is an object with command arguments. There are several commands available. Let's investigate each of available server API commands. publish \u00b6 Publish command allows publishing data into a channel. It looks like this: { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"chat\" , \"data\" : { \"text\" : \"hello\" } } } Let's apply all information said above and send publish command to Centrifugo. We will send request using requests library for Python. import json import requests command = { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"docs\" , \"data\" : { \"content\" : \"1\" } } } api_key = \"YOUR_API_KEY\" data = json . dumps ( command ) headers = { 'Content-type' : 'application/json' , 'Authorization' : 'apikey ' + api_key } resp = requests . post ( \"https://centrifuge.example.com/api\" , data = data , headers = headers ) print ( resp . json ()) The same using httpie console tool: $ echo '{\"method\": \"publish\", \"params\": {\"channel\": \"chat\", \"data\": {\"text\": \"hello\"}}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" -vvv POST /api HTTP/1.1 Accept: application/json, */* Accept-Encoding: gzip, deflate Authorization: apikey KEY Connection: keep-alive Content-Length: 80 Content-Type: application/json Host: localhost:8000 User-Agent: HTTPie/0.9.8 { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"chat\" , \"data\" : { \"text\" : \"hello\" } } } HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Thu, 17 May 2018 22 :01:42 GMT {} In case of error response object will contain error field: $ echo '{\"method\": \"publish\", \"params\": {\"channel\": \"unknown:chat\", \"data\": {\"text\": \"hello\"}}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 55 Content-Type: application/json Date: Thu, 17 May 2018 22 :03:09 GMT { \"error\" : { \"code\" : 102 , \"message\" : \"namespace not found\" } } error object contains error code and message - this also the same for other commands described below. publish command is the main command you need. Again - remember that we have client API libraries that can help you avoid some boilerplate we just wrote and help to properly handle error responses from Centrifugo. Let's look at other available commands: broadcast \u00b6 Similar to publish but allows to send the same data into many channels. { \"method\" : \"broadcast\" , \"params\" : { \"channels\" : [ \"CHANNEL_1\" , \"CHANNEL_2\" ], \"data\" : { \"text\" : \"hello\" } } } unsubscribe \u00b6 unsubscribe allows unsubscribing user from a channel. params is an object with two keys: channel and user (user ID you want to unsubscribe) { \"method\" : \"unsubscribe\" , \"params\" : { \"channel\" : \"CHANNEL NAME\" , \"user\" : \"USER ID\" } } disconnect \u00b6 disconnect allows disconnecting user by ID. params in an object with user key. { \"method\" : \"disconnect\" , \"params\" : { \"user\" : \"USER ID\" } } presence \u00b6 presence allows getting channel presence information (all clients currently subscribed on this channel). params is an object with channel key. { \"method\" : \"presence\" , \"params\" : { \"channel\" : \"chat\" } } Example: fz@centrifugo: echo '{\"method\": \"presence\", \"params\": {\"channel\": \"chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 127 Content-Type: application/json Date: Thu, 17 May 2018 22 :13:17 GMT { \"result\" : { \"presence\" : { \"c54313b2-0442-499a-a70c-051f8588020f\" : { \"client\" : \"c54313b2-0442-499a-a70c-051f8588020f\" , \"user\" : \"42\" } , \"adad13b1-0442-499a-a70c-051f858802da\" : { \"client\" : \"adad13b1-0442-499a-a70c-051f858802da\" , \"user\" : \"42\" } } } } presence_stats \u00b6 presence_stats allows getting short channel presence information. { \"method\" : \"presence_stats\" , \"params\" : { \"channel\" : \"chat\" } } Example: $ echo '{\"method\": \"presence_stats\", \"params\": {\"channel\": \"public:chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22 :09:44 GMT { \"result\" : { \"num_clients\" : 0 , \"num_users\" : 0 } } history \u00b6 history allows getting channel history information (list of last messages published into channel). params is an object with channel key: { \"method\" : \"history\" , \"params\" : { \"channel\" : \"chat\" } } Example: $ echo '{\"method\": \"history\", \"params\": {\"channel\": \"public:chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 87 Content-Type: application/json Date: Thu, 17 May 2018 22 :14:10 GMT { \"result\" : { \"publications\" : [ { \"data\" : { \"text\" : \"hello\" } , \"uid\" : \"BWcn14OTBrqUhTXyjNg0fg\" } , { \"data\" : { \"text\" : \"hi!\" } , \"uid\" : \"Ascn14OTBrq14OXyjNg0hg\" } ] } } channels \u00b6 channels allows getting list of active (with one or more subscribers) channels. { \"method\" : \"channels\" , \"params\" : {} } Example: $ echo '{\"method\": \"channels\", \"params\": {}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 27 Content-Type: application/json Date: Thu, 17 May 2018 22 :08:31 GMT { \"result\" : { \"channels\" : [ \"chat\" ] } } Keep in mind that as channels API command returns all active channel snapshot it can be really heavy for massive deployments. At moment there is no way to paginate over channels list and we don't know a case where this could be useful and not error prone. At moment we mostly suppose that channels command will be used in development process and in not very massive Centrifugo setups (with no more than 10k channels). Also channels command is considered optional in engine implementations. info \u00b6 info method allows getting information about running Centrifugo nodes. { \"method\" : \"info\" , \"params\" : {} } Example: $ echo '{\"method\": \"info\", \"params\": {}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 184 Content-Type: application/json Date: Thu, 17 May 2018 22 :07:58 GMT { \"result\" : { \"nodes\" : [ { \"name\" : \"Alexanders-MacBook-Pro.local_8000\" , \"num_channels\" : 0 , \"num_clients\" : 0 , \"num_users\" : 0 , \"uid\" : \"f844a2ed-5edf-4815-b83c-271974003db9\" , \"uptime\" : 0 , \"version\" : \"\" } ] } } Command pipelining \u00b6 It's possible to combine several commands into one request to Centrifugo. To do this use JSON streaming format. This can improve server throughput and reduce traffic travelling around.","title":"Server HTTP API"},{"location":"server/http_api/#server-http-api","text":"HTTP API is a way to send commands to Centrifugo. Why we need API? If you look at configuration options you see an option called publish defined on configuration top-level and for a channel namespace. When turned on this option allows browser clients to publish into channels directly. If client publishes a message into channel directly \u2013 your application will not receive that message (it just goes through Centrifugo towards subscribed clients). This pattern can be useful sometimes but in most cases you first need to send new event from client to backend over non-Centrifugo transport (for example via AJAX request in web application), then process it on application backend side \u2013 probably validate, save into main app database \u2013 and then publish into Centrifugo using HTTP API so Centrifugo broadcast message to all clients subscribed on a channel. Server API works on /api endpoint. It's very simple to use: you just have to send POST request with JSON command to this endpoint. In this chapter we will look at API protocol internals - for new API client library authors and just if you are curious how existing API clients work. API request is a POST HTTP request with application/json Content-Type and JSON payload in request body. API protected by api_key set in Centrifugo configuration. I.e. api_key must be added to config, like: { ... \"api_key\" : \"<YOUR API KEY>\" } This API key must be set in request Authorization header in this way: Authorization: apikey <KEY> Starting from Centrifugo v2.2.7 it's also possible to pass API key over URL query param. This solves some edge cases where it's not possible to use Authorization header. Simply add ?api_key=<YOUR API KEY> query param to API endpoint. Keep in mind that passing API key in Authorization header is a recommended way. It's possible to disable API key check on Centrifugo side using api_insecure configuration option. Be sure to protect API endpoint by firewall rules in this case to prevent anyone in internet to send commands over your unprotected Centrifugo API. API key auth is not very safe for man-in-the-middle so recommended way is running Centrifugo with TLS (we are in 2018 in the end). Command is a JSON object with two properties: method and params . method is a name of command you want to call. params is an object with command arguments. There are several commands available. Let's investigate each of available server API commands.","title":"Server HTTP API"},{"location":"server/http_api/#publish","text":"Publish command allows publishing data into a channel. It looks like this: { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"chat\" , \"data\" : { \"text\" : \"hello\" } } } Let's apply all information said above and send publish command to Centrifugo. We will send request using requests library for Python. import json import requests command = { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"docs\" , \"data\" : { \"content\" : \"1\" } } } api_key = \"YOUR_API_KEY\" data = json . dumps ( command ) headers = { 'Content-type' : 'application/json' , 'Authorization' : 'apikey ' + api_key } resp = requests . post ( \"https://centrifuge.example.com/api\" , data = data , headers = headers ) print ( resp . json ()) The same using httpie console tool: $ echo '{\"method\": \"publish\", \"params\": {\"channel\": \"chat\", \"data\": {\"text\": \"hello\"}}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" -vvv POST /api HTTP/1.1 Accept: application/json, */* Accept-Encoding: gzip, deflate Authorization: apikey KEY Connection: keep-alive Content-Length: 80 Content-Type: application/json Host: localhost:8000 User-Agent: HTTPie/0.9.8 { \"method\" : \"publish\" , \"params\" : { \"channel\" : \"chat\" , \"data\" : { \"text\" : \"hello\" } } } HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Thu, 17 May 2018 22 :01:42 GMT {} In case of error response object will contain error field: $ echo '{\"method\": \"publish\", \"params\": {\"channel\": \"unknown:chat\", \"data\": {\"text\": \"hello\"}}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 55 Content-Type: application/json Date: Thu, 17 May 2018 22 :03:09 GMT { \"error\" : { \"code\" : 102 , \"message\" : \"namespace not found\" } } error object contains error code and message - this also the same for other commands described below. publish command is the main command you need. Again - remember that we have client API libraries that can help you avoid some boilerplate we just wrote and help to properly handle error responses from Centrifugo. Let's look at other available commands:","title":"publish"},{"location":"server/http_api/#broadcast","text":"Similar to publish but allows to send the same data into many channels. { \"method\" : \"broadcast\" , \"params\" : { \"channels\" : [ \"CHANNEL_1\" , \"CHANNEL_2\" ], \"data\" : { \"text\" : \"hello\" } } }","title":"broadcast"},{"location":"server/http_api/#unsubscribe","text":"unsubscribe allows unsubscribing user from a channel. params is an object with two keys: channel and user (user ID you want to unsubscribe) { \"method\" : \"unsubscribe\" , \"params\" : { \"channel\" : \"CHANNEL NAME\" , \"user\" : \"USER ID\" } }","title":"unsubscribe"},{"location":"server/http_api/#disconnect","text":"disconnect allows disconnecting user by ID. params in an object with user key. { \"method\" : \"disconnect\" , \"params\" : { \"user\" : \"USER ID\" } }","title":"disconnect"},{"location":"server/http_api/#presence","text":"presence allows getting channel presence information (all clients currently subscribed on this channel). params is an object with channel key. { \"method\" : \"presence\" , \"params\" : { \"channel\" : \"chat\" } } Example: fz@centrifugo: echo '{\"method\": \"presence\", \"params\": {\"channel\": \"chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 127 Content-Type: application/json Date: Thu, 17 May 2018 22 :13:17 GMT { \"result\" : { \"presence\" : { \"c54313b2-0442-499a-a70c-051f8588020f\" : { \"client\" : \"c54313b2-0442-499a-a70c-051f8588020f\" , \"user\" : \"42\" } , \"adad13b1-0442-499a-a70c-051f858802da\" : { \"client\" : \"adad13b1-0442-499a-a70c-051f858802da\" , \"user\" : \"42\" } } } }","title":"presence"},{"location":"server/http_api/#presence_stats","text":"presence_stats allows getting short channel presence information. { \"method\" : \"presence_stats\" , \"params\" : { \"channel\" : \"chat\" } } Example: $ echo '{\"method\": \"presence_stats\", \"params\": {\"channel\": \"public:chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 43 Content-Type: application/json Date: Thu, 17 May 2018 22 :09:44 GMT { \"result\" : { \"num_clients\" : 0 , \"num_users\" : 0 } }","title":"presence_stats"},{"location":"server/http_api/#history","text":"history allows getting channel history information (list of last messages published into channel). params is an object with channel key: { \"method\" : \"history\" , \"params\" : { \"channel\" : \"chat\" } } Example: $ echo '{\"method\": \"history\", \"params\": {\"channel\": \"public:chat\"}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 87 Content-Type: application/json Date: Thu, 17 May 2018 22 :14:10 GMT { \"result\" : { \"publications\" : [ { \"data\" : { \"text\" : \"hello\" } , \"uid\" : \"BWcn14OTBrqUhTXyjNg0fg\" } , { \"data\" : { \"text\" : \"hi!\" } , \"uid\" : \"Ascn14OTBrq14OXyjNg0hg\" } ] } }","title":"history"},{"location":"server/http_api/#channels","text":"channels allows getting list of active (with one or more subscribers) channels. { \"method\" : \"channels\" , \"params\" : {} } Example: $ echo '{\"method\": \"channels\", \"params\": {}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 27 Content-Type: application/json Date: Thu, 17 May 2018 22 :08:31 GMT { \"result\" : { \"channels\" : [ \"chat\" ] } } Keep in mind that as channels API command returns all active channel snapshot it can be really heavy for massive deployments. At moment there is no way to paginate over channels list and we don't know a case where this could be useful and not error prone. At moment we mostly suppose that channels command will be used in development process and in not very massive Centrifugo setups (with no more than 10k channels). Also channels command is considered optional in engine implementations.","title":"channels"},{"location":"server/http_api/#info","text":"info method allows getting information about running Centrifugo nodes. { \"method\" : \"info\" , \"params\" : {} } Example: $ echo '{\"method\": \"info\", \"params\": {}}' | http \"localhost:8000/api\" Authorization: \"apikey KEY\" HTTP/1.1 200 OK Content-Length: 184 Content-Type: application/json Date: Thu, 17 May 2018 22 :07:58 GMT { \"result\" : { \"nodes\" : [ { \"name\" : \"Alexanders-MacBook-Pro.local_8000\" , \"num_channels\" : 0 , \"num_clients\" : 0 , \"num_users\" : 0 , \"uid\" : \"f844a2ed-5edf-4815-b83c-271974003db9\" , \"uptime\" : 0 , \"version\" : \"\" } ] } }","title":"info"},{"location":"server/http_api/#command-pipelining","text":"It's possible to combine several commands into one request to Centrifugo. To do this use JSON streaming format. This can improve server throughput and reduce traffic travelling around.","title":"Command pipelining"},{"location":"server/private_channels/","text":"Private channels \u00b6 In channels chapter we mentioned private channels. This chapter has more information about private channel mechanism in Centrifugo. All channels starting with $ considered private. In this case your backend should additionally provide token for subscription request. The way how this token obtained varies depending on client implementation. For example in Javascript client AJAX POST request automatically sent to /centrifuge/subscribe endpoint on every private channel subscription attempt. Other client libraries can provide a hook for your custom code that will obtain private channel subscription token from application backend. Private channel subscription token is also JWT (like connection token described in authentication chapter ). But it has different claims. Note Connection token and private channel subscription token are different entities. Though both are JWT, and you can generate them using any JWT library. Note Even when authorizing subscription to private channel with private subscription JWT you should set a proper connection JWT for a client as it provides user authentication details to Centrifugo. Note When you need to use namespace for private channel then the name of namespace should be written after $ symbol, i.e. if you have namespace name chat then private channel which belongs to that namespace must be written as sth like $chat:stream . Supported JWT algorithms for private subscription tokens match algorithms to create connection JWT. Claims \u00b6 Private channel subscription token claims are: client , channel , info , b64info , exp and eto . What do they mean? Let's describe in detail. client \u00b6 Required. Client ID which wants to subscribe on a channel ( string ). Note Centrifugo server sets a unique client ID for each incoming connection. This client ID regenerated on every reconnect. You must use this client ID for private channel subscription token. If you are using centrifuge-js library then Client ID and Subscription Channels will be automaticaly added to POST request. In other cases refer to specific client documentation (in most cases you will have client ID in private subscription event context) channel \u00b6 Required. Channel that client tries to subscribe to ( string ). info \u00b6 Optional. Additional information for connection inside this channel ( valid JSON ). b64info \u00b6 Optional. Additional information for connection inside this channel in base64 format ( string ). exp \u00b6 Optional. This is standard JWT claim that allows to set private channel subscription token expiration time. At moment if subscription token expires client connection will be closed and client will try to reconnect. In most cases you don't need this and should prefer using exp of connection token to deactivate connection. But if you need more granular per-channel control this may fit your needs. Once exp set in token every subscription token must be periodically refreshed. Refer to specific client documentation in order to see how to refresh subscription tokens. eto \u00b6 Optional. An eto boolean flag can be used to indicate that Centrifugo must only check token expiration but not turn on Subscription expiration checks on server side. This allows to implement one-time subcription tokens. Example \u00b6 So to generate subscription token you can use something like this in Python (assuming client ID is XXX and private channel is $gossips ): import jwt token = jwt . encode ({ \"client\" : \"XXX\" , \"channel\" : \"$gossips\" }, \"secret\" , algorithm = \"HS256\" ) . decode () print ( token ) Where \"secret\" is the token_hmac_secret_key from Centrifugo configuration (we use HMAC tokens in this example which relies on shared secret key, for RSA tokens you need to use private key known only by your backend).","title":"Private channels"},{"location":"server/private_channels/#private-channels","text":"In channels chapter we mentioned private channels. This chapter has more information about private channel mechanism in Centrifugo. All channels starting with $ considered private. In this case your backend should additionally provide token for subscription request. The way how this token obtained varies depending on client implementation. For example in Javascript client AJAX POST request automatically sent to /centrifuge/subscribe endpoint on every private channel subscription attempt. Other client libraries can provide a hook for your custom code that will obtain private channel subscription token from application backend. Private channel subscription token is also JWT (like connection token described in authentication chapter ). But it has different claims. Note Connection token and private channel subscription token are different entities. Though both are JWT, and you can generate them using any JWT library. Note Even when authorizing subscription to private channel with private subscription JWT you should set a proper connection JWT for a client as it provides user authentication details to Centrifugo. Note When you need to use namespace for private channel then the name of namespace should be written after $ symbol, i.e. if you have namespace name chat then private channel which belongs to that namespace must be written as sth like $chat:stream . Supported JWT algorithms for private subscription tokens match algorithms to create connection JWT.","title":"Private channels"},{"location":"server/private_channels/#claims","text":"Private channel subscription token claims are: client , channel , info , b64info , exp and eto . What do they mean? Let's describe in detail.","title":"Claims"},{"location":"server/private_channels/#client","text":"Required. Client ID which wants to subscribe on a channel ( string ). Note Centrifugo server sets a unique client ID for each incoming connection. This client ID regenerated on every reconnect. You must use this client ID for private channel subscription token. If you are using centrifuge-js library then Client ID and Subscription Channels will be automaticaly added to POST request. In other cases refer to specific client documentation (in most cases you will have client ID in private subscription event context)","title":"client"},{"location":"server/private_channels/#channel","text":"Required. Channel that client tries to subscribe to ( string ).","title":"channel"},{"location":"server/private_channels/#info","text":"Optional. Additional information for connection inside this channel ( valid JSON ).","title":"info"},{"location":"server/private_channels/#b64info","text":"Optional. Additional information for connection inside this channel in base64 format ( string ).","title":"b64info"},{"location":"server/private_channels/#exp","text":"Optional. This is standard JWT claim that allows to set private channel subscription token expiration time. At moment if subscription token expires client connection will be closed and client will try to reconnect. In most cases you don't need this and should prefer using exp of connection token to deactivate connection. But if you need more granular per-channel control this may fit your needs. Once exp set in token every subscription token must be periodically refreshed. Refer to specific client documentation in order to see how to refresh subscription tokens.","title":"exp"},{"location":"server/private_channels/#eto","text":"Optional. An eto boolean flag can be used to indicate that Centrifugo must only check token expiration but not turn on Subscription expiration checks on server side. This allows to implement one-time subcription tokens.","title":"eto"},{"location":"server/private_channels/#example","text":"So to generate subscription token you can use something like this in Python (assuming client ID is XXX and private channel is $gossips ): import jwt token = jwt . encode ({ \"client\" : \"XXX\" , \"channel\" : \"$gossips\" }, \"secret\" , algorithm = \"HS256\" ) . decode () print ( token ) Where \"secret\" is the token_hmac_secret_key from Centrifugo configuration (we use HMAC tokens in this example which relies on shared secret key, for RSA tokens you need to use private key known only by your backend).","title":"Example"},{"location":"server/protobuf/","text":"Protobuf binary protocol \u00b6 In most cases you will use Centrifugo with JSON protocol which is used by default. It consists of simple human-readable frames that can be easily inspected. Also it's a very simple task to publish JSON encoded data to HTTP API endpoint. You may want to use binary Protobuf client protocol if: you want less traffic on wire as Protobuf is very compact you want maximum performance as Protobuf encoding/decoding is very efficient you can sacrifice human-readable JSON for your application Binary protobuf protocol only works for raw Websocket connections (as SockJS can't deal with binary). With most clients to use binary you just need to provide query parameter format to Websocket URL, so final URL look like: wss://centrifugo.example.com/connection/websocket?format=protobuf After doing this Centrifugo will use binary frames to pass data between client and server. Your application specific payload can be random bytes. Note You still can continue to encode your application specific data as JSON when using Protobuf protocol thus have a possibility to coexist with clients that use JSON protocol on the same Centrifugo installation inside the same channels.","title":"Protobuf protocol"},{"location":"server/protobuf/#protobuf-binary-protocol","text":"In most cases you will use Centrifugo with JSON protocol which is used by default. It consists of simple human-readable frames that can be easily inspected. Also it's a very simple task to publish JSON encoded data to HTTP API endpoint. You may want to use binary Protobuf client protocol if: you want less traffic on wire as Protobuf is very compact you want maximum performance as Protobuf encoding/decoding is very efficient you can sacrifice human-readable JSON for your application Binary protobuf protocol only works for raw Websocket connections (as SockJS can't deal with binary). With most clients to use binary you just need to provide query parameter format to Websocket URL, so final URL look like: wss://centrifugo.example.com/connection/websocket?format=protobuf After doing this Centrifugo will use binary frames to pass data between client and server. Your application specific payload can be random bytes. Note You still can continue to encode your application specific data as JSON when using Protobuf protocol thus have a possibility to coexist with clients that use JSON protocol on the same Centrifugo installation inside the same channels.","title":"Protobuf binary protocol"},{"location":"server/protocol/","text":"Client protocol \u00b6 This chapter describes internal client-server protocol in details to help developers build new client libraries and understand how existing client libraries work. Note that you can always look at existing client implementations in case of any questions. Client implementation checklist \u00b6 First we will look at list of features client library should support. Depending on client implementation some features can be not implemented. If you an author of client library you can use this list as checklist. Note Field and method names presented in this checklist can have different names depending on the programmer's taste and language style guide. So, client: Should work both with Centrifugo and Centrifuge library based server. To work with Centrifugo client must have a method to set connection token. To work with Centrifuge lib client must provide a method to set custom headers (the only exception is browser clients where browser automatically sets headers with respect to domain rules) Should allow to use JSON payload Should allow to use binary payload (actually you can only implement Protobuf protocol as you can pass JSON over it) Must handle cases when many different Replies received in one websocket frame. In case of JSON protocol newline-delimited JSON-streaming format is used to combine several Replies into one websocket frame. In case of Protobuf protocol varint length-prefixed format is used Must survive server reload/restart and internet connection lost. In this case client must try to reconnect with exponentioal backoff strategy Must have several callback methods: onConnect , onDisconnect . Depending on implementation you can also use onError callback for critical errors that could not be gracefully handled. Should also have onMessage callback to handle async messages from server. Must have method to subscribe on channel and set several event handlers: onPublish , onJoin , onLeave , onSubscribeSuccess , onSubscribeError , onUnsubscribe . After subscribe method called it should return Subscription object to caller. This subscription object in turn should have some methods: publish , unsubscribe , subscribe , history , presence , presence_stats . Should have publish method to publish into channel without subscribing to it. Should have rpc method to send RPC request to server. Should have send method to send asynchronous message to server (without waiting response). Should handle disconnect reason. In case of Websocket it is sent by server in CLOSE Websocket frame. This is a string containing JSON object with fields: reason (string) and reconnect (bool). Client should give users access to these fields in disconnect event and automatically follow reconnect advice Must send periodic ping commands to server and thus detect broken connection. If no ping reply received from server in configured window reconnect workflow must be initiated Should fully reconnect if subscription request timed out. Timeout can be configured by client library users. Should send commands to server with timeout and handle timeout error - depending on method called timeout error handling can differ a bit. For example as said above timeout on subscription request results in full client reconnect workflow. Should support connection token refresh mechanism Should support private channel subscriptions and private subscription token refresh mechanism Should automatically recover messages after reconnect and set appropriate fields in subscribe event context. Two important fields in onSubscribeSuccess event context are recovered and isResubscribe . First field let user know what server thinks about subscription state - were all messages recovered or not. The second field must only be true if resubscribe was caused by temporary network connection lost. If user initiated resubscribe himself (calling unsubscribe method and then subscribe method) then recover workflow should not be used and isResubscribe must be false . Below in this document we will describe protocol concepts in detail. This document describes protocol specifics for Websocket transport which supports binary and text formats to transfer data. As Centrifuge has various types of messages it serializes protocol messages using JSON or Protobuf (in case of binary websockets). Note SockJS works almost the same way as JSON websocket described here but has its own extra framing on top of Centrifuge protocol messages. SockJS can only work with JSON - it's not possible to transfer binary data over it. SockJS is only needed as fallback to Websocket in web browsers. Top level framing \u00b6 Centrifuge protocol defined in Protobuf schema . That schema is a source of truth and all protocol description below describes messages from that schema. Client sends Command to server. Server sends Reply to client. One request from client to server and one response from server to client can have more than one Command or Reply . When JSON format is used then many Command can be sent from client to server in JSON streaming line-delimited format. I.e. each individual Command encoded to JSON and then commands joined together using new line symbol \\n : { \"id\" : 1 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch1\" }} { \"id\" : 2 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch2\" }} For example here is how we do this in Javascript client when JSON format used: function encodeCommands ( commands ) { const encodedCommands = []; for ( const i in commands ) { if ( commands . hasOwnProperty ( i )) { encodedCommands . push ( JSON . stringify ( commands [ i ])); } } return encodedCommands . join ( '\\n' ); } Note This doc will use JSON format for examples because it's human-readable. Everything said here for JSON is also true for Protobuf encoded case. The only difference is how several individual Command or server Reply joined into one request \u2013 see below. Note Method is made as ENUM in protobuf schema and can be sent as integer value but it's possible to send it as string in JSON case \u2013 this was made to make JSON protocol human-friendly. When Protobuf format is used then many Command can be sent from client to server in length-delimited format where each individual Command marshaled to bytes prepended by varint length. See existing client implementations for encoding example. The same rules relate to many Reply in one response from server to client. Line-delimited JSON and varint-length prefixed Protobuf. For example here is how we read server response and extracting individual replies in Javascript client when JSON format used: function decodeReplies ( data ) { const replies = []; const encodedReplies = data . split ( '\\n' ); for ( const i in encodedReplies ) { if ( encodedReplies . hasOwnProperty ( i )) { if ( ! encodedReplies [ i ]) { continue ; } const reply = JSON . parse ( encodedReplies [ i ]); replies . push ( reply ); } } return replies ; } For Protobuf case see existing client implementations for decoding example. As you can see each Command has id field. This is an incremental integer field. This field will be echoed in server to client replies to commands so client could match a certain Reply to Command sent before. This is important because Websocket is asynchronous protocol where server and client both send messages at any moment and there is no builtin request-response pattern. Having id allows to match reply to command send before. So you can expect something like this in response after sending commands to server: { \"id\" : 1 , \"result\" : {}} { \"id\" : 2 , \"result\" : {}} Besides id Reply from server to client have two important fields: result and error . result contains useful payload object which can be different depending on Reply . error contains error object in case of Command processing resulted in some error on server. error is optional and if Reply does not have error then it means that Command processed successfuly and client can parse result object in an appropriate way. error objects looks like this: { \"code\" : 100 , \"message\" : \"internal server error\" } We will talk more about error handling below. The special type of Reply is asynchronous Reply . Those replies have no id field set (or id can be equal to zero). Async replies can come to client in any moment - not as reaction to issued Command but as message from server to client in arbitrary time. For example this can be message published into channel. Centrifuge library defines several command types client can issue. And well-written client must be aware of all those commands and client workflow. Communication with Centrifuge/Centrifugo server starts with connect command. Connect \u00b6 First of all client must dial with server and then send connect Command to it. Default Websocket endpoint in Centrifugo is: ws://centrifugo.example.com/connection/websocket In case of using TLS: wss://centrifugo.example.com/connection/websocket After successful dial to websocket endpoint client must send connect command to server to authorize itself. connect command looks like: { \"id\" : 1 , \"method\" : \"connect\" , \"params\" : { \"token\" : \"JWT\" , \"data\" : {} } } Where params fields are passed to client from application backend: string token - connection token. JSON data - this is only available for Centrifuge library and not for Centrifugo server. It contains custom connect data, for example it can contain client settings. In response to connect command server sends connect reply. It looks this way: { \"id\" : 1 , \"result\" : { \"client\" : \"421bf374-dd01-4f82-9def-8c31697e956f\" , \"version\" : \"2.0.0\" } } result has some fields: string client - unique client connection ID server issued to this connection string version - server version optional bool expires - whether or not server will expire connection optional int32 ttl - time in seconds until connection will expire Subscribe \u00b6 As soon as client successfully connected and got unique connection ID it is ready to subscribe on channels. To do this it must send subscribe command to server: { \"id\" : 2 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch1\" } } Fields that can be set in params are: string channel - channel to subscribe In response to subscribe client receives reply like: { \"id\" : 2 , \"result\" : {} } result can have the following fields that relate to subscription expiration: optional bool expires - indicates whether subscription expires or not. optional uint32 ttl - number of seconds until subscription expire. And several fields that relate to message recovery: optional bool recoverable - means that messages can be recovered in this subscription. optional uint32 seq - current publication sequence inside channel optional uint32 gen - current publication generation inside channel optional string epoch - current epoch inside channel optional array publications - this is an array of missed publications in channel. When received client must call general publication event handler for each message in this array. optional bool recovered - this flag is set to true when server thinks that all missed publications were successfully recovered and send in subscribe reply (in publications array) and false otherwise. See more about meaning of recovery related fields in special doc chapter . After client received successful reply on subscribe command it will receive asynchronous reply messages published to this channel. Messages can be of several types: Publication message Join message Leave message Unsub message See more about asynchronous messages below. Unsubscribe \u00b6 This is simple. When client wants to unsubscribe from channel and therefore stop receiving asynchronous subscription messages from connection related to channel it must call unsubscribe command: { \"id\" : 3 , \"method\" : \"unsubscribe\" , \"params\" : { \"channel\" : \"ch1\" } } Actually server response does not mean a lot for client - it must immediately remove channel subscription from internal implementation data structures and ignore all messages related to channel. Refresh \u00b6 It's possible to turn on client connection expiration mechanism on server. While enabled server will keep track of connections whose time of life (defined by exp timestamp) is close to the end. In this case connection will be closed. Client can prevent closing connection refreshing it's connection credentials. To do this it must send refresh command to server. refresh command similar to connect : { \"id\" : 4 , \"method\" : \"refresh\" , \"params\" : { \"token\" : \"JWT\" } } Just with actual exp and new sign . The tip whether or not connection must be refreshed comes in reply to connect command shown above - fields expires and ttl . When client connection expire mechanism is on the value of field expires in connect reply is true . In this case client implementation should look at ttl value which is seconds left until connection will be considered expired. Client must send refresh command after this ttl seconds. Server gives client a configured window to refresh token after ttl passed and then closes connection if client have not updated its token. When connecting with already expired token an error will be returned (with code 109 ). In this case client should refresh its token and reconnect with exponential backoff. RPC-like calls: publish, history, presence \u00b6 The mechanics of these calls is simple - client sends command and expects response from server. publish command allows to publish message into channel from client. Note To publish from client publish option in server configuration must be set to true history allows to ask server for channel history if enabled. presence allows to ask server for channel presence information if enabled. Asynchronous server-to-client messages \u00b6 There are several types of asynchronous messages that can come from server to client. All of them relate to current client subscriptions. The most important message is Publication : { \"result\" : { \"channel\" : \"ch1\" , \"data\" : { \"data\" : { \"input\" : \"1\" }, \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } Publication is a message published into channel. Note that there is no id field in this message - this symptom allows to distinguish it from Reply to Command . Next message is Join message: { \"result\" : { \"type\" : 1 , \"channel\" : \"ch1\" , \"data\" : { \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } Join messages sent when someone joined (subscribed on) channel. Note To enable Join and Leave messages join_leave option must be enabled on server globally or for channel namespace. Leave messages sent when someone left (unsubscribed from) channel. { \"result\" : { \"type\" : 2 , \"channel\" : \"ch1\" , \"data\" : { \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } And finally Unsub message that means that server unsubscribed current client from channel: { \"result\" : { \"type\" : 3 , \"channel\" : \"ch1\" , \"data\" : {} } } It's possible to distinguish between different types of asynchronous messages looking at type field (for Publication this field not set or 0 ). Ping Pong \u00b6 To maintain connection alive and detect broken connections client must periodically send ping commands to server and expect replies to it. Ping command looks like: { \"id\" : 32 , \"method\" : \"ping\" } Server just echoes this command back. When client does not receive ping reply for some time it must consider connection broken and try to reconnect. Recommended ping interval is 25 seconds, recommended period to wait for pong is 1-5 seconds. Though those numbers can vary. Handle disconnects \u00b6 Client should handle disconnect advices from server. In websocket case disconnect advice is sent in reason field of CLOSE Websocket frame. Reason contains string which is disconnect object encoded into JSON (even in case of Protobuf scenario). That objects looks like: { \"reason\" : \"shutdown\" , \"reconnect\" : true } It contains string reason of connection closing and advice to reconnect or not. Client should take this reconnect advice into account. In case of network problems and random disconnect from server without well known reason client should always try to reconnect with exponential intervals. Handle errors \u00b6 This section contains advices to error handling in client implementations. Errors can happen during various operations and can be handled in special way in context of some commands to tolerate network and server problems. Errors during connect must result in full client reconnect with exponential backoff strategy. The special case is error with code 110 which signals that connection token already expired. As we said above client should update its connection JWT before connecting to server again. Errors during subscribe must result in full client reconnect in case of internal error (code 100 ). And be sent to subscribe error event handler of subscription if received error is persistent. Persistent errors are errors like permission denied , bad request , namespace not found etc. Persistent errors in most situation mean a mistake from developers side. The special corner case is client-side timeout during subscribe operation. As protocol is asynchronous it's possible in this case that server will eventually subscribe client on channel but client will think that it's not subscribed. It's possible to retry subscription request and tolerate already subscribed (code 105 ) error as expected. But the simplest solution is to reconnect entirely as this is simpler and gives client a chance to connect to working server instance. Errors during rpc-like operations can be just returned to caller - i.e. user javascript code. Calls like history and presence are idempotent. You should be accurate with unidempotent operations like publish - in case of client timeout it's possible to send the same message into channel twice if retry publish after timeout - so users of libraries must care about this case \u2013 making sure they have some protection from displaying message twice on client side (maybe some sort of unique key in payload). Client implementation advices \u00b6 Here are some advices about client public API. Examples here are in Javascript language. This is just an attempt to help in developing a client - but rules here is not obligatorily the best way to implement client. Create client instance: var centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" , {}); Set connection token (in case of using Centrifugo): centrifuge . setToken ( \"XXX\" ) Connect to server: centrifuge . connect (); 2 event handlers can be set to centrifuge object: connect and disconnect centrifuge . on ( 'connect' , function ( context ) { console . log ( context ); }); centrifuge . on ( 'disconnect' , function ( context ) { console . log ( context ); }); Client created in disconnected state with reconnect attribute set to true and reconnecting flag set to false . After connect() called state goes to connecting . It's only possible to connect from disconnected state. Every time connect() called reconnect flag of client must be set to true . After each failed connect attempt state must be set to disconnected , disconnect event must be emitted (only if reconnecting flag is false ), and then reconnecting flag must be set to true (if client should continue reconnecting) to not emit disconnect event again after next in a row connect attempt failure. In case of failure next connection attempt must be scheduled automatically with backoff strategy. On successful connect reconnecting flag must be set to false , backoff retry must be resetted and connect event must be emitted. When connection lost then the same set of actions as when connect failed must be performed. Client must allow to subscribe on channels: var subscription = centrifuge . subscribe ( \"channel\" , eventHandlers ); Subscription object created and control immediately returned to caller - subscribing must be performed asynchronously. This is required because client can automatically reconnect later so event-based model better suites for subscriptions. Subscription should support several event handlers: handler for publication received from channel join message handler leave message handler error handler subscribe success event handler unsubscribe event handler Every time client connects to server it must restore all subscriptions. Every time client disconnects from server it must call unsubscribe handlers for all active subscriptions and then emit disconnect event. Client must periodically (once in 25 secs, configurable) send ping messages to server. If pong has not beed received in 5 secs (configurable) then client must disconnect from server and try to reconnect with backoff strategy. Client can automatically batch several requests into one frame to server and also must be able to handle several replies received from server in one frame.","title":"Client protocol"},{"location":"server/protocol/#client-protocol","text":"This chapter describes internal client-server protocol in details to help developers build new client libraries and understand how existing client libraries work. Note that you can always look at existing client implementations in case of any questions.","title":"Client protocol"},{"location":"server/protocol/#client-implementation-checklist","text":"First we will look at list of features client library should support. Depending on client implementation some features can be not implemented. If you an author of client library you can use this list as checklist. Note Field and method names presented in this checklist can have different names depending on the programmer's taste and language style guide. So, client: Should work both with Centrifugo and Centrifuge library based server. To work with Centrifugo client must have a method to set connection token. To work with Centrifuge lib client must provide a method to set custom headers (the only exception is browser clients where browser automatically sets headers with respect to domain rules) Should allow to use JSON payload Should allow to use binary payload (actually you can only implement Protobuf protocol as you can pass JSON over it) Must handle cases when many different Replies received in one websocket frame. In case of JSON protocol newline-delimited JSON-streaming format is used to combine several Replies into one websocket frame. In case of Protobuf protocol varint length-prefixed format is used Must survive server reload/restart and internet connection lost. In this case client must try to reconnect with exponentioal backoff strategy Must have several callback methods: onConnect , onDisconnect . Depending on implementation you can also use onError callback for critical errors that could not be gracefully handled. Should also have onMessage callback to handle async messages from server. Must have method to subscribe on channel and set several event handlers: onPublish , onJoin , onLeave , onSubscribeSuccess , onSubscribeError , onUnsubscribe . After subscribe method called it should return Subscription object to caller. This subscription object in turn should have some methods: publish , unsubscribe , subscribe , history , presence , presence_stats . Should have publish method to publish into channel without subscribing to it. Should have rpc method to send RPC request to server. Should have send method to send asynchronous message to server (without waiting response). Should handle disconnect reason. In case of Websocket it is sent by server in CLOSE Websocket frame. This is a string containing JSON object with fields: reason (string) and reconnect (bool). Client should give users access to these fields in disconnect event and automatically follow reconnect advice Must send periodic ping commands to server and thus detect broken connection. If no ping reply received from server in configured window reconnect workflow must be initiated Should fully reconnect if subscription request timed out. Timeout can be configured by client library users. Should send commands to server with timeout and handle timeout error - depending on method called timeout error handling can differ a bit. For example as said above timeout on subscription request results in full client reconnect workflow. Should support connection token refresh mechanism Should support private channel subscriptions and private subscription token refresh mechanism Should automatically recover messages after reconnect and set appropriate fields in subscribe event context. Two important fields in onSubscribeSuccess event context are recovered and isResubscribe . First field let user know what server thinks about subscription state - were all messages recovered or not. The second field must only be true if resubscribe was caused by temporary network connection lost. If user initiated resubscribe himself (calling unsubscribe method and then subscribe method) then recover workflow should not be used and isResubscribe must be false . Below in this document we will describe protocol concepts in detail. This document describes protocol specifics for Websocket transport which supports binary and text formats to transfer data. As Centrifuge has various types of messages it serializes protocol messages using JSON or Protobuf (in case of binary websockets). Note SockJS works almost the same way as JSON websocket described here but has its own extra framing on top of Centrifuge protocol messages. SockJS can only work with JSON - it's not possible to transfer binary data over it. SockJS is only needed as fallback to Websocket in web browsers.","title":"Client implementation checklist"},{"location":"server/protocol/#top-level-framing","text":"Centrifuge protocol defined in Protobuf schema . That schema is a source of truth and all protocol description below describes messages from that schema. Client sends Command to server. Server sends Reply to client. One request from client to server and one response from server to client can have more than one Command or Reply . When JSON format is used then many Command can be sent from client to server in JSON streaming line-delimited format. I.e. each individual Command encoded to JSON and then commands joined together using new line symbol \\n : { \"id\" : 1 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch1\" }} { \"id\" : 2 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch2\" }} For example here is how we do this in Javascript client when JSON format used: function encodeCommands ( commands ) { const encodedCommands = []; for ( const i in commands ) { if ( commands . hasOwnProperty ( i )) { encodedCommands . push ( JSON . stringify ( commands [ i ])); } } return encodedCommands . join ( '\\n' ); } Note This doc will use JSON format for examples because it's human-readable. Everything said here for JSON is also true for Protobuf encoded case. The only difference is how several individual Command or server Reply joined into one request \u2013 see below. Note Method is made as ENUM in protobuf schema and can be sent as integer value but it's possible to send it as string in JSON case \u2013 this was made to make JSON protocol human-friendly. When Protobuf format is used then many Command can be sent from client to server in length-delimited format where each individual Command marshaled to bytes prepended by varint length. See existing client implementations for encoding example. The same rules relate to many Reply in one response from server to client. Line-delimited JSON and varint-length prefixed Protobuf. For example here is how we read server response and extracting individual replies in Javascript client when JSON format used: function decodeReplies ( data ) { const replies = []; const encodedReplies = data . split ( '\\n' ); for ( const i in encodedReplies ) { if ( encodedReplies . hasOwnProperty ( i )) { if ( ! encodedReplies [ i ]) { continue ; } const reply = JSON . parse ( encodedReplies [ i ]); replies . push ( reply ); } } return replies ; } For Protobuf case see existing client implementations for decoding example. As you can see each Command has id field. This is an incremental integer field. This field will be echoed in server to client replies to commands so client could match a certain Reply to Command sent before. This is important because Websocket is asynchronous protocol where server and client both send messages at any moment and there is no builtin request-response pattern. Having id allows to match reply to command send before. So you can expect something like this in response after sending commands to server: { \"id\" : 1 , \"result\" : {}} { \"id\" : 2 , \"result\" : {}} Besides id Reply from server to client have two important fields: result and error . result contains useful payload object which can be different depending on Reply . error contains error object in case of Command processing resulted in some error on server. error is optional and if Reply does not have error then it means that Command processed successfuly and client can parse result object in an appropriate way. error objects looks like this: { \"code\" : 100 , \"message\" : \"internal server error\" } We will talk more about error handling below. The special type of Reply is asynchronous Reply . Those replies have no id field set (or id can be equal to zero). Async replies can come to client in any moment - not as reaction to issued Command but as message from server to client in arbitrary time. For example this can be message published into channel. Centrifuge library defines several command types client can issue. And well-written client must be aware of all those commands and client workflow. Communication with Centrifuge/Centrifugo server starts with connect command.","title":"Top level framing"},{"location":"server/protocol/#connect","text":"First of all client must dial with server and then send connect Command to it. Default Websocket endpoint in Centrifugo is: ws://centrifugo.example.com/connection/websocket In case of using TLS: wss://centrifugo.example.com/connection/websocket After successful dial to websocket endpoint client must send connect command to server to authorize itself. connect command looks like: { \"id\" : 1 , \"method\" : \"connect\" , \"params\" : { \"token\" : \"JWT\" , \"data\" : {} } } Where params fields are passed to client from application backend: string token - connection token. JSON data - this is only available for Centrifuge library and not for Centrifugo server. It contains custom connect data, for example it can contain client settings. In response to connect command server sends connect reply. It looks this way: { \"id\" : 1 , \"result\" : { \"client\" : \"421bf374-dd01-4f82-9def-8c31697e956f\" , \"version\" : \"2.0.0\" } } result has some fields: string client - unique client connection ID server issued to this connection string version - server version optional bool expires - whether or not server will expire connection optional int32 ttl - time in seconds until connection will expire","title":"Connect"},{"location":"server/protocol/#subscribe","text":"As soon as client successfully connected and got unique connection ID it is ready to subscribe on channels. To do this it must send subscribe command to server: { \"id\" : 2 , \"method\" : \"subscribe\" , \"params\" : { \"channel\" : \"ch1\" } } Fields that can be set in params are: string channel - channel to subscribe In response to subscribe client receives reply like: { \"id\" : 2 , \"result\" : {} } result can have the following fields that relate to subscription expiration: optional bool expires - indicates whether subscription expires or not. optional uint32 ttl - number of seconds until subscription expire. And several fields that relate to message recovery: optional bool recoverable - means that messages can be recovered in this subscription. optional uint32 seq - current publication sequence inside channel optional uint32 gen - current publication generation inside channel optional string epoch - current epoch inside channel optional array publications - this is an array of missed publications in channel. When received client must call general publication event handler for each message in this array. optional bool recovered - this flag is set to true when server thinks that all missed publications were successfully recovered and send in subscribe reply (in publications array) and false otherwise. See more about meaning of recovery related fields in special doc chapter . After client received successful reply on subscribe command it will receive asynchronous reply messages published to this channel. Messages can be of several types: Publication message Join message Leave message Unsub message See more about asynchronous messages below.","title":"Subscribe"},{"location":"server/protocol/#unsubscribe","text":"This is simple. When client wants to unsubscribe from channel and therefore stop receiving asynchronous subscription messages from connection related to channel it must call unsubscribe command: { \"id\" : 3 , \"method\" : \"unsubscribe\" , \"params\" : { \"channel\" : \"ch1\" } } Actually server response does not mean a lot for client - it must immediately remove channel subscription from internal implementation data structures and ignore all messages related to channel.","title":"Unsubscribe"},{"location":"server/protocol/#refresh","text":"It's possible to turn on client connection expiration mechanism on server. While enabled server will keep track of connections whose time of life (defined by exp timestamp) is close to the end. In this case connection will be closed. Client can prevent closing connection refreshing it's connection credentials. To do this it must send refresh command to server. refresh command similar to connect : { \"id\" : 4 , \"method\" : \"refresh\" , \"params\" : { \"token\" : \"JWT\" } } Just with actual exp and new sign . The tip whether or not connection must be refreshed comes in reply to connect command shown above - fields expires and ttl . When client connection expire mechanism is on the value of field expires in connect reply is true . In this case client implementation should look at ttl value which is seconds left until connection will be considered expired. Client must send refresh command after this ttl seconds. Server gives client a configured window to refresh token after ttl passed and then closes connection if client have not updated its token. When connecting with already expired token an error will be returned (with code 109 ). In this case client should refresh its token and reconnect with exponential backoff.","title":"Refresh"},{"location":"server/protocol/#rpc-like-calls-publish-history-presence","text":"The mechanics of these calls is simple - client sends command and expects response from server. publish command allows to publish message into channel from client. Note To publish from client publish option in server configuration must be set to true history allows to ask server for channel history if enabled. presence allows to ask server for channel presence information if enabled.","title":"RPC-like calls: publish, history, presence"},{"location":"server/protocol/#asynchronous-server-to-client-messages","text":"There are several types of asynchronous messages that can come from server to client. All of them relate to current client subscriptions. The most important message is Publication : { \"result\" : { \"channel\" : \"ch1\" , \"data\" : { \"data\" : { \"input\" : \"1\" }, \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } Publication is a message published into channel. Note that there is no id field in this message - this symptom allows to distinguish it from Reply to Command . Next message is Join message: { \"result\" : { \"type\" : 1 , \"channel\" : \"ch1\" , \"data\" : { \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } Join messages sent when someone joined (subscribed on) channel. Note To enable Join and Leave messages join_leave option must be enabled on server globally or for channel namespace. Leave messages sent when someone left (unsubscribed from) channel. { \"result\" : { \"type\" : 2 , \"channel\" : \"ch1\" , \"data\" : { \"info\" : { \"user\" : \"2694\" , \"client\" : \"5c48510e-cf49-4fa8-a9b2-490b22231e74\" , \"conn_info\" : { \"name\" : \"Alexander\" }, \"chan_info\" : {} } } } } And finally Unsub message that means that server unsubscribed current client from channel: { \"result\" : { \"type\" : 3 , \"channel\" : \"ch1\" , \"data\" : {} } } It's possible to distinguish between different types of asynchronous messages looking at type field (for Publication this field not set or 0 ).","title":"Asynchronous server-to-client messages"},{"location":"server/protocol/#ping-pong","text":"To maintain connection alive and detect broken connections client must periodically send ping commands to server and expect replies to it. Ping command looks like: { \"id\" : 32 , \"method\" : \"ping\" } Server just echoes this command back. When client does not receive ping reply for some time it must consider connection broken and try to reconnect. Recommended ping interval is 25 seconds, recommended period to wait for pong is 1-5 seconds. Though those numbers can vary.","title":"Ping Pong"},{"location":"server/protocol/#handle-disconnects","text":"Client should handle disconnect advices from server. In websocket case disconnect advice is sent in reason field of CLOSE Websocket frame. Reason contains string which is disconnect object encoded into JSON (even in case of Protobuf scenario). That objects looks like: { \"reason\" : \"shutdown\" , \"reconnect\" : true } It contains string reason of connection closing and advice to reconnect or not. Client should take this reconnect advice into account. In case of network problems and random disconnect from server without well known reason client should always try to reconnect with exponential intervals.","title":"Handle disconnects"},{"location":"server/protocol/#handle-errors","text":"This section contains advices to error handling in client implementations. Errors can happen during various operations and can be handled in special way in context of some commands to tolerate network and server problems. Errors during connect must result in full client reconnect with exponential backoff strategy. The special case is error with code 110 which signals that connection token already expired. As we said above client should update its connection JWT before connecting to server again. Errors during subscribe must result in full client reconnect in case of internal error (code 100 ). And be sent to subscribe error event handler of subscription if received error is persistent. Persistent errors are errors like permission denied , bad request , namespace not found etc. Persistent errors in most situation mean a mistake from developers side. The special corner case is client-side timeout during subscribe operation. As protocol is asynchronous it's possible in this case that server will eventually subscribe client on channel but client will think that it's not subscribed. It's possible to retry subscription request and tolerate already subscribed (code 105 ) error as expected. But the simplest solution is to reconnect entirely as this is simpler and gives client a chance to connect to working server instance. Errors during rpc-like operations can be just returned to caller - i.e. user javascript code. Calls like history and presence are idempotent. You should be accurate with unidempotent operations like publish - in case of client timeout it's possible to send the same message into channel twice if retry publish after timeout - so users of libraries must care about this case \u2013 making sure they have some protection from displaying message twice on client side (maybe some sort of unique key in payload).","title":"Handle errors"},{"location":"server/protocol/#client-implementation-advices","text":"Here are some advices about client public API. Examples here are in Javascript language. This is just an attempt to help in developing a client - but rules here is not obligatorily the best way to implement client. Create client instance: var centrifuge = new Centrifuge ( \"ws://localhost:8000/connection/websocket\" , {}); Set connection token (in case of using Centrifugo): centrifuge . setToken ( \"XXX\" ) Connect to server: centrifuge . connect (); 2 event handlers can be set to centrifuge object: connect and disconnect centrifuge . on ( 'connect' , function ( context ) { console . log ( context ); }); centrifuge . on ( 'disconnect' , function ( context ) { console . log ( context ); }); Client created in disconnected state with reconnect attribute set to true and reconnecting flag set to false . After connect() called state goes to connecting . It's only possible to connect from disconnected state. Every time connect() called reconnect flag of client must be set to true . After each failed connect attempt state must be set to disconnected , disconnect event must be emitted (only if reconnecting flag is false ), and then reconnecting flag must be set to true (if client should continue reconnecting) to not emit disconnect event again after next in a row connect attempt failure. In case of failure next connection attempt must be scheduled automatically with backoff strategy. On successful connect reconnecting flag must be set to false , backoff retry must be resetted and connect event must be emitted. When connection lost then the same set of actions as when connect failed must be performed. Client must allow to subscribe on channels: var subscription = centrifuge . subscribe ( \"channel\" , eventHandlers ); Subscription object created and control immediately returned to caller - subscribing must be performed asynchronously. This is required because client can automatically reconnect later so event-based model better suites for subscriptions. Subscription should support several event handlers: handler for publication received from channel join message handler leave message handler error handler subscribe success event handler unsubscribe event handler Every time client connects to server it must restore all subscriptions. Every time client disconnects from server it must call unsubscribe handlers for all active subscriptions and then emit disconnect event. Client must periodically (once in 25 secs, configurable) send ping messages to server. If pong has not beed received in 5 secs (configurable) then client must disconnect from server and try to reconnect with backoff strategy. Client can automatically batch several requests into one frame to server and also must be able to handle several replies received from server in one frame.","title":"Client implementation advices"},{"location":"server/proxy/","text":"Proxy calls to app backend \u00b6 Starting from Centrifugo v2.3.0 it's possible to proxy some client connection events over HTTP to application backend and react to them in a way you need. For example you can authenticate connection via request from Centrifugo to your app backend, refresh client sessions and answer to RPC calls sent by client over WebSocket or SockJS connections. Proxy request structure \u00b6 All proxy calls are HTTP POST requests that will be sent from Centrifugo to configured endpoints with configured timeout. These requests will have some headers copied from original client request (see details below) and include JSON body which varies depending on call type (for example data sent by client in RPC call etc, see more details about JSON bodies below). Proxy headers \u00b6 By default the following headers from original client request will be copied to proxied request: Origin (Centrifugo >= v2.3.1) User-Agent Cookie Authorization X-Real-Ip X-Forwarded-For X-Request-Id It's possible to add extra headers using proxy_extra_http_headers configuration option (available since v2.3.1). This is an array of strings in configuration file, ex: { ... \"proxy_extra_http_headers\" : [ \"X-B3-TraceId\" , \"X-B3-SpanId\" ] } Alternatively you can set extra headers via environment variable (space separated): export CENTRIFUGO_PROXY_EXTRA_HTTP_HEADERS=\"X-B3-TraceId X-B3-SpanId\" ./centrifugo Since v2.5.1 Centrifugo also forces Content-Type header to be application/json in all proxy HTTP requests since it sends body in JSON format to application backend. connect proxy \u00b6 With the following options in configuration file: { ... \"proxy_connect_endpoint\" : \"http://localhost:3000/centrifugo/connect\" , \"proxy_connect_timeout\" : 1 } \u2013 connection requests without JWT set will be proxied to proxy_connect_endpoint URL endpoint. On your backend side you can authenticate incoming connection and return client credentials to Centrifugo in response to proxied request. This means you don't need to generate JWT token and pass it to client side and can rely on cookie while authenticating user. Centrifugo should work on same domain in this case so your site cookie could be passed to Centrifugo . This also means that every new connection from user will result in HTTP POST request to your application backend. While with JWT token you usually generate it once on application page reload, if client reconnects due to Centrifugo restart or internet connection loss it uses the same JWT it had before thus usually no additional requests generated during reconnect process (until JWT expired). Payload example that will be sent to app backend when client without token wants to establish connection with Centrifugo and proxy_connect_endpoint is set to non-empty URL string: { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" } Request fields: client is a unique client ID generated by Centrifugo for each incoming connection transport is transport name ( websocket or sockjs at moment) protocol is a protocol type used by client ( json or protobuf at moment) encoding is a protocol encoding type used ( json or binary at moment) Response expected: { \"result\" : { \"user\" : \"56\" }} Result fields you can set: user (string) is user ID (calculated on app backend based on request cookie header for example). Return it as empty string for accepting unauthenticated request expire_at (optional integer) is a timestamp when connection must be considered expired. If not set or set to 0 connection won't expire at all info (optional JSON) is a connection info JSON b64info (optional string) is a binary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages data (optional JSON) is a custom data to send to client in connect command response. Supported since v2.3.1 b64data (optional string) is a custom data to send to client in connect command response for binary connections, will be decoded to raw bytes on Centrifugo side before sending to client. Supported since v2.3.1 channels (optional array of strings) - allows to provide a list of server-side channels to subscribe connection to. See more details about server-side subscriptions . Supported since v2.4.0 proxy_connect_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend. Here is the simplest example of connect handler in Tornado Python framework (note that in real system you need to authenticate user on your backend side, here we just return \"56\" as user ID): class CentrifugoConnectHandler ( tornado . web . RequestHandler ): def check_xsrf_cookie ( self ): pass def post ( self ): self . set_header ( 'Content-Type' , 'application/json; charset=\"utf-8\"' ) data = json . dumps ({ 'result' : { 'user' : '56' } }) self . write ( data ) def main (): options . parse_command_line () app = tornado . web . Application ([ ( r '/centrifugo/connect' , CentrifugoConnectHandler ), ]) app . listen ( 3000 ) tornado . ioloop . IOLoop . instance () . start () if __name__ == '__main__' : main () This example should help you to implement similar HTTP handler in any language/framework you are using on backend side. refresh proxy \u00b6 With the following options in configuration file: { ... \"proxy_refresh_endpoint\" : \"http://localhost:3000/centrifugo/refresh\" , \"proxy_refresh_timeout\" : 1 } \u2013 Centrifugo will call proxy_refresh_endpoint when it's time to refresh connection. Centrifugo itself will ask your backend about connection validity instead of refresh workflow on client side. Payload sent to app backend in refresh request (when connection is going to expire): { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" , \"user\" : \"56\" } Request fields: client , transport , protocol , encoding are the same as described for connect request payload user is a connection user ID obtained during authentication process Response expected: { \"result\" : { \"expire_at\" : 1565436268 }} Result fields: expired (boolean) is a flag to mark connection as expired - client will be diconnected expire_at (integer) is a next timestamp when connection must be considered expired info (optional JSON) is a connection info JSON b64info (optional string) is a binary connection info encoded in base64 format proxy_refresh_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend. rpc proxy \u00b6 With the following option in configuration file: { ... \"proxy_rpc_endpoint\" : \"http://localhost:3000/centrifugo/connect\" , \"proxy_rpc_timeout\" : 1 } RPC calls over client connection will be proxied to proxy_rpc_endpoint . This allows developer to utilize WebSocket (or SockJS) connection in bidirectional way. Payload sent to app backend in RPC request: { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" , \"user\" : \"56\" , \"data\" :{ \"method\" : \"getCurrentYear\" } } Request fields: client , transport , protocol , encoding are the same as described for connect request payload user is a connection user ID obtained during authentication process data is an RPC data sent by client b64data will be set instead of data field for connections that use binary payload encoding Response expected: { \"result\" : { \"data\" : { \"answer\" : \"2019\" }}} Result fields: data (JSON) is and RPC response - any valid JSON is supported b64data string can be set instead of data for binary response encoded in base64 format proxy_rpc_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend. Return custom error \u00b6 Application backend can return JSON object that contain an error to return it to client: { \"error\" : { \"code\" : 1000 , \"message\" : \"custom error\" } } Application can use error codes >= 1000, error codes in range 0-999 are reserved by Centrifugo internal protocol. This does not apply to response on refresh request as there is no sense in returning an error (will not reach client anyway). proxy_rpc_timeout (float, in seconds) controls timeout of HTTP POST request sent to app backend. Return custom disconnect \u00b6 Application backend can return JSON object that contain an custom disconnect object to disconnect client in custom way: { \"disconnect\" : { \"code\" : 4000 , \"reconnect\" : false , \"reason\" : \"custom disconnect\" } } Application can use numbers in range 4000-4999 for custom disconnect codes. Numbers below 4000 are reserved by Centrifugo internal protocol. Keep in mind that due to WebSocket protocol limitations and Centrifugo internal protocol needs you need to keep disconnect reason string no longer than 32 symbols . This does not apply to response on refresh request as there is no way to control disconnect at moment - client will always be disconnected with expired disconnect reason.","title":"Proxy to backend"},{"location":"server/proxy/#proxy-calls-to-app-backend","text":"Starting from Centrifugo v2.3.0 it's possible to proxy some client connection events over HTTP to application backend and react to them in a way you need. For example you can authenticate connection via request from Centrifugo to your app backend, refresh client sessions and answer to RPC calls sent by client over WebSocket or SockJS connections.","title":"Proxy calls to app backend"},{"location":"server/proxy/#proxy-request-structure","text":"All proxy calls are HTTP POST requests that will be sent from Centrifugo to configured endpoints with configured timeout. These requests will have some headers copied from original client request (see details below) and include JSON body which varies depending on call type (for example data sent by client in RPC call etc, see more details about JSON bodies below).","title":"Proxy request structure"},{"location":"server/proxy/#proxy-headers","text":"By default the following headers from original client request will be copied to proxied request: Origin (Centrifugo >= v2.3.1) User-Agent Cookie Authorization X-Real-Ip X-Forwarded-For X-Request-Id It's possible to add extra headers using proxy_extra_http_headers configuration option (available since v2.3.1). This is an array of strings in configuration file, ex: { ... \"proxy_extra_http_headers\" : [ \"X-B3-TraceId\" , \"X-B3-SpanId\" ] } Alternatively you can set extra headers via environment variable (space separated): export CENTRIFUGO_PROXY_EXTRA_HTTP_HEADERS=\"X-B3-TraceId X-B3-SpanId\" ./centrifugo Since v2.5.1 Centrifugo also forces Content-Type header to be application/json in all proxy HTTP requests since it sends body in JSON format to application backend.","title":"Proxy headers"},{"location":"server/proxy/#connect-proxy","text":"With the following options in configuration file: { ... \"proxy_connect_endpoint\" : \"http://localhost:3000/centrifugo/connect\" , \"proxy_connect_timeout\" : 1 } \u2013 connection requests without JWT set will be proxied to proxy_connect_endpoint URL endpoint. On your backend side you can authenticate incoming connection and return client credentials to Centrifugo in response to proxied request. This means you don't need to generate JWT token and pass it to client side and can rely on cookie while authenticating user. Centrifugo should work on same domain in this case so your site cookie could be passed to Centrifugo . This also means that every new connection from user will result in HTTP POST request to your application backend. While with JWT token you usually generate it once on application page reload, if client reconnects due to Centrifugo restart or internet connection loss it uses the same JWT it had before thus usually no additional requests generated during reconnect process (until JWT expired). Payload example that will be sent to app backend when client without token wants to establish connection with Centrifugo and proxy_connect_endpoint is set to non-empty URL string: { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" } Request fields: client is a unique client ID generated by Centrifugo for each incoming connection transport is transport name ( websocket or sockjs at moment) protocol is a protocol type used by client ( json or protobuf at moment) encoding is a protocol encoding type used ( json or binary at moment) Response expected: { \"result\" : { \"user\" : \"56\" }} Result fields you can set: user (string) is user ID (calculated on app backend based on request cookie header for example). Return it as empty string for accepting unauthenticated request expire_at (optional integer) is a timestamp when connection must be considered expired. If not set or set to 0 connection won't expire at all info (optional JSON) is a connection info JSON b64info (optional string) is a binary connection info encoded in base64 format, will be decoded to raw bytes on Centrifugo before using in messages data (optional JSON) is a custom data to send to client in connect command response. Supported since v2.3.1 b64data (optional string) is a custom data to send to client in connect command response for binary connections, will be decoded to raw bytes on Centrifugo side before sending to client. Supported since v2.3.1 channels (optional array of strings) - allows to provide a list of server-side channels to subscribe connection to. See more details about server-side subscriptions . Supported since v2.4.0 proxy_connect_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend. Here is the simplest example of connect handler in Tornado Python framework (note that in real system you need to authenticate user on your backend side, here we just return \"56\" as user ID): class CentrifugoConnectHandler ( tornado . web . RequestHandler ): def check_xsrf_cookie ( self ): pass def post ( self ): self . set_header ( 'Content-Type' , 'application/json; charset=\"utf-8\"' ) data = json . dumps ({ 'result' : { 'user' : '56' } }) self . write ( data ) def main (): options . parse_command_line () app = tornado . web . Application ([ ( r '/centrifugo/connect' , CentrifugoConnectHandler ), ]) app . listen ( 3000 ) tornado . ioloop . IOLoop . instance () . start () if __name__ == '__main__' : main () This example should help you to implement similar HTTP handler in any language/framework you are using on backend side.","title":"connect proxy"},{"location":"server/proxy/#refresh-proxy","text":"With the following options in configuration file: { ... \"proxy_refresh_endpoint\" : \"http://localhost:3000/centrifugo/refresh\" , \"proxy_refresh_timeout\" : 1 } \u2013 Centrifugo will call proxy_refresh_endpoint when it's time to refresh connection. Centrifugo itself will ask your backend about connection validity instead of refresh workflow on client side. Payload sent to app backend in refresh request (when connection is going to expire): { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" , \"user\" : \"56\" } Request fields: client , transport , protocol , encoding are the same as described for connect request payload user is a connection user ID obtained during authentication process Response expected: { \"result\" : { \"expire_at\" : 1565436268 }} Result fields: expired (boolean) is a flag to mark connection as expired - client will be diconnected expire_at (integer) is a next timestamp when connection must be considered expired info (optional JSON) is a connection info JSON b64info (optional string) is a binary connection info encoded in base64 format proxy_refresh_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.","title":"refresh proxy"},{"location":"server/proxy/#rpc-proxy","text":"With the following option in configuration file: { ... \"proxy_rpc_endpoint\" : \"http://localhost:3000/centrifugo/connect\" , \"proxy_rpc_timeout\" : 1 } RPC calls over client connection will be proxied to proxy_rpc_endpoint . This allows developer to utilize WebSocket (or SockJS) connection in bidirectional way. Payload sent to app backend in RPC request: { \"client\" : \"9336a229-2400-4ebc-8c50-0a643d22e8a0\" , \"transport\" : \"websocket\" , \"protocol\" : \"json\" , \"encoding\" : \"json\" , \"user\" : \"56\" , \"data\" :{ \"method\" : \"getCurrentYear\" } } Request fields: client , transport , protocol , encoding are the same as described for connect request payload user is a connection user ID obtained during authentication process data is an RPC data sent by client b64data will be set instead of data field for connections that use binary payload encoding Response expected: { \"result\" : { \"data\" : { \"answer\" : \"2019\" }}} Result fields: data (JSON) is and RPC response - any valid JSON is supported b64data string can be set instead of data for binary response encoded in base64 format proxy_rpc_timeout (float, in seconds) config option controls timeout of HTTP POST request sent to app backend.","title":"rpc proxy"},{"location":"server/proxy/#return-custom-error","text":"Application backend can return JSON object that contain an error to return it to client: { \"error\" : { \"code\" : 1000 , \"message\" : \"custom error\" } } Application can use error codes >= 1000, error codes in range 0-999 are reserved by Centrifugo internal protocol. This does not apply to response on refresh request as there is no sense in returning an error (will not reach client anyway). proxy_rpc_timeout (float, in seconds) controls timeout of HTTP POST request sent to app backend.","title":"Return custom error"},{"location":"server/proxy/#return-custom-disconnect","text":"Application backend can return JSON object that contain an custom disconnect object to disconnect client in custom way: { \"disconnect\" : { \"code\" : 4000 , \"reconnect\" : false , \"reason\" : \"custom disconnect\" } } Application can use numbers in range 4000-4999 for custom disconnect codes. Numbers below 4000 are reserved by Centrifugo internal protocol. Keep in mind that due to WebSocket protocol limitations and Centrifugo internal protocol needs you need to keep disconnect reason string no longer than 32 symbols . This does not apply to response on refresh request as there is no way to control disconnect at moment - client will always be disconnected with expired disconnect reason.","title":"Return custom disconnect"},{"location":"server/recover/","text":"How message recovery works \u00b6 This description uses offset field available since Centrifugo v2.5.0 which replaced two uint32 fields seq and gen in client protocol schema. This means that here we describe a case when Centrifugo config contains v3_use_offset option enabled: { ... \"v3_use_offset\" : true } Note For seq and gen recovery works in similar way, but we have two uint32 fields instead of single uint64 offset . One of the most interesting features of Centrifugo is message recovery after short network disconnects. This mechanism allows client to automatically get missed publications on successful resubscribe to channel after being disconnected for a while. In general, you would query your application backend for actual state on every client reconnect - but message recovery feature allows Centrifugo to deal with this and restore missed publications from history cache thus radically reducing load on your application backend and your main database in some scenarios. To enable recovery mechanism for channels set history_recover boolean configuration option to true on the configuration file top-level or for a channel namespace. When subscribing on channels Centrifugo will return missed publications to client in subscribe Reply , also it will return special recovered boolean flag to indicate whether all missed publications successfully recovered after disconnect or not. Centrifugo recovery model based on two fields in protocol: offset and epoch . All fields managed automatically by Centrifugo client libraries, but it's good to know how recovery works under the hood. Once history_recover option enabled every publication will have incremental (inside channel) offset field. This field has uint64 type. Another field is string epoch . It exists to handle cases when history storage has been restarted while client was in disconnected state so publication numeration in a channel started from scratch. For example at moment Memory engine does not persist publication sequences on disk so every restart will start numeration from scratch, after each restart new epoch field generated, and we can understand in recovery process that client could miss messages thus returning correct recovered flag in subscribe Reply . This also applies to Redis engine \u2013 if you do not use AOF with fsync then sequences can be lost after Redis restart. When using Redis engine you need to use fully in-memory model strategy or AOF with fsync to guarantee reliability of recovered flag sent by Centrifugo. When server receives subscribe command with boolean flag recover set to true and offset , epoch set to values last seen by a client (see SubscribeRequest type in protocol definitions ) it can try to find all missed publications from history cache. Recovered publications will be passed to client in subscribe Reply in correct order, and your publication handler will be automatically called to process each missed message. You can also manually implement your own recovery algorithm on top of basic PUB/SUB possibilities that Centrifugo provides. As we said above you can simply ask your backend for an actual state after every client reconnect completely bypassing recovery mechanism described here.","title":"Message recovery"},{"location":"server/recover/#how-message-recovery-works","text":"This description uses offset field available since Centrifugo v2.5.0 which replaced two uint32 fields seq and gen in client protocol schema. This means that here we describe a case when Centrifugo config contains v3_use_offset option enabled: { ... \"v3_use_offset\" : true } Note For seq and gen recovery works in similar way, but we have two uint32 fields instead of single uint64 offset . One of the most interesting features of Centrifugo is message recovery after short network disconnects. This mechanism allows client to automatically get missed publications on successful resubscribe to channel after being disconnected for a while. In general, you would query your application backend for actual state on every client reconnect - but message recovery feature allows Centrifugo to deal with this and restore missed publications from history cache thus radically reducing load on your application backend and your main database in some scenarios. To enable recovery mechanism for channels set history_recover boolean configuration option to true on the configuration file top-level or for a channel namespace. When subscribing on channels Centrifugo will return missed publications to client in subscribe Reply , also it will return special recovered boolean flag to indicate whether all missed publications successfully recovered after disconnect or not. Centrifugo recovery model based on two fields in protocol: offset and epoch . All fields managed automatically by Centrifugo client libraries, but it's good to know how recovery works under the hood. Once history_recover option enabled every publication will have incremental (inside channel) offset field. This field has uint64 type. Another field is string epoch . It exists to handle cases when history storage has been restarted while client was in disconnected state so publication numeration in a channel started from scratch. For example at moment Memory engine does not persist publication sequences on disk so every restart will start numeration from scratch, after each restart new epoch field generated, and we can understand in recovery process that client could miss messages thus returning correct recovered flag in subscribe Reply . This also applies to Redis engine \u2013 if you do not use AOF with fsync then sequences can be lost after Redis restart. When using Redis engine you need to use fully in-memory model strategy or AOF with fsync to guarantee reliability of recovered flag sent by Centrifugo. When server receives subscribe command with boolean flag recover set to true and offset , epoch set to values last seen by a client (see SubscribeRequest type in protocol definitions ) it can try to find all missed publications from history cache. Recovered publications will be passed to client in subscribe Reply in correct order, and your publication handler will be automatically called to process each missed message. You can also manually implement your own recovery algorithm on top of basic PUB/SUB possibilities that Centrifugo provides. As we said above you can simply ask your backend for an actual state after every client reconnect completely bypassing recovery mechanism described here.","title":"How message recovery works"},{"location":"server/server_subs/","text":"Server-side subscriptions \u00b6 Starting from v2.4.0 Centrifugo supports server-side subscriptions. Overview \u00b6 Before v2.4.0 only client could initiate subscription to channel calling Subscribe method. While this is actually the most flexible approach as client side usually knows well which channels it needs to consume in concrete moment \u2013 in some situations all you need is to subscribe your connections to several channels on server side at the moment of connection establishement. So client effectively starts receiving publications from those channels without explicitly call Subscribe method. You can set a list of channels for connection in two ways at moment: over connection JWT using channels claim, which is an array of strings over connect proxy returning channels field in result (also an array of strings) On client side you need to listen for publications from server-side channels using top-level client event handler. At this moment server-side subscriptions only supported by centrifuge-js client. With it all you need to do on client side is sth like this: var centrifuge = new Centrifuge ( address ); centrifuge . on ( 'publish' , function ( ctx ) { const channel = ctx . channel ; const payload = JSON . stringify ( ctx . data ); console . log ( 'Publication from server-side channel' , channel , payload ); }); centrifuge . connect (); I.e. listen for publications without any usage of subscription objects. You can look at channel publication relates to using field in callback context as shown in example. In future this mechanism will be supported by all our clients . Hopefully with community help this will happen very soon. Server-side subscription limitations \u00b6 Such subscriptions do not fit well for dynamic subscriptions \u2013 as Centrifugo does not have subscribe server API at moment. Also the same rules about best practices of working with channels and client-side subscriptions equally applicable to server-side subscription. Automatic personal channel subscription \u00b6 v2.4.0 also introduced some sugar on top of server-side subscriptions. It's possible to automatically subscribe user to personal server-side channel. To enable this you need to enable user_subscribe_to_personal boolean option (by default false ). As soon as you do this every connection with non-empty user ID will be automatically subscribed to personal user-limited channel. Anonymous users with empty user ID won't be subscribed to any channel. For example, if you set this option and user with ID 87334 connects to Centrifugo it will be automatically subscribed to channel #87334 and you can process personal publications on client side in the same way as shown above. As you can see by default generated personal channel name belongs to default namespace (i.e. no explicit namespace used). To set custom namespace name use user_personal_channel_namespace option (string, default \"\" ) \u2013 i.e. the name of namespace from configured configuration namespaces array. In this case if you set user_personal_channel_namespace to personal for example \u2013 then the automatically generated personal channel will be personal:#87334 \u2013 user will be automatically subscribed to it on connect and you can use this channel name to publish personal notifications to online user. Mark namespace as server-side \u00b6 v2.4.0 also introduced new channel namespace boolean option called server_side (default false ). When on then all client-side subscription requests to channels in namespace will be rejected with PermissionDenied error.","title":"Server-side subscriptions"},{"location":"server/server_subs/#server-side-subscriptions","text":"Starting from v2.4.0 Centrifugo supports server-side subscriptions.","title":"Server-side subscriptions"},{"location":"server/server_subs/#overview","text":"Before v2.4.0 only client could initiate subscription to channel calling Subscribe method. While this is actually the most flexible approach as client side usually knows well which channels it needs to consume in concrete moment \u2013 in some situations all you need is to subscribe your connections to several channels on server side at the moment of connection establishement. So client effectively starts receiving publications from those channels without explicitly call Subscribe method. You can set a list of channels for connection in two ways at moment: over connection JWT using channels claim, which is an array of strings over connect proxy returning channels field in result (also an array of strings) On client side you need to listen for publications from server-side channels using top-level client event handler. At this moment server-side subscriptions only supported by centrifuge-js client. With it all you need to do on client side is sth like this: var centrifuge = new Centrifuge ( address ); centrifuge . on ( 'publish' , function ( ctx ) { const channel = ctx . channel ; const payload = JSON . stringify ( ctx . data ); console . log ( 'Publication from server-side channel' , channel , payload ); }); centrifuge . connect (); I.e. listen for publications without any usage of subscription objects. You can look at channel publication relates to using field in callback context as shown in example. In future this mechanism will be supported by all our clients . Hopefully with community help this will happen very soon.","title":"Overview"},{"location":"server/server_subs/#server-side-subscription-limitations","text":"Such subscriptions do not fit well for dynamic subscriptions \u2013 as Centrifugo does not have subscribe server API at moment. Also the same rules about best practices of working with channels and client-side subscriptions equally applicable to server-side subscription.","title":"Server-side subscription limitations"},{"location":"server/server_subs/#automatic-personal-channel-subscription","text":"v2.4.0 also introduced some sugar on top of server-side subscriptions. It's possible to automatically subscribe user to personal server-side channel. To enable this you need to enable user_subscribe_to_personal boolean option (by default false ). As soon as you do this every connection with non-empty user ID will be automatically subscribed to personal user-limited channel. Anonymous users with empty user ID won't be subscribed to any channel. For example, if you set this option and user with ID 87334 connects to Centrifugo it will be automatically subscribed to channel #87334 and you can process personal publications on client side in the same way as shown above. As you can see by default generated personal channel name belongs to default namespace (i.e. no explicit namespace used). To set custom namespace name use user_personal_channel_namespace option (string, default \"\" ) \u2013 i.e. the name of namespace from configured configuration namespaces array. In this case if you set user_personal_channel_namespace to personal for example \u2013 then the automatically generated personal channel will be personal:#87334 \u2013 user will be automatically subscribed to it on connect and you can use this channel name to publish personal notifications to online user.","title":"Automatic personal channel subscription"},{"location":"server/server_subs/#mark-namespace-as-server-side","text":"v2.4.0 also introduced new channel namespace boolean option called server_side (default false ). When on then all client-side subscription requests to channels in namespace will be rejected with PermissionDenied error.","title":"Mark namespace as server-side"},{"location":"server/signals/","text":"Signal handling \u00b6 You can send HUP signal to Centrifugo to reload configuration: kill -HUP <PID> Though at moment this will only reload channel and namespace configuration. Also Centrifugo tries to gracefully shutdown client connections when SIGINT or SIGTERM signals received. By default maximum graceful shutdown period is 30 seconds but can be changed using shutdown_timeout configuration option.","title":"Signal handling"},{"location":"server/signals/#signal-handling","text":"You can send HUP signal to Centrifugo to reload configuration: kill -HUP <PID> Though at moment this will only reload channel and namespace configuration. Also Centrifugo tries to gracefully shutdown client connections when SIGINT or SIGTERM signals received. By default maximum graceful shutdown period is 30 seconds but can be changed using shutdown_timeout configuration option.","title":"Signal handling"},{"location":"transports/","text":"This section describes client transports that Centrifugo supports and some specific topics and configuration regarding to each of those transports. At moment Centrifugo supports 2 transports: Websocket SockJS Having both of these transport means that it's possible to connect to Centrifugo from everywhere.","title":"Transports overview"},{"location":"transports/sockjs/","text":"SockJS \u00b6 SockJS is a polyfill browser library which provides HTTP-based fallback transports in case when it's not possible to establish Websocket connection. This can happen in old client browsers or because of some proxy behind client and server that cuts of Websocket traffic. You can find more information on SockJS project Github page . If you have a requirement to work everywhere SockJS is the solution. SockJS will automatically choose best fallback transport if Websocket connection failed for some reason. Some of the fallback transports are: Eventsource (SSE) XHR-streaming Long-polling And more (see SockJS docs ) One caveat when using SockJS is that you need to use sticky sessions mechanism if you have many Centrifugo nodes running . This mechanism usually supported by load balancers (for example Nginx). Sticky sessions mean that all requests from the same client will come to the same Centrifugo node. This is necessary because SockJS maintains connection session in process memory thus allowing bidirectional communication between a client and a server. Sticky mechanism not required if you only use one Centrifugo node on a backend. See how enable sticky sessions in Nginx in deploy section of this doc. SockJS connection endpoint in Centrifugo is /connection/sockjs . SockJS does not support binary data, so you are limited in using JSON with it.","title":"SockJS"},{"location":"transports/sockjs/#sockjs","text":"SockJS is a polyfill browser library which provides HTTP-based fallback transports in case when it's not possible to establish Websocket connection. This can happen in old client browsers or because of some proxy behind client and server that cuts of Websocket traffic. You can find more information on SockJS project Github page . If you have a requirement to work everywhere SockJS is the solution. SockJS will automatically choose best fallback transport if Websocket connection failed for some reason. Some of the fallback transports are: Eventsource (SSE) XHR-streaming Long-polling And more (see SockJS docs ) One caveat when using SockJS is that you need to use sticky sessions mechanism if you have many Centrifugo nodes running . This mechanism usually supported by load balancers (for example Nginx). Sticky sessions mean that all requests from the same client will come to the same Centrifugo node. This is necessary because SockJS maintains connection session in process memory thus allowing bidirectional communication between a client and a server. Sticky mechanism not required if you only use one Centrifugo node on a backend. See how enable sticky sessions in Nginx in deploy section of this doc. SockJS connection endpoint in Centrifugo is /connection/sockjs . SockJS does not support binary data, so you are limited in using JSON with it.","title":"SockJS"},{"location":"transports/websocket/","text":"Websocket \u00b6 Websocket is the main transport in Centrifugo. It's a very efficient low-overhead protocol on top of TCP. The biggest advantage is that Websocket works out of the box in all modern browsers and almost all programming languages have Websocket implementations. This makes Websocket a pretty universal transport that can even be used to connect to Centrifugo from web apps and mobile apps and other environments. Websocket connection endpoint in Centrifugo is /connection/websocket . If you want to use Protobuf binary protocol then you need to connect to /connection/websocket?format=protobuf Websocket compression \u00b6 An experimental feature for raw websocket endpoint - permessage-deflate compression for websocket messages. Btw look at great article about websocket compression. We consider this experimental because this websocket compression is experimental in Gorilla Websocket library that Centrifugo uses internally. Websocket compression can reduce an amount of traffic travelling over the wire. But keep in mind that enabling websocket compression will result in much slower Centrifugo performance and more memory usage \u2013 depending on your message rate this can be very noticeable. To enable websocket compression for raw websocket endpoint set websocket_compression: true in configuration file. After this clients that support permessage-deflate will negotiate compression with server automatically. Note that enabling compression does not mean that every connection will use it - this depends on client support for this feature. Another option is websocket_compression_min_size . Default 0. This is a minimal size of message in bytes for which we use deflate compression when writing it to client's connection. Default value 0 means that we will compress all messages when websocket_compression enabled and compression support negotiated with client. It's also possible to control websocket compression level defined at compress/flate By default when compression with a client negotiated Centrifugo uses compression level 1 (BestSpeed). If you want to set custom compression level use websocket_compression_level configuration option. If you have a few writes then websocket_use_write_buffer_pool (boolean, default false ) option can reduce memory usage of Centrifugo a bit as there won't be separate write buffer binded to each WebSocket connection.","title":"Websocket"},{"location":"transports/websocket/#websocket","text":"Websocket is the main transport in Centrifugo. It's a very efficient low-overhead protocol on top of TCP. The biggest advantage is that Websocket works out of the box in all modern browsers and almost all programming languages have Websocket implementations. This makes Websocket a pretty universal transport that can even be used to connect to Centrifugo from web apps and mobile apps and other environments. Websocket connection endpoint in Centrifugo is /connection/websocket . If you want to use Protobuf binary protocol then you need to connect to /connection/websocket?format=protobuf","title":"Websocket"},{"location":"transports/websocket/#websocket-compression","text":"An experimental feature for raw websocket endpoint - permessage-deflate compression for websocket messages. Btw look at great article about websocket compression. We consider this experimental because this websocket compression is experimental in Gorilla Websocket library that Centrifugo uses internally. Websocket compression can reduce an amount of traffic travelling over the wire. But keep in mind that enabling websocket compression will result in much slower Centrifugo performance and more memory usage \u2013 depending on your message rate this can be very noticeable. To enable websocket compression for raw websocket endpoint set websocket_compression: true in configuration file. After this clients that support permessage-deflate will negotiate compression with server automatically. Note that enabling compression does not mean that every connection will use it - this depends on client support for this feature. Another option is websocket_compression_min_size . Default 0. This is a minimal size of message in bytes for which we use deflate compression when writing it to client's connection. Default value 0 means that we will compress all messages when websocket_compression enabled and compression support negotiated with client. It's also possible to control websocket compression level defined at compress/flate By default when compression with a client negotiated Centrifugo uses compression level 1 (BestSpeed). If you want to set custom compression level use websocket_compression_level configuration option. If you have a few writes then websocket_use_write_buffer_pool (boolean, default false ) option can reduce memory usage of Centrifugo a bit as there won't be separate write buffer binded to each WebSocket connection.","title":"Websocket compression"}]}